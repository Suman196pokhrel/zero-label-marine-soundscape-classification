{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "247df43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66abcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration for two-stage detector\"\"\"\n",
    "    \n",
    "    ROOT_DIR = Path(\"~/Uni-stuff/semester-2/applied_Ml/reef_zmsc\").expanduser()\n",
    "    \n",
    "    # Existing Stage 1 model (AMBIENT vs BIO)\n",
    "    STAGE1_MODEL = ROOT_DIR / \"data/autolabeling_fixed/models/classifier.joblib\"\n",
    "    \n",
    "    # Input data\n",
    "    CLUSTERED_DATA = ROOT_DIR / \"data/clustering/results_50k/clustered_data_kmeans.parquet\"\n",
    "    PREPROCESSED_DATA = ROOT_DIR / \"data/features/embeds_preprocessed_50k/preprocessed_features_pca.parquet\"\n",
    "    \n",
    "    # Output directory\n",
    "    OUTPUT_DIR = ROOT_DIR / \"data/two_stage_detector\"\n",
    "    MODEL_DIR = OUTPUT_DIR / \"models\"\n",
    "    RESULTS_DIR = OUTPUT_DIR / \"results\"\n",
    "    \n",
    "    # Feature columns\n",
    "    FEATURE_COLS = [f\"pca_{i}\" for i in range(39)]\n",
    "    \n",
    "    RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "982f0394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanSoundDetector:\n",
    "    \"\"\"\n",
    "    Detects anthropogenic sounds (boats, engines, equipment)\n",
    "    within clips classified as AMBIENT by Stage 1.\n",
    "    \n",
    "    Uses acoustic signatures known for human sounds:\n",
    "    - Low frequency rumble (20-200 Hz) → boat engines\n",
    "    - Tonal/harmonic patterns → vessels, machinery\n",
    "    - Sustained energy → continuous operation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with acoustic rules for human sounds\"\"\"\n",
    "        self.rules = self._define_human_rules()\n",
    "        self.model = None\n",
    "        self.threshold = 0.6  # Confidence threshold for HUMAN classification\n",
    "    \n",
    "    def _define_human_rules(self):\n",
    "        \"\"\"\n",
    "        Define acoustic signatures for human/anthropogenic sounds\n",
    "        Based on marine acoustic monitoring literature\n",
    "        \"\"\"\n",
    "        rules = {\n",
    "            'boat_engine': {\n",
    "                'description': 'Boat/vessel engine (low freq rumble, 20-200 Hz)',\n",
    "                'pca_signature': {\n",
    "                    'pca_0': (-2.0, -0.5),  # Very negative (low freq indicator)\n",
    "                    'pca_2': (-1.5, 1.5),   # Mid-range\n",
    "                    'variance': (None, 0.4)  # Low variance (sustained)\n",
    "                }\n",
    "            },\n",
    "            'vessel_propeller': {\n",
    "                'description': 'Vessel propeller (tonal harmonics, 50-500 Hz)',\n",
    "                'pca_signature': {\n",
    "                    'pca_0': (-1.5, 0.0),\n",
    "                    'pca_1': (-1.0, 1.0),\n",
    "                    'variance': (None, 0.5)\n",
    "                }\n",
    "            },\n",
    "            'machinery': {\n",
    "                'description': 'Underwater machinery/equipment',\n",
    "                'pca_signature': {\n",
    "                    'pca_3': (None, -1.0),  # Specific pattern\n",
    "                    'pca_4': (None, -1.0),\n",
    "                    'variance': (None, 0.3)  # Very low variance\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return rules\n",
    "    \n",
    "    def calculate_human_score(self, features_df):\n",
    "        \"\"\"\n",
    "        Calculate likelihood that clips are HUMAN sounds\n",
    "        \n",
    "        Args:\n",
    "            features_df: DataFrame with PCA features\n",
    "        \n",
    "        Returns:\n",
    "            Array of human likelihood scores (0-1)\n",
    "        \"\"\"\n",
    "        \n",
    "        n_clips = len(features_df)\n",
    "        human_scores = np.zeros(n_clips)\n",
    "        \n",
    "        for rule_name, rule_def in self.rules.items():\n",
    "            rule_match = np.ones(n_clips, dtype=bool)\n",
    "            \n",
    "            sig = rule_def['pca_signature']\n",
    "            \n",
    "            # Check PCA component ranges\n",
    "            for pca_col, (min_val, max_val) in sig.items():\n",
    "                if pca_col == 'variance':\n",
    "                    # Check overall variance across components\n",
    "                    variance = features_df[Config.FEATURE_COLS].std(axis=1)\n",
    "                    if min_val is not None:\n",
    "                        rule_match &= (variance >= min_val)\n",
    "                    if max_val is not None:\n",
    "                        rule_match &= (variance <= max_val)\n",
    "                else:\n",
    "                    if pca_col in features_df.columns:\n",
    "                        vals = features_df[pca_col]\n",
    "                        if min_val is not None:\n",
    "                            rule_match &= (vals >= min_val)\n",
    "                        if max_val is not None:\n",
    "                            rule_match &= (vals <= max_val)\n",
    "            \n",
    "            # Add to score (normalized)\n",
    "            human_scores += rule_match.astype(float) / len(self.rules)\n",
    "        \n",
    "        return human_scores\n",
    "    \n",
    "    def find_human_candidates(self, ambient_df, top_n=500):\n",
    "        \"\"\"\n",
    "        From AMBIENT-labeled clips, find most likely HUMAN sounds\n",
    "        \n",
    "        Args:\n",
    "            ambient_df: DataFrame of clips labeled AMBIENT by Stage 1\n",
    "            top_n: Number of top candidates to extract\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame of likely HUMAN sounds\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n🔍 Searching for HUMAN sounds in {len(ambient_df):,} AMBIENT clips...\")\n",
    "        \n",
    "        # Calculate human scores\n",
    "        human_scores = self.calculate_human_score(ambient_df)\n",
    "        ambient_df = ambient_df.copy()\n",
    "        ambient_df['human_score'] = human_scores\n",
    "        \n",
    "        # Get top candidates\n",
    "        human_candidates = ambient_df.nlargest(top_n, 'human_score')\n",
    "        \n",
    "        print(f\"\\n📊 Human score distribution:\")\n",
    "        print(f\"   Mean: {human_scores.mean():.3f}\")\n",
    "        print(f\"   Std:  {human_scores.std():.3f}\")\n",
    "        print(f\"   Max:  {human_scores.max():.3f}\")\n",
    "        print(f\"   Top {top_n} scores: {human_candidates['human_score'].min():.3f} - {human_candidates['human_score'].max():.3f}\")\n",
    "        \n",
    "        # Show sample files for manual verification\n",
    "        print(f\"\\n📁 Top 10 HUMAN candidates (for manual verification):\")\n",
    "        for idx, row in human_candidates.head(10).iterrows():\n",
    "            print(f\"   {row['filepath']} (score: {row['human_score']:.3f})\")\n",
    "        \n",
    "        return human_candidates\n",
    "    \n",
    "    def create_training_data(self, ambient_df, human_candidates, negative_sample_size=1000):\n",
    "        \"\"\"\n",
    "        Create training data for HUMAN detector\n",
    "        \n",
    "        Args:\n",
    "            ambient_df: All AMBIENT clips\n",
    "            human_candidates: Likely HUMAN sounds\n",
    "            negative_sample_size: How many true AMBIENT to include\n",
    "        \n",
    "        Returns:\n",
    "            X, y for training\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n📦 Creating HUMAN detector training data...\")\n",
    "        \n",
    "        # Positive class: Top HUMAN candidates\n",
    "        human_df = human_candidates.copy()\n",
    "        human_df['stage2_label'] = 'HUMAN'\n",
    "        \n",
    "        # Negative class: Sample from remaining AMBIENT\n",
    "        # Exclude human candidates\n",
    "        true_ambient = ambient_df[~ambient_df.index.isin(human_candidates.index)]\n",
    "        true_ambient_sample = true_ambient.sample(\n",
    "            min(negative_sample_size, len(true_ambient)),\n",
    "            random_state=Config.RANDOM_STATE\n",
    "        )\n",
    "        true_ambient_sample = true_ambient_sample.copy()\n",
    "        true_ambient_sample['stage2_label'] = 'AMBIENT'\n",
    "        \n",
    "        # Combine\n",
    "        training_df = pd.concat([human_df, true_ambient_sample], ignore_index=True)\n",
    "        \n",
    "        print(f\"\\n   HUMAN samples: {len(human_df):,}\")\n",
    "        print(f\"   AMBIENT samples: {len(true_ambient_sample):,}\")\n",
    "        print(f\"   Total: {len(training_df):,}\")\n",
    "        print(f\"   Class balance: {len(human_df) / len(training_df):.1%} HUMAN\")\n",
    "        \n",
    "        return training_df\n",
    "    \n",
    "    def train(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Train the Stage 2 HUMAN detector\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"TRAINING STAGE 2: HUMAN DETECTOR\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        \n",
    "        print(f\"\\n  Training samples: {len(X_train):,}\")\n",
    "        print(f\"  Test samples: {len(X_test):,}\")\n",
    "        print(f\"\\n  Training distribution:\")\n",
    "        for label, count in pd.Series(y_train).value_counts().items():\n",
    "            print(f\"    {label}: {count:,}\")\n",
    "        \n",
    "        # Train Random Forest (better for imbalanced data)\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=20,\n",
    "            class_weight='balanced',  # Handle imbalance\n",
    "            random_state=Config.RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_proba = self.model.predict_proba(X_test)\n",
    "        \n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"\\n  ✅ Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"\\n  Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred, digits=3))\n",
    "        \n",
    "        # Show confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"\\n  Confusion Matrix:\")\n",
    "        print(f\"                Predicted\")\n",
    "        print(f\"                AMBIENT  HUMAN\")\n",
    "        print(f\"  Actual AMBIENT  {cm[0,0]:5d}  {cm[0,1]:5d}\")\n",
    "        print(f\"  Actual HUMAN    {cm[1,0]:5d}  {cm[1,1]:5d}\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, features, return_proba=False):\n",
    "        \"\"\"Predict HUMAN vs AMBIENT\"\"\"\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained! Call train() first.\")\n",
    "        \n",
    "        if return_proba:\n",
    "            return self.model.predict_proba(features)\n",
    "        else:\n",
    "            return self.model.predict(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4816549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStageDetector:\n",
    "    \"\"\"\n",
    "    Complete two-stage hierarchical classifier\n",
    "    \n",
    "    Stage 1: Separate BIO from everything else\n",
    "    Stage 2: Within non-BIO, separate HUMAN from AMBIENT\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stage1_model_path, stage2_detector=None):\n",
    "        \"\"\"\n",
    "        Initialize with Stage 1 model\n",
    "        \n",
    "        Args:\n",
    "            stage1_model_path: Path to Stage 1 classifier (AMBIENT vs BIO)\n",
    "            stage2_detector: HumanSoundDetector instance (optional)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"TWO-STAGE HIERARCHICAL DETECTOR\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        \n",
    "        # Load Stage 1 model\n",
    "        print(f\"\\n📥 Loading Stage 1 model (AMBIENT vs BIO)...\")\n",
    "        self.stage1_model = joblib.load(stage1_model_path)\n",
    "        print(f\"   ✅ Loaded from: {stage1_model_path}\")\n",
    "        \n",
    "        # Stage 2 detector\n",
    "        self.stage2_detector = stage2_detector\n",
    "        if stage2_detector is not None:\n",
    "            print(f\"   ✅ Stage 2 HUMAN detector ready\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  Stage 2 HUMAN detector not provided (will be trained)\")\n",
    "    \n",
    "    def predict(self, features_df, return_confidence=False):\n",
    "        \"\"\"\n",
    "        Run complete two-stage prediction\n",
    "        \n",
    "        Args:\n",
    "            features_df: DataFrame with PCA features\n",
    "            return_confidence: If True, also return confidence scores\n",
    "        \n",
    "        Returns:\n",
    "            predictions (and optionally confidences)\n",
    "        \"\"\"\n",
    "        \n",
    "        X = features_df[Config.FEATURE_COLS].values\n",
    "        \n",
    "        # Stage 1: AMBIENT vs BIO\n",
    "        stage1_pred = self.stage1_model.predict(X)\n",
    "        stage1_proba = self.stage1_model.predict_proba(X)\n",
    "        \n",
    "        # Initialize final predictions\n",
    "        final_pred = stage1_pred.copy()\n",
    "        final_confidence = stage1_proba.max(axis=1)\n",
    "        \n",
    "        # Stage 2: For AMBIENT clips, check if HUMAN\n",
    "        if self.stage2_detector is not None and self.stage2_detector.model is not None:\n",
    "            ambient_mask = (stage1_pred == 'AMBIENT')\n",
    "            \n",
    "            if ambient_mask.sum() > 0:\n",
    "                # Run HUMAN detector on AMBIENT clips\n",
    "                X_ambient = X[ambient_mask]\n",
    "                stage2_pred = self.stage2_detector.predict(X_ambient)\n",
    "                stage2_proba = self.stage2_detector.predict(X_ambient, return_proba=True)\n",
    "                \n",
    "                # Update predictions\n",
    "                final_pred[ambient_mask] = stage2_pred\n",
    "                \n",
    "                # Update confidence (min of both stages for HUMAN)\n",
    "                human_in_stage2 = (stage2_pred == 'HUMAN')\n",
    "                if human_in_stage2.sum() > 0:\n",
    "                    stage1_conf = stage1_proba[ambient_mask][human_in_stage2].max(axis=1)\n",
    "                    stage2_conf = stage2_proba[human_in_stage2].max(axis=1)\n",
    "                    final_confidence[ambient_mask][human_in_stage2] = np.minimum(stage1_conf, stage2_conf)\n",
    "        \n",
    "        if return_confidence:\n",
    "            return final_pred, final_confidence\n",
    "        else:\n",
    "            return final_pred\n",
    "    \n",
    "    def predict_batch(self, features_df):\n",
    "        \"\"\"Predict with detailed output\"\"\"\n",
    "        \n",
    "        predictions, confidences = self.predict(features_df, return_confidence=True)\n",
    "        \n",
    "        results_df = features_df.copy()\n",
    "        results_df['predicted_category'] = predictions\n",
    "        results_df['confidence'] = confidences\n",
    "        \n",
    "        return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65f0d28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TWO-STAGE DETECTOR TRAINING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "📥 Loading clustered data...\n",
      "   ✅ 50,000 clips\n",
      "\n",
      "📥 Loading PCA features...\n",
      "   ✅ 50,000 clips\n",
      "\n",
      "🔗 Merging...\n",
      "   ✅ 15,392 unique clips\n",
      "\n",
      "================================================================================\n",
      "STAGE 1: AMBIENT vs BIO CLASSIFICATION\n",
      "================================================================================\n",
      "\n",
      "📊 Stage 1 Results:\n",
      "   AMBIENT : 14,524 ( 94.4%)\n",
      "   BIO     :    868 (  5.6%)\n",
      "\n",
      "================================================================================\n",
      "STAGE 2: HUMAN SOUND DETECTION\n",
      "================================================================================\n",
      "\n",
      "📊 AMBIENT clips from Stage 1: 14,524\n",
      "\n",
      "🔍 Searching for HUMAN sounds in 14,524 AMBIENT clips...\n",
      "\n",
      "📊 Human score distribution:\n",
      "   Mean: 0.000\n",
      "   Std:  0.000\n",
      "   Max:  0.000\n",
      "   Top 500 scores: 0.000 - 0.000\n",
      "\n",
      "📁 Top 10 HUMAN candidates (for manual verification):\n",
      "   /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/wav/PAPCA_test/2802/20080226/wav/47C3ED9D.wav (score: 0.000)\n",
      "   /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/wav/PAPCA_test/2802/20080226/wav/47C3E311.wav (score: 0.000)\n",
      "   /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/wav/PAPCA_test/2802/20080226/wav/47C3E695.wav (score: 0.000)\n",
      "   /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/wav/PAPCA_test/2802/20080226/wav/47C3F828.wav (score: 0.000)\n",
      "   /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/wav/PAPCA_test/2802/20080226/wav/47C3F121.wav (score: 0.000)\n",
      "   /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/wav/PAPCA_test/2802/20080226/wav/47C3FBAC.wav (score: 0.000)\n",
      "   /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/wav/PAPCA_test/2802/20080226/wav/47C3F4A5.wav (score: 0.000)\n",
      "   /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/wav/PAPCA_test/2802/20080226/wav/47C410C5.wav (score: 0.000)\n",
      "   /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/wav/PAPCA_test/2802/20080226/wav/47C41449.wav (score: 0.000)\n",
      "   /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/wav/PAPCA_test/2802/20080226/wav/47C40D41.wav (score: 0.000)\n",
      "\n",
      "📦 Creating HUMAN detector training data...\n",
      "\n",
      "   HUMAN samples: 500\n",
      "   AMBIENT samples: 1,000\n",
      "   Total: 1,500\n",
      "   Class balance: 33.3% HUMAN\n",
      "\n",
      "============================================================\n",
      "TRAINING STAGE 2: HUMAN DETECTOR\n",
      "============================================================\n",
      "\n",
      "  Training samples: 1,200\n",
      "  Test samples: 300\n",
      "\n",
      "  Training distribution:\n",
      "    AMBIENT: 800\n",
      "    HUMAN: 400\n",
      "\n",
      "  ✅ Test Accuracy: 0.7733\n",
      "\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     AMBIENT      0.888     0.755     0.816       200\n",
      "       HUMAN      0.623     0.810     0.704       100\n",
      "\n",
      "    accuracy                          0.773       300\n",
      "   macro avg      0.756     0.782     0.760       300\n",
      "weighted avg      0.800     0.773     0.779       300\n",
      "\n",
      "\n",
      "  Confusion Matrix:\n",
      "                Predicted\n",
      "                AMBIENT  HUMAN\n",
      "  Actual AMBIENT    151     49\n",
      "  Actual HUMAN       19     81\n",
      "\n",
      "================================================================================\n",
      "SAVING MODELS\n",
      "================================================================================\n",
      "\n",
      "  ✅ Stage 2 detector: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/two_stage_detector/models/stage2_human_detector.joblib\n",
      "\n",
      "================================================================================\n",
      "TWO-STAGE HIERARCHICAL DETECTOR\n",
      "================================================================================\n",
      "\n",
      "📥 Loading Stage 1 model (AMBIENT vs BIO)...\n",
      "   ✅ Loaded from: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/autolabeling_fixed/models/classifier.joblib\n",
      "   ✅ Stage 2 HUMAN detector ready\n",
      "  ✅ Complete pipeline: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/two_stage_detector/models/two_stage_pipeline.joblib\n",
      "\n",
      "================================================================================\n",
      "TESTING ON FULL DATASET\n",
      "================================================================================\n",
      "\n",
      "📊 Final Classification Results:\n",
      "   AMBIENT : 10,623 ( 69.0%)\n",
      "   HUMAN   :  3,901 ( 25.3%)\n",
      "   BIO     :    868 (  5.6%)\n",
      "\n",
      "  ✅ Results saved: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/two_stage_detector/results/two_stage_predictions.parquet\n",
      "  ✅ Summary: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/two_stage_detector/results/summary.txt\n",
      "\n",
      "================================================================================\n",
      "✅ TWO-STAGE DETECTOR COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "📦 Outputs:\n",
      "   • Stage 2 detector: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/two_stage_detector/models/stage2_human_detector.joblib\n",
      "   • Complete pipeline: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/two_stage_detector/models/two_stage_pipeline.joblib\n",
      "   • Predictions: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/two_stage_detector/results/two_stage_predictions.parquet\n",
      "\n",
      "🎯 Next steps:\n",
      "   1. Review HUMAN candidates manually\n",
      "   2. Listen to top-scored files\n",
      "   3. Refine rules if needed\n",
      "   4. Deploy for real-time monitoring\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Build and train two-stage detector\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TWO-STAGE DETECTOR TRAINING PIPELINE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create output directories\n",
    "    Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    Config.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    Config.RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # LOAD DATA\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LOADING DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load clustered data\n",
    "    print(f\"\\n📥 Loading clustered data...\")\n",
    "    clustered_df = pd.read_parquet(Config.CLUSTERED_DATA)\n",
    "    print(f\"   ✅ {len(clustered_df):,} clips\")\n",
    "    \n",
    "    # Load PCA features\n",
    "    print(f\"\\n📥 Loading PCA features...\")\n",
    "    pca_df = pd.read_parquet(Config.PREPROCESSED_DATA)\n",
    "    print(f\"   ✅ {len(pca_df):,} clips\")\n",
    "    \n",
    "    # Merge (proper deduplication)\n",
    "    print(f\"\\n🔗 Merging...\")\n",
    "    merged_df = clustered_df.merge(\n",
    "        pca_df,\n",
    "        on=['filepath', 'logger', 'date'],\n",
    "        how='inner'\n",
    "    )\n",
    "    merged_df = merged_df.drop_duplicates(subset=['filepath'])\n",
    "    print(f\"   ✅ {len(merged_df):,} unique clips\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE 1: RUN EXISTING MODEL\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STAGE 1: AMBIENT vs BIO CLASSIFICATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load Stage 1 model\n",
    "    stage1_model = joblib.load(Config.STAGE1_MODEL)\n",
    "    \n",
    "    # Predict\n",
    "    X = merged_df[Config.FEATURE_COLS].values\n",
    "    stage1_predictions = stage1_model.predict(X)\n",
    "    \n",
    "    merged_df['stage1_prediction'] = stage1_predictions\n",
    "    \n",
    "    print(f\"\\n📊 Stage 1 Results:\")\n",
    "    for category, count in pd.Series(stage1_predictions).value_counts().items():\n",
    "        pct = (count / len(stage1_predictions)) * 100\n",
    "        print(f\"   {category:8s}: {count:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE 2: BUILD HUMAN DETECTOR\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STAGE 2: HUMAN SOUND DETECTION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initialize HUMAN detector\n",
    "    human_detector = HumanSoundDetector()\n",
    "    \n",
    "    # Get AMBIENT clips from Stage 1\n",
    "    ambient_clips = merged_df[merged_df['stage1_prediction'] == 'AMBIENT']\n",
    "    print(f\"\\n📊 AMBIENT clips from Stage 1: {len(ambient_clips):,}\")\n",
    "    \n",
    "    # Find HUMAN candidates\n",
    "    human_candidates = human_detector.find_human_candidates(\n",
    "        ambient_clips,\n",
    "        top_n=min(500, int(len(ambient_clips) * 0.05))  # Top 5% or 500\n",
    "    )\n",
    "    \n",
    "    # Create training data\n",
    "    training_df = human_detector.create_training_data(\n",
    "        ambient_clips,\n",
    "        human_candidates,\n",
    "        negative_sample_size=len(human_candidates) * 2  # 1:2 ratio\n",
    "    )\n",
    "    \n",
    "    # Prepare features\n",
    "    X = training_df[Config.FEATURE_COLS].values\n",
    "    y = training_df['stage2_label'].values\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=Config.RANDOM_STATE,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train Stage 2\n",
    "    human_detector.train(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SAVE MODELS\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAVING MODELS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Save Stage 2 detector\n",
    "    stage2_path = Config.MODEL_DIR / \"stage2_human_detector.joblib\"\n",
    "    joblib.dump(human_detector, stage2_path)\n",
    "    print(f\"\\n  ✅ Stage 2 detector: {stage2_path}\")\n",
    "    \n",
    "    # Create and save complete pipeline\n",
    "    two_stage = TwoStageDetector(Config.STAGE1_MODEL, human_detector)\n",
    "    pipeline_path = Config.MODEL_DIR / \"two_stage_pipeline.joblib\"\n",
    "    joblib.dump(two_stage, pipeline_path)\n",
    "    print(f\"  ✅ Complete pipeline: {pipeline_path}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TEST ON FULL DATASET\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TESTING ON FULL DATASET\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Run two-stage prediction\n",
    "    final_predictions = two_stage.predict(merged_df)\n",
    "    \n",
    "    print(f\"\\n📊 Final Classification Results:\")\n",
    "    for category, count in pd.Series(final_predictions).value_counts().items():\n",
    "        pct = (count / len(final_predictions)) * 100\n",
    "        print(f\"   {category:8s}: {count:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Save results\n",
    "    merged_df['final_category'] = final_predictions\n",
    "    results_path = Config.RESULTS_DIR / \"two_stage_predictions.parquet\"\n",
    "    merged_df.to_parquet(results_path, index=False)\n",
    "    print(f\"\\n  ✅ Results saved: {results_path}\")\n",
    "    \n",
    "    # Summary\n",
    "    summary_path = Config.RESULTS_DIR / \"summary.txt\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(\"Two-Stage Detector Summary\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Total clips: {len(merged_df):,}\\n\\n\")\n",
    "        f.write(\"Stage 1 (AMBIENT vs BIO):\\n\")\n",
    "        for cat, count in merged_df['stage1_prediction'].value_counts().items():\n",
    "            f.write(f\"  {cat}: {count:,}\\n\")\n",
    "        f.write(\"\\nFinal (with HUMAN detection):\\n\")\n",
    "        for cat, count in merged_df['final_category'].value_counts().items():\n",
    "            f.write(f\"  {cat}: {count:,}\\n\")\n",
    "    \n",
    "    print(f\"  ✅ Summary: {summary_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✅ TWO-STAGE DETECTOR COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\n📦 Outputs:\")\n",
    "    print(f\"   • Stage 2 detector: {stage2_path}\")\n",
    "    print(f\"   • Complete pipeline: {pipeline_path}\")\n",
    "    print(f\"   • Predictions: {results_path}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Next steps:\")\n",
    "    print(f\"   1. Review HUMAN candidates manually\")\n",
    "    print(f\"   2. Listen to top-scored files\")\n",
    "    print(f\"   3. Refine rules if needed\")\n",
    "    print(f\"   4. Deploy for real-time monitoring\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reef_zmsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
