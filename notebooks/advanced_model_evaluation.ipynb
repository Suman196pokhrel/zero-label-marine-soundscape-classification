{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab94c09c-41bc-4fca-8920-d2e7f185958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd2204b0-ba08-4833-b31d-27f6e10c3697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration for validation tests\"\"\"\n",
    "    \n",
    "    ROOT_DIR = Path(\"~/Uni-stuff/semester-2/applied_Ml/reef_zmsc\").expanduser()\n",
    "    \n",
    "    # Test data\n",
    "    TEST_PREDICTIONS = ROOT_DIR / \"data/model_testing/results/test_predictions.csv\"\n",
    "    \n",
    "    # Models\n",
    "    STAGE1_MODEL = ROOT_DIR / \"data/autolabeling_fixed/models/classifier.joblib\"\n",
    "    \n",
    "    # Training data for Test 2\n",
    "    TRAINING_DATA = ROOT_DIR / \"data/autolabeling_fixed/results/cluster_labels.csv\"\n",
    "    CLUSTERED_DATA = ROOT_DIR / \"data/clustering/results_50k/clustered_data_kmeans.parquet\"\n",
    "    PREPROCESSED_DATA = ROOT_DIR / \"data/features/embeds_preprocessed_50k/preprocessed_features_pca.parquet\"\n",
    "    \n",
    "    # Output\n",
    "    OUTPUT_DIR = ROOT_DIR / \"data/model_validation\"\n",
    "    \n",
    "    FEATURE_COLS = [f\"pca_{i}\" for i in range(39)]\n",
    "    RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b0a2a4a-2eba-4470-b7cb-a649b1c2214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustnessTest:\n",
    "    \"\"\"\n",
    "    Test model robustness to audio perturbations\n",
    "    \n",
    "    A good model should predict the same category for:\n",
    "    - Original clip\n",
    "    - Slightly noisy version\n",
    "    - Slightly pitch-shifted version\n",
    "    - Slightly time-stretched version\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def add_noise(self, features, noise_level=0.05):\n",
    "        \"\"\"\n",
    "        Add Gaussian noise to features\n",
    "        \n",
    "        Args:\n",
    "            features: PCA features (n, 39)\n",
    "            noise_level: Std of noise relative to feature std\n",
    "        \n",
    "        Returns:\n",
    "            Noisy features\n",
    "        \"\"\"\n",
    "        noise = np.random.randn(*features.shape) * noise_level\n",
    "        return features + noise\n",
    "    \n",
    "    def scale_features(self, features, scale_factor=1.1):\n",
    "        \"\"\"\n",
    "        Scale features slightly (simulates volume/energy change)\n",
    "        \n",
    "        Args:\n",
    "            features: PCA features\n",
    "            scale_factor: Multiplication factor\n",
    "        \n",
    "        Returns:\n",
    "            Scaled features\n",
    "        \"\"\"\n",
    "        return features * scale_factor\n",
    "    \n",
    "    def perturb_features(self, features, perturbation_type='noise', strength=0.05):\n",
    "        \"\"\"\n",
    "        Apply various perturbations to features\n",
    "        \n",
    "        Args:\n",
    "            features: PCA features\n",
    "            perturbation_type: Type of perturbation\n",
    "            strength: Perturbation strength\n",
    "        \n",
    "        Returns:\n",
    "            Perturbed features\n",
    "        \"\"\"\n",
    "        \n",
    "        if perturbation_type == 'noise':\n",
    "            return self.add_noise(features, strength)\n",
    "        \n",
    "        elif perturbation_type == 'scale':\n",
    "            return self.scale_features(features, 1 + strength)\n",
    "        \n",
    "        elif perturbation_type == 'shift':\n",
    "            # Shift all features by small amount\n",
    "            return features + strength\n",
    "        \n",
    "        elif perturbation_type == 'dropout':\n",
    "            # Randomly set some features to 0 (like dropout)\n",
    "            mask = np.random.random(features.shape) > strength\n",
    "            return features * mask\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown perturbation: {perturbation_type}\")\n",
    "    \n",
    "    def test_consistency(self, features_df, n_tests=5):\n",
    "        \"\"\"\n",
    "        Test model consistency under multiple perturbations\n",
    "        \n",
    "        Args:\n",
    "            features_df: DataFrame with PCA features\n",
    "            n_tests: Number of perturbation tests per clip\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with consistency metrics\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"TEST 1: CONSISTENCY / ROBUSTNESS TESTING\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(f\"\\n📊 Testing {len(features_df)} clips with {n_tests} perturbations each\")\n",
    "        \n",
    "        X = features_df[Config.FEATURE_COLS].values\n",
    "        \n",
    "        # Original predictions\n",
    "        original_preds = self.model.predict(X)\n",
    "        \n",
    "        # Test different perturbation types\n",
    "        perturbation_types = ['noise', 'scale', 'shift', 'dropout']\n",
    "        results = {}\n",
    "        \n",
    "        for pert_type in perturbation_types:\n",
    "            print(f\"\\n{'─' * 60}\")\n",
    "            print(f\"Testing: {pert_type.upper()}\")\n",
    "            print(f\"{'─' * 60}\")\n",
    "            \n",
    "            agreements = []\n",
    "            \n",
    "            for strength in [0.01, 0.05, 0.10]:  # Different strengths\n",
    "                \n",
    "                # Run multiple tests\n",
    "                test_agreements = []\n",
    "                \n",
    "                for test_idx in range(n_tests):\n",
    "                    # Perturb features\n",
    "                    X_perturbed = self.perturb_features(X, pert_type, strength)\n",
    "                    \n",
    "                    # Predict\n",
    "                    perturbed_preds = self.model.predict(X_perturbed)\n",
    "                    \n",
    "                    # Check agreement\n",
    "                    agreement = (original_preds == perturbed_preds).mean()\n",
    "                    test_agreements.append(agreement)\n",
    "                \n",
    "                mean_agreement = np.mean(test_agreements)\n",
    "                std_agreement = np.std(test_agreements)\n",
    "                \n",
    "                agreements.append({\n",
    "                    'strength': strength,\n",
    "                    'mean': mean_agreement,\n",
    "                    'std': std_agreement\n",
    "                })\n",
    "                \n",
    "                print(f\"   Strength {strength:.2f}: {mean_agreement:.3f} ± {std_agreement:.3f} agreement\")\n",
    "            \n",
    "            results[pert_type] = agreements\n",
    "        \n",
    "        # Overall summary\n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"ROBUSTNESS SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        all_agreements = []\n",
    "        for pert_type, agreements in results.items():\n",
    "            for ag in agreements:\n",
    "                all_agreements.append(ag['mean'])\n",
    "        \n",
    "        overall_robustness = np.mean(all_agreements)\n",
    "        \n",
    "        print(f\"\\n📊 Overall Robustness Score: {overall_robustness:.3f}\")\n",
    "        \n",
    "        if overall_robustness > 0.95:\n",
    "            print(f\"   🌟 EXCELLENT! Model is very robust\")\n",
    "            print(f\"      Predictions are stable under perturbations\")\n",
    "        elif overall_robustness > 0.85:\n",
    "            print(f\"   ✅ GOOD! Model is reasonably robust\")\n",
    "            print(f\"      Most predictions remain stable\")\n",
    "        elif overall_robustness > 0.75:\n",
    "            print(f\"   ⚠️  FAIR. Model has moderate robustness\")\n",
    "            print(f\"      Some sensitivity to perturbations\")\n",
    "        else:\n",
    "            print(f\"   ❌ POOR. Model is not robust\")\n",
    "            print(f\"      Predictions change easily with small changes\")\n",
    "        \n",
    "        print(f\"\\n💡 Interpretation:\")\n",
    "        print(f\"   • High robustness (>0.9) = Model learned meaningful patterns\")\n",
    "        print(f\"   • Low robustness (<0.8) = Model may be overfitting to noise\")\n",
    "        \n",
    "        return {\n",
    "            'overall_robustness': overall_robustness,\n",
    "            'by_perturbation': results\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a92c974e-ff5d-4531-ac13-2506a9d27245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAgreementTest:\n",
    "    \"\"\"\n",
    "    Train multiple different models on same data\n",
    "    Check if they agree on predictions\n",
    "    \n",
    "    High agreement = Features and labels are good\n",
    "    Low agreement = Problems with data or labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.predictions = {}\n",
    "    \n",
    "    def create_models(self):\n",
    "        \"\"\"\n",
    "        Create diverse set of models\n",
    "        Different algorithms, different biases\n",
    "        \"\"\"\n",
    "        \n",
    "        models = {\n",
    "            'Logistic Regression': LogisticRegression(\n",
    "                penalty='l2',\n",
    "                solver='lbfgs',\n",
    "                max_iter=500,\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                multi_class='multinomial'\n",
    "            ),\n",
    "            \n",
    "            'Random Forest': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=10,\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            \n",
    "            'Gradient Boosting': GradientBoostingClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.1,\n",
    "                random_state=Config.RANDOM_STATE\n",
    "            ),\n",
    "            \n",
    "            'SVM': SVC(\n",
    "                kernel='rbf',\n",
    "                C=1.0,\n",
    "                random_state=Config.RANDOM_STATE\n",
    "            ),\n",
    "            \n",
    "            'Neural Network': MLPClassifier(\n",
    "                hidden_layer_sizes=(50, 30),\n",
    "                max_iter=500,\n",
    "                random_state=Config.RANDOM_STATE\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def train_all_models(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Train all models on same data\n",
    "        \n",
    "        Args:\n",
    "            X_train, y_train: Training data\n",
    "            X_test, y_test: Test data\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all trained models\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"TEST 2: MODEL AGREEMENT TESTING\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(f\"\\n📊 Training {len(self.create_models())} different models...\")\n",
    "        print(f\"   Training samples: {len(X_train):,}\")\n",
    "        print(f\"   Test samples: {len(X_test):,}\")\n",
    "        \n",
    "        models = self.create_models()\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"\\n{'─' * 60}\")\n",
    "            print(f\"Training: {name}\")\n",
    "            print(f\"{'─' * 60}\")\n",
    "            \n",
    "            # Train\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict\n",
    "            train_pred = model.predict(X_train)\n",
    "            test_pred = model.predict(X_test)\n",
    "            \n",
    "            # Evaluate\n",
    "            train_acc = accuracy_score(y_train, train_pred)\n",
    "            test_acc = accuracy_score(y_test, test_pred)\n",
    "            \n",
    "            print(f\"   Train accuracy: {train_acc:.4f}\")\n",
    "            print(f\"   Test accuracy:  {test_acc:.4f}\")\n",
    "            \n",
    "            if test_acc < train_acc - 0.1:\n",
    "                print(f\"   ⚠️  Significant drop (overfitting?)\")\n",
    "            else:\n",
    "                print(f\"   ✅ Good generalization\")\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'train_acc': train_acc,\n",
    "                'test_acc': test_acc,\n",
    "                'train_pred': train_pred,\n",
    "                'test_pred': test_pred\n",
    "            }\n",
    "        \n",
    "        self.models = results\n",
    "        return results\n",
    "    \n",
    "    def analyze_agreement(self, X_test):\n",
    "        \"\"\"\n",
    "        Analyze agreement between different models\n",
    "        \n",
    "        Args:\n",
    "            X_test: Test features\n",
    "        \n",
    "        Returns:\n",
    "            Agreement metrics\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"MODEL AGREEMENT ANALYSIS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Get all predictions\n",
    "        model_names = list(self.models.keys())\n",
    "        all_predictions = {name: self.models[name]['test_pred'] \n",
    "                          for name in model_names}\n",
    "        \n",
    "        # Pairwise agreement\n",
    "        print(f\"\\n📊 Pairwise Agreement Matrix:\")\n",
    "        print(f\"\\n{'Model':<20}\", end='')\n",
    "        for name in model_names:\n",
    "            print(f\"{name[:12]:<14}\", end='')\n",
    "        print()\n",
    "        print(f\"{'-' * (20 + 14 * len(model_names))}\")\n",
    "        \n",
    "        agreement_matrix = np.zeros((len(model_names), len(model_names)))\n",
    "        \n",
    "        for i, name1 in enumerate(model_names):\n",
    "            print(f\"{name1:<20}\", end='')\n",
    "            \n",
    "            for j, name2 in enumerate(model_names):\n",
    "                agreement = (all_predictions[name1] == all_predictions[name2]).mean()\n",
    "                agreement_matrix[i, j] = agreement\n",
    "                \n",
    "                if i == j:\n",
    "                    print(f\"{'1.000':<14}\", end='')\n",
    "                else:\n",
    "                    print(f\"{agreement:.3f}{' ':<9}\", end='')\n",
    "            print()\n",
    "        \n",
    "        # Overall agreement\n",
    "        upper_triangle = agreement_matrix[np.triu_indices_from(agreement_matrix, k=1)]\n",
    "        mean_agreement = upper_triangle.mean()\n",
    "        \n",
    "        print(f\"\\n📊 Overall Model Agreement: {mean_agreement:.3f}\")\n",
    "        \n",
    "        # Interpretation\n",
    "        print(f\"\\n💡 Interpretation:\")\n",
    "        if mean_agreement > 0.95:\n",
    "            print(f\"   🌟 EXCELLENT! Models strongly agree\")\n",
    "            print(f\"      → Features are highly informative\")\n",
    "            print(f\"      → Pseudo-labels are very reliable\")\n",
    "            print(f\"      → High confidence in predictions\")\n",
    "        elif mean_agreement > 0.85:\n",
    "            print(f\"   ✅ GOOD! Models mostly agree\")\n",
    "            print(f\"      → Features capture meaningful patterns\")\n",
    "            print(f\"      → Pseudo-labels are reasonable\")\n",
    "            print(f\"      → Confident in most predictions\")\n",
    "        elif mean_agreement > 0.75:\n",
    "            print(f\"   ⚠️  FAIR. Moderate agreement\")\n",
    "            print(f\"      → Some ambiguity in data\")\n",
    "            print(f\"      → Pseudo-labels may have errors\")\n",
    "            print(f\"      → Verify disagreement cases\")\n",
    "        else:\n",
    "            print(f\"   ❌ POOR. Models disagree significantly\")\n",
    "            print(f\"      → Features may not be informative enough\")\n",
    "            print(f\"      → Pseudo-labels may be unreliable\")\n",
    "            print(f\"      → Need better training data\")\n",
    "        \n",
    "        # Find disagreement cases\n",
    "        print(f\"\\n🔍 Analyzing disagreements...\")\n",
    "        \n",
    "        # For each test sample, count how many models agree\n",
    "        n_samples = len(X_test)\n",
    "        agreement_counts = np.zeros(n_samples)\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Get predictions from all models for this sample\n",
    "            sample_preds = [all_predictions[name][i] for name in model_names]\n",
    "            \n",
    "            # Count most common prediction\n",
    "            from collections import Counter\n",
    "            counts = Counter(sample_preds)\n",
    "            most_common_count = counts.most_common(1)[0][1]\n",
    "            \n",
    "            agreement_counts[i] = most_common_count / len(model_names)\n",
    "        \n",
    "        # Classify samples by agreement level\n",
    "        unanimous = (agreement_counts == 1.0).sum()\n",
    "        majority = ((agreement_counts >= 0.6) & (agreement_counts < 1.0)).sum()\n",
    "        split = (agreement_counts < 0.6).sum()\n",
    "        \n",
    "        print(f\"\\n   Unanimous (all models agree): {unanimous} ({unanimous/n_samples*100:.1f}%)\")\n",
    "        print(f\"   Majority (≥60% agree):         {majority} ({majority/n_samples*100:.1f}%)\")\n",
    "        print(f\"   Split (<60% agree):            {split} ({split/n_samples*100:.1f}%)\")\n",
    "        \n",
    "        if split > 0:\n",
    "            print(f\"\\n   ⚠️  {split} samples have low agreement\")\n",
    "            print(f\"      These are ambiguous cases - check manually\")\n",
    "        \n",
    "        # Cohen's Kappa (inter-rater reliability)\n",
    "        print(f\"\\n📊 Cohen's Kappa (Inter-Model Agreement):\")\n",
    "        kappas = []\n",
    "        for i, name1 in enumerate(model_names):\n",
    "            for j, name2 in enumerate(model_names):\n",
    "                if i < j:\n",
    "                    kappa = cohen_kappa_score(all_predictions[name1], all_predictions[name2])\n",
    "                    kappas.append(kappa)\n",
    "                    print(f\"   {name1[:15]:15s} vs {name2[:15]:15s}: κ = {kappa:.3f}\")\n",
    "        \n",
    "        mean_kappa = np.mean(kappas)\n",
    "        print(f\"\\n   Mean Cohen's Kappa: {mean_kappa:.3f}\")\n",
    "        \n",
    "        if mean_kappa > 0.8:\n",
    "            print(f\"   🌟 Almost perfect agreement\")\n",
    "        elif mean_kappa > 0.6:\n",
    "            print(f\"   ✅ Substantial agreement\")\n",
    "        elif mean_kappa > 0.4:\n",
    "            print(f\"   ⚠️  Moderate agreement\")\n",
    "        else:\n",
    "            print(f\"   ❌ Slight/poor agreement\")\n",
    "        \n",
    "        return {\n",
    "            'mean_agreement': mean_agreement,\n",
    "            'agreement_matrix': agreement_matrix,\n",
    "            'unanimous': unanimous,\n",
    "            'majority': majority,\n",
    "            'split': split,\n",
    "            'mean_kappa': mean_kappa\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a11d5c6-4930-4666-bec3-55c81dace76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🧪 ADVANCED MODEL VALIDATION\n",
      "No ground truth labels required!\n",
      "================================================================================\n",
      "\n",
      "📥 Loading data...\n",
      "   Test clips: 20\n",
      "   Training clips: 15392\n",
      "\n",
      "================================================================================\n",
      "TEST 1: CONSISTENCY / ROBUSTNESS TESTING\n",
      "================================================================================\n",
      "\n",
      "📊 Testing 20 clips with 5 perturbations each\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Testing: NOISE\n",
      "────────────────────────────────────────────────────────────\n",
      "   Strength 0.01: 1.000 ± 0.000 agreement\n",
      "   Strength 0.05: 1.000 ± 0.000 agreement\n",
      "   Strength 0.10: 1.000 ± 0.000 agreement\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Testing: SCALE\n",
      "────────────────────────────────────────────────────────────\n",
      "   Strength 0.01: 1.000 ± 0.000 agreement\n",
      "   Strength 0.05: 1.000 ± 0.000 agreement\n",
      "   Strength 0.10: 1.000 ± 0.000 agreement\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Testing: SHIFT\n",
      "────────────────────────────────────────────────────────────\n",
      "   Strength 0.01: 1.000 ± 0.000 agreement\n",
      "   Strength 0.05: 1.000 ± 0.000 agreement\n",
      "   Strength 0.10: 1.000 ± 0.000 agreement\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Testing: DROPOUT\n",
      "────────────────────────────────────────────────────────────\n",
      "   Strength 0.01: 1.000 ± 0.000 agreement\n",
      "   Strength 0.05: 0.990 ± 0.020 agreement\n",
      "   Strength 0.10: 0.980 ± 0.024 agreement\n",
      "\n",
      "================================================================================\n",
      "ROBUSTNESS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "📊 Overall Robustness Score: 0.998\n",
      "   🌟 EXCELLENT! Model is very robust\n",
      "      Predictions are stable under perturbations\n",
      "\n",
      "💡 Interpretation:\n",
      "   • High robustness (>0.9) = Model learned meaningful patterns\n",
      "   • Low robustness (<0.8) = Model may be overfitting to noise\n",
      "\n",
      "================================================================================\n",
      "TEST 2: MODEL AGREEMENT TESTING\n",
      "================================================================================\n",
      "\n",
      "📊 Training 5 different models...\n",
      "   Training samples: 12,313\n",
      "   Test samples: 3,079\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Training: Logistic Regression\n",
      "────────────────────────────────────────────────────────────\n",
      "   Train accuracy: 0.9999\n",
      "   Test accuracy:  0.9990\n",
      "   ✅ Good generalization\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Training: Random Forest\n",
      "────────────────────────────────────────────────────────────\n",
      "   Train accuracy: 1.0000\n",
      "   Test accuracy:  0.9997\n",
      "   ✅ Good generalization\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Training: Gradient Boosting\n",
      "────────────────────────────────────────────────────────────\n",
      "   Train accuracy: 1.0000\n",
      "   Test accuracy:  0.9990\n",
      "   ✅ Good generalization\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Training: SVM\n",
      "────────────────────────────────────────────────────────────\n",
      "   Train accuracy: 0.9999\n",
      "   Test accuracy:  0.9997\n",
      "   ✅ Good generalization\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Training: Neural Network\n",
      "────────────────────────────────────────────────────────────\n",
      "   Train accuracy: 0.9998\n",
      "   Test accuracy:  0.9981\n",
      "   ✅ Good generalization\n",
      "\n",
      "================================================================================\n",
      "MODEL AGREEMENT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "📊 Pairwise Agreement Matrix:\n",
      "\n",
      "Model               Logistic Reg  Random Fores  Gradient Boo  SVM           Neural Netwo  \n",
      "------------------------------------------------------------------------------------------\n",
      "Logistic Regression 1.000         0.999         0.999         0.999         0.998         \n",
      "Random Forest       0.999         1.000         0.999         1.000         0.998         \n",
      "Gradient Boosting   0.999         0.999         1.000         0.999         0.997         \n",
      "SVM                 0.999         1.000         0.999         1.000         0.998         \n",
      "Neural Network      0.998         0.998         0.997         0.998         1.000         \n",
      "\n",
      "📊 Overall Model Agreement: 0.999\n",
      "\n",
      "💡 Interpretation:\n",
      "   🌟 EXCELLENT! Models strongly agree\n",
      "      → Features are highly informative\n",
      "      → Pseudo-labels are very reliable\n",
      "      → High confidence in predictions\n",
      "\n",
      "🔍 Analyzing disagreements...\n",
      "\n",
      "   Unanimous (all models agree): 3070 (99.7%)\n",
      "   Majority (≥60% agree):         9 (0.3%)\n",
      "   Split (<60% agree):            0 (0.0%)\n",
      "\n",
      "📊 Cohen's Kappa (Inter-Model Agreement):\n",
      "   Logistic Regres vs Random Forest  : κ = 0.988\n",
      "   Logistic Regres vs Gradient Boosti: κ = 0.994\n",
      "   Logistic Regres vs SVM            : κ = 0.988\n",
      "   Logistic Regres vs Neural Network : κ = 0.979\n",
      "   Random Forest   vs Gradient Boosti: κ = 0.994\n",
      "   Random Forest   vs SVM            : κ = 1.000\n",
      "   Random Forest   vs Neural Network : κ = 0.979\n",
      "   Gradient Boosti vs SVM            : κ = 0.994\n",
      "   Gradient Boosti vs Neural Network : κ = 0.973\n",
      "   SVM             vs Neural Network : κ = 0.979\n",
      "\n",
      "   Mean Cohen's Kappa: 0.987\n",
      "   🌟 Almost perfect agreement\n",
      "\n",
      "================================================================================\n",
      "🎯 COMBINED VALIDATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "📊 Validation Scores:\n",
      "   Robustness:     0.998 (consistency under perturbations)\n",
      "   Model Agreement: 0.999 (different models agree)\n",
      "\n",
      "   Combined Confidence Score: 0.998\n",
      "\n",
      "💡 Overall Assessment:\n",
      "   🌟 EXCELLENT VALIDATION\n",
      "      ✅ Model is robust\n",
      "      ✅ Multiple models agree\n",
      "      ✅ High confidence in predictions\n",
      "      → Ready for deployment!\n",
      "\n",
      "💾 Results saved: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/model_validation/validation_summary.txt\n",
      "\n",
      "================================================================================\n",
      "✅ VALIDATION COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def run_validation_tests():\n",
    "    \"\"\"Run both validation tests\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🧪 ADVANCED MODEL VALIDATION\")\n",
    "    print(\"No ground truth labels required!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # LOAD DATA\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n📥 Loading data...\")\n",
    "    \n",
    "    # Test data (for Test 1)\n",
    "    test_predictions = pd.read_csv(Config.TEST_PREDICTIONS)\n",
    "    print(f\"   Test clips: {len(test_predictions)}\")\n",
    "    \n",
    "    # Training data (for Test 2)\n",
    "    cluster_labels = pd.read_csv(Config.TRAINING_DATA)\n",
    "    clustered_df = pd.read_parquet(Config.CLUSTERED_DATA)\n",
    "    pca_df = pd.read_parquet(Config.PREPROCESSED_DATA)\n",
    "    \n",
    "    # Merge training data\n",
    "    training_df = clustered_df.merge(cluster_labels[['cluster', 'category']], on='cluster')\n",
    "    training_df = training_df.merge(pca_df, on=['filepath', 'logger', 'date'], how='inner')\n",
    "    training_df = training_df.drop_duplicates(subset=['filepath'])\n",
    "    \n",
    "    # Filter high confidence\n",
    "    training_df = training_df[training_df['category'].isin(['AMBIENT', 'BIO'])]\n",
    "    \n",
    "    print(f\"   Training clips: {len(training_df)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TEST 1: ROBUSTNESS\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Load model\n",
    "    model = joblib.load(Config.STAGE1_MODEL)\n",
    "    \n",
    "    robustness_test = RobustnessTest(model)\n",
    "    robustness_results = robustness_test.test_consistency(test_predictions, n_tests=5)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TEST 2: MODEL AGREEMENT\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Prepare training/test split\n",
    "    X = training_df[Config.FEATURE_COLS].values\n",
    "    y = training_df['category'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=Config.RANDOM_STATE,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    agreement_test = ModelAgreementTest()\n",
    "    model_results = agreement_test.train_all_models(X_train, y_train, X_test, y_test)\n",
    "    agreement_results = agreement_test.analyze_agreement(X_test)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # COMBINED SUMMARY\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎯 COMBINED VALIDATION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    robustness_score = robustness_results['overall_robustness']\n",
    "    agreement_score = agreement_results['mean_agreement']\n",
    "    \n",
    "    print(f\"\\n📊 Validation Scores:\")\n",
    "    print(f\"   Robustness:     {robustness_score:.3f} (consistency under perturbations)\")\n",
    "    print(f\"   Model Agreement: {agreement_score:.3f} (different models agree)\")\n",
    "    \n",
    "    # Combined confidence\n",
    "    combined_score = (robustness_score + agreement_score) / 2\n",
    "    print(f\"\\n   Combined Confidence Score: {combined_score:.3f}\")\n",
    "    \n",
    "    print(f\"\\n💡 Overall Assessment:\")\n",
    "    if combined_score > 0.9:\n",
    "        print(f\"   🌟 EXCELLENT VALIDATION\")\n",
    "        print(f\"      ✅ Model is robust\")\n",
    "        print(f\"      ✅ Multiple models agree\")\n",
    "        print(f\"      ✅ High confidence in predictions\")\n",
    "        print(f\"      → Ready for deployment!\")\n",
    "    elif combined_score > 0.8:\n",
    "        print(f\"   ✅ GOOD VALIDATION\")\n",
    "        print(f\"      ✅ Model is reasonably robust\")\n",
    "        print(f\"      ✅ Models mostly agree\")\n",
    "        print(f\"      → Suitable for most applications\")\n",
    "    elif combined_score > 0.7:\n",
    "        print(f\"   ⚠️  FAIR VALIDATION\")\n",
    "        print(f\"      ⚠️  Some robustness issues\")\n",
    "        print(f\"      ⚠️  Some model disagreement\")\n",
    "        print(f\"      → Verify critical predictions manually\")\n",
    "    else:\n",
    "        print(f\"   ❌ POOR VALIDATION\")\n",
    "        print(f\"      ❌ Model is not robust\")\n",
    "        print(f\"      ❌ Models disagree significantly\")\n",
    "        print(f\"      → Need to improve training data or features\")\n",
    "    \n",
    "    # Save results\n",
    "    summary_path = Config.OUTPUT_DIR / \"validation_summary.txt\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(\"Model Validation Summary\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        f.write(f\"Robustness Score: {robustness_score:.3f}\\n\")\n",
    "        f.write(f\"Model Agreement Score: {agreement_score:.3f}\\n\")\n",
    "        f.write(f\"Combined Confidence: {combined_score:.3f}\\n\\n\")\n",
    "        f.write(f\"Test 1 - Robustness:\\n\")\n",
    "        f.write(f\"  Model predictions remain stable under perturbations\\n\\n\")\n",
    "        f.write(f\"Test 2 - Model Agreement:\\n\")\n",
    "        f.write(f\"  {len(model_results)} different models trained\\n\")\n",
    "        f.write(f\"  Mean agreement: {agreement_score:.3f}\\n\")\n",
    "        f.write(f\"  Mean Cohen's Kappa: {agreement_results['mean_kappa']:.3f}\\n\\n\")\n",
    "        f.write(f\"Unanimous predictions: {agreement_results['unanimous']}/{len(X_test)}\\n\")\n",
    "        f.write(f\"Majority predictions: {agreement_results['majority']}/{len(X_test)}\\n\")\n",
    "        f.write(f\"Split predictions: {agreement_results['split']}/{len(X_test)}\\n\")\n",
    "    \n",
    "    print(f\"\\n💾 Results saved: {summary_path}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"✅ VALIDATION COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return {\n",
    "        'robustness': robustness_results,\n",
    "        'agreement': agreement_results,\n",
    "        'combined_score': combined_score\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_validation_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
