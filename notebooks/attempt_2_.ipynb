{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c677569",
   "metadata": {},
   "source": [
    "## After we get clip_manifest.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310447f8",
   "metadata": {},
   "source": [
    "### Inspecting the clip_manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "634f14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c88105ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Absolute roots ===\n",
    "HOME = Path(os.environ[\"HOME\"])\n",
    "REPO_ROOT = HOME / \"Uni-stuff/semester-2/applied_Ml/reef_zmsc\"\n",
    "\n",
    "# === Paths ===\n",
    "IN_MANIFEST = REPO_ROOT / \"data/manifests/clip_manifest.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4dfaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_manifest(manifest_path):\n",
    "    \"\"\"Load and display basic info about the manifest\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"LOADING MANIFEST\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = pd.read_parquet(manifest_path)\n",
    "    \n",
    "    print(f\"\\n✓ Loaded manifest with {len(df):,} clips\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "    print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def check_required_columns(df):\n",
    "    \"\"\"Verify essential columns exist\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CHECKING REQUIRED COLUMNS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    required = ['clip_id', 'filepath', 'start_time']\n",
    "    optional = ['end_time', 'duration', 'logger_id', 'date', 'hour']\n",
    "    \n",
    "    missing = [col for col in required if col not in df.columns]\n",
    "    present_optional = [col for col in optional if col in df.columns]\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"\\n✗ Missing required columns: {missing}\")\n",
    "        print(\"\\nExpected columns for this analysis:\")\n",
    "        print(\"  - clip_id: Unique identifier\")\n",
    "        print(\"  - filepath: Path to audio file\")\n",
    "        print(\"  - start_time: Timestamp (for temporal stratification)\")\n",
    "    else:\n",
    "        print(f\"\\n✓ All required columns present\")\n",
    "    \n",
    "    if present_optional:\n",
    "        print(f\"\\n✓ Optional columns found: {present_optional}\")\n",
    "    \n",
    "    return len(missing) == 0\n",
    "\n",
    "def inspect_timestamps(df):\n",
    "    \"\"\"Analyze temporal distribution\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEMPORAL ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if 'start_time' not in df.columns:\n",
    "        print(\"\\n✗ No 'start_time' column - cannot analyze temporal patterns\")\n",
    "        return None\n",
    "    \n",
    "    # Try to parse timestamps\n",
    "    try:\n",
    "        df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "        print(\"\\n✓ Timestamps parsed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error parsing timestamps: {e}\")\n",
    "        print(f\"\\nSample values:\\n{df['start_time'].head()}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract temporal features\n",
    "    df['date'] = df['start_time'].dt.date\n",
    "    df['hour'] = df['start_time'].dt.hour\n",
    "    df['day_of_week'] = df['start_time'].dt.day_name()\n",
    "    \n",
    "    # Basic stats\n",
    "    print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    print(f\"Total days: {df['date'].nunique()}\")\n",
    "    print(f\"Total hours covered: {(df['start_time'].max() - df['start_time'].min()).total_seconds() / 3600:.1f}\")\n",
    "    \n",
    "    # Hourly distribution\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"HOURLY DISTRIBUTION\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    hourly_counts = df['hour'].value_counts().sort_index()\n",
    "    print(\"\\nClips per hour:\")\n",
    "    for hour, count in hourly_counts.items():\n",
    "        bar = '█' * int(count / hourly_counts.max() * 50)\n",
    "        print(f\"  {hour:02d}:00  {count:>7,}  {bar}\")\n",
    "    \n",
    "    # Identify biological windows\n",
    "    dawn_clips = df[df['hour'].between(4, 6)].shape[0]\n",
    "    dusk_clips = df[df['hour'].between(17, 19)].shape[0]\n",
    "    total_bio_windows = dawn_clips + dusk_clips\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"BIOLOGICAL ACTIVITY WINDOWS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"\\nDawn chorus (04:00-06:59): {dawn_clips:,} clips ({dawn_clips/len(df)*100:.1f}%)\")\n",
    "    print(f\"Dusk chorus (17:00-19:59): {dusk_clips:,} clips ({dusk_clips/len(df)*100:.1f}%)\")\n",
    "    print(f\"Total biological windows:   {total_bio_windows:,} clips ({total_bio_windows/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    if total_bio_windows / len(df) < 0.15:\n",
    "        print(\"\\n⚠ WARNING: <15% of clips in dawn/dusk windows\")\n",
    "        print(\"  Random sampling will likely miss biological activity!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def check_file_paths(df, sample_size=10):\n",
    "    \"\"\"Verify file paths exist\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FILE PATH VALIDATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if 'filepath' not in df.columns:\n",
    "        print(\"\\n✗ No 'filepath' column found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nChecking sample of {sample_size} file paths...\")\n",
    "    \n",
    "    sample_paths = df['filepath'].sample(min(sample_size, len(df)))\n",
    "    existing = 0\n",
    "    missing = 0\n",
    "    \n",
    "    for path in sample_paths:\n",
    "        if Path(path).exists():\n",
    "            existing += 1\n",
    "        else:\n",
    "            missing += 1\n",
    "            if missing == 1:  # Show first missing file as example\n",
    "                print(f\"\\nExample path: {path}\")\n",
    "                print(f\"Exists: {Path(path).exists()}\")\n",
    "    \n",
    "    print(f\"\\n✓ Existing files: {existing}/{sample_size}\")\n",
    "    if missing > 0:\n",
    "        print(f\"✗ Missing files: {missing}/{sample_size}\")\n",
    "        print(\"\\nNote: Files may be on a different machine/mount\")\n",
    "    else:\n",
    "        print(\"\\n✓ All sampled files exist\")\n",
    "\n",
    "def analyze_logger_distribution(df):\n",
    "    \"\"\"Check if multiple loggers/deployments\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LOGGER/DEPLOYMENT ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    logger_cols = [col for col in df.columns if 'logger' in col.lower() or 'deployment' in col.lower()]\n",
    "    \n",
    "    if not logger_cols:\n",
    "        print(\"\\n⚠ No logger/deployment columns found\")\n",
    "        print(\"  Assuming single deployment\")\n",
    "        return\n",
    "    \n",
    "    for col in logger_cols:\n",
    "        unique_vals = df[col].nunique()\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Unique values: {unique_vals}\")\n",
    "        if unique_vals < 10:\n",
    "            print(f\"  Distribution:\\n{df[col].value_counts()}\")\n",
    "\n",
    "def calculate_sampling_recommendations(df):\n",
    "    \"\"\"Suggest sampling strategy based on data\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAMPLING RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if 'hour' not in df.columns:\n",
    "        print(\"\\n⚠ Cannot provide temporal recommendations without hour data\")\n",
    "        return\n",
    "    \n",
    "    total_clips = len(df)\n",
    "    \n",
    "    # Define temporal bins\n",
    "    bins = {\n",
    "        'dawn_chorus': (4, 7),\n",
    "        'morning': (7, 12),\n",
    "        'midday': (12, 16),\n",
    "        'dusk_chorus': (17, 20),\n",
    "        'night': (20, 24),\n",
    "        'pre_dawn': (0, 4)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nFor 20,000 clip sample:\")\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(f\"{'Temporal Bin':<20} {'Available':>12} {'Random':>10} {'Stratified':>10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for bin_name, (start, end) in bins.items():\n",
    "        available = df[df['hour'].between(start, end-1)].shape[0]\n",
    "        random_expected = int(20000 * (available / total_clips))\n",
    "        \n",
    "        # Stratified suggestion (oversample biological windows)\n",
    "        if 'chorus' in bin_name:\n",
    "            stratified = 5000  # 25% each for dawn/dusk\n",
    "        elif bin_name == 'morning' or bin_name == 'night':\n",
    "            stratified = 3000  # 15% each\n",
    "        else:\n",
    "            stratified = 2000  # 10% each\n",
    "        \n",
    "        print(f\"{bin_name:<20} {available:>12,} {random_expected:>10,} {stratified:>10,}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\nKey insight:\")\n",
    "    print(\"  • Random sampling: ~17% in dawn/dusk (proportional to data)\")\n",
    "    print(\"  • Stratified sampling: 50% in dawn/dusk (biological focus)\")\n",
    "    print(\"\\nRecommendation: Use STRATIFIED sampling to capture biological activity\")\n",
    "\n",
    "def generate_summary_report(df):\n",
    "    \"\"\"Create summary statistics\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SUMMARY REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nTotal clips: {len(df):,}\")\n",
    "    \n",
    "    if 'start_time' in df.columns and pd.api.types.is_datetime64_any_dtype(df['start_time']):\n",
    "        duration_days = (df['start_time'].max() - df['start_time'].min()).days\n",
    "        print(f\"Time span: {duration_days} days\")\n",
    "        print(f\"Clips per day: {len(df) / max(duration_days, 1):,.0f}\")\n",
    "    \n",
    "    if 'duration' in df.columns:\n",
    "        total_hours = df['duration'].sum() / 3600\n",
    "        print(f\"Total audio: {total_hours:,.1f} hours\")\n",
    "    elif 'start_time' in df.columns:\n",
    "        # Assume 10s clips\n",
    "        total_hours = len(df) * 10 / 3600\n",
    "        print(f\"Total audio (estimated): {total_hours:,.1f} hours\")\n",
    "    \n",
    "    print(\"\\nData quality:\")\n",
    "    print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"  Duplicate clip_ids: {df['clip_id'].duplicated().sum() if 'clip_id' in df.columns else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4549c2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting manifest at: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/manifests/clip_manifest.parquet\n",
      "\n",
      "================================================================================\n",
      "LOADING MANIFEST\n",
      "================================================================================\n",
      "\n",
      "✓ Loaded manifest with 1,053,610 clips\n",
      "\n",
      "Columns: ['clip_id', 'filepath', 'logger', 'date', 'clip_index', 'start_time', 'timestamp_source', 'start_s', 'end_s', 'duration_s', 'sr', 'channels', 'start_frame', 'end_frame', 'n_frames', 'overload_flags', 'first_data_time_utc', 'finalised_time_utc', 'rec_start_time_utc', 'rms']\n",
      "\n",
      "Data types:\n",
      "clip_id                 object\n",
      "filepath                object\n",
      "logger                  object\n",
      "date                    object\n",
      "clip_index               int64\n",
      "start_time              object\n",
      "timestamp_source        object\n",
      "start_s                float64\n",
      "end_s                  float64\n",
      "duration_s             float64\n",
      "sr                       int64\n",
      "channels                 int64\n",
      "start_frame              int64\n",
      "end_frame                int64\n",
      "n_frames                 int64\n",
      "overload_flags          object\n",
      "first_data_time_utc     object\n",
      "finalised_time_utc      object\n",
      "rec_start_time_utc      object\n",
      "rms                    float64\n",
      "dtype: object\n",
      "\n",
      "Memory usage: 871.16 MB\n",
      "\n",
      "================================================================================\n",
      "CHECKING REQUIRED COLUMNS\n",
      "================================================================================\n",
      "\n",
      "✓ All required columns present\n",
      "\n",
      "✓ Optional columns found: ['date']\n",
      "\n",
      "================================================================================\n",
      "TEMPORAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "✓ Timestamps parsed successfully\n",
      "\n",
      "Date range: 2008-02-26 to 2009-10-11\n",
      "Total days: 271\n",
      "Total hours covered: 14242.9\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "HOURLY DISTRIBUTION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Clips per hour:\n",
      "  00:00   44,221  █████████████████████████████████████████████████\n",
      "  01:00   44,179  █████████████████████████████████████████████████\n",
      "  02:00   44,140  █████████████████████████████████████████████████\n",
      "  03:00   44,000  █████████████████████████████████████████████████\n",
      "  04:00   44,030  █████████████████████████████████████████████████\n",
      "  05:00   43,820  █████████████████████████████████████████████████\n",
      "  06:00   43,840  █████████████████████████████████████████████████\n",
      "  07:00   43,680  █████████████████████████████████████████████████\n",
      "  08:00   43,500  ████████████████████████████████████████████████\n",
      "  09:00   43,760  █████████████████████████████████████████████████\n",
      "  10:00   43,500  ████████████████████████████████████████████████\n",
      "  11:00   43,490  ████████████████████████████████████████████████\n",
      "  12:00   43,680  █████████████████████████████████████████████████\n",
      "  13:00   43,820  █████████████████████████████████████████████████\n",
      "  14:00   43,860  █████████████████████████████████████████████████\n",
      "  15:00   43,790  █████████████████████████████████████████████████\n",
      "  16:00   43,680  █████████████████████████████████████████████████\n",
      "  17:00   43,680  █████████████████████████████████████████████████\n",
      "  18:00   43,910  █████████████████████████████████████████████████\n",
      "  19:00   44,130  █████████████████████████████████████████████████\n",
      "  20:00   44,070  █████████████████████████████████████████████████\n",
      "  21:00   43,851  █████████████████████████████████████████████████\n",
      "  22:00   44,519  ██████████████████████████████████████████████████\n",
      "  23:00   44,460  █████████████████████████████████████████████████\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "BIOLOGICAL ACTIVITY WINDOWS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Dawn chorus (04:00-06:59): 131,690 clips (12.5%)\n",
      "Dusk chorus (17:00-19:59): 131,720 clips (12.5%)\n",
      "Total biological windows:   263,410 clips (25.0%)\n",
      "\n",
      "================================================================================\n",
      "FILE PATH VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Checking sample of 10 file paths...\n",
      "\n",
      "✓ Existing files: 10/10\n",
      "\n",
      "✓ All sampled files exist\n",
      "\n",
      "================================================================================\n",
      "LOGGER/DEPLOYMENT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "logger:\n",
      "  Unique values: 0\n",
      "  Distribution:\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "================================================================================\n",
      "SAMPLING RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "For 20,000 clip sample:\n",
      "\n",
      "------------------------------------------------------------\n",
      "Temporal Bin            Available     Random Stratified\n",
      "------------------------------------------------------------\n",
      "dawn_chorus               131,690      2,499      5,000\n",
      "morning                   217,930      4,136      3,000\n",
      "midday                    175,150      3,324      2,000\n",
      "dusk_chorus               131,720      2,500      5,000\n",
      "night                     176,900      3,357      3,000\n",
      "pre_dawn                  176,540      3,351      2,000\n",
      "------------------------------------------------------------\n",
      "\n",
      "Key insight:\n",
      "  • Random sampling: ~17% in dawn/dusk (proportional to data)\n",
      "  • Stratified sampling: 50% in dawn/dusk (biological focus)\n",
      "\n",
      "Recommendation: Use STRATIFIED sampling to capture biological activity\n",
      "\n",
      "================================================================================\n",
      "SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "Total clips: 1,053,610\n",
      "Time span: 593 days\n",
      "Clips per day: 1,777\n",
      "Total audio (estimated): 2,926.7 hours\n",
      "\n",
      "Data quality:\n",
      "  Missing values: 1053610\n",
      "  Duplicate clip_ids: 0\n",
      "\n",
      "================================================================================\n",
      "NEXT STEPS\n",
      "================================================================================\n",
      "\n",
      "1. If temporal distribution looks good:\n",
      "   → Proceed to stratified sampling script\n",
      "\n",
      "2. If timestamps are missing/broken:\n",
      "   → Need to regenerate manifest with proper timestamps\n",
      "\n",
      "3. If file paths are wrong:\n",
      "   → Update manifest with correct paths\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(f\"Inspecting manifest at: {IN_MANIFEST}\")\n",
    "    print()\n",
    "    \n",
    "    if not IN_MANIFEST.exists():\n",
    "        print(f\"✗ Error: File not found: {IN_MANIFEST}\")\n",
    "        print(f\"\\nExpected location: {IN_MANIFEST}\")\n",
    "        print(f\"Repository root: {REPO_ROOT}\")\n",
    "        print(f\"\\nPlease verify:\")\n",
    "        print(f\"  1. Repository is at: {REPO_ROOT}\")\n",
    "        print(f\"  2. Manifest exists at: data/manifests/clip_manifest.parquet\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Run inspection pipeline\n",
    "    df = load_manifest(IN_MANIFEST)\n",
    "    \n",
    "    if not check_required_columns(df):\n",
    "        print(\"\\n⚠ WARNING: Missing required columns. Analysis may be incomplete.\")\n",
    "    \n",
    "    df = inspect_timestamps(df)\n",
    "    check_file_paths(df, sample_size=10)\n",
    "    analyze_logger_distribution(df)\n",
    "    \n",
    "    if df is not None and 'hour' in df.columns:\n",
    "        calculate_sampling_recommendations(df)\n",
    "    \n",
    "    generate_summary_report(df if df is not None else pd.read_parquet(IN_MANIFEST))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e70240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
