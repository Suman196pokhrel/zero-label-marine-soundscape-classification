{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d35d679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 07:42:56.738074: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-27 07:42:56.773619: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-27 07:42:58.321566: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ba93f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set modern aesthetic style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "COLORS = sns.color_palette(\"husl\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "003d0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "HOME = Path.home()\n",
    "REPO_ROOT = HOME / \"Uni-stuff/semester-2/applied_Ml/reef_zmsc\"\n",
    "FUSED_BASE = REPO_ROOT / \"data/features/embeds_fused_50k/PAPCA\"\n",
    "OUTPUT_BASE = REPO_ROOT / \"data/features/embeds_preprocessed_50k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef66cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA variance to keep\n",
    "PCA_VARIANCE = 0.95  # Keep 95% of variance\n",
    "# UMAP settings for visualization\n",
    "UMAP_N_NEIGHBORS = 15\n",
    "UMAP_MIN_DIST = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36083375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_fused_features():\n",
    "    \"\"\"Load all fused features into a single DataFrame.\"\"\"\n",
    "    print(\"üìÇ Loading fused features...\")\n",
    "    \n",
    "    all_dfs = []\n",
    "    for parquet_file in tqdm(list(FUSED_BASE.rglob(\"features.parquet\")), desc=\"Loading files\"):\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"   ‚úì Loaded {len(combined):,} clips\")\n",
    "    print(f\"   ‚úì Shape: {combined.shape}\")\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d6d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_metadata_features(df):\n",
    "    \"\"\"Separate metadata columns from feature columns.\"\"\"\n",
    "    metadata_cols = ['filepath', 'start_s', 'end_s', 'logger', 'date']\n",
    "    feature_cols = [c for c in df.columns if c not in metadata_cols]\n",
    "    \n",
    "    metadata = df[metadata_cols].copy()\n",
    "    features = df[feature_cols].copy()\n",
    "    \n",
    "    print(f\"\\nüìä Feature breakdown:\")\n",
    "    yamnet_cols = [c for c in feature_cols if c.startswith('yamnet_')]\n",
    "    eco_cols = [c for c in feature_cols if not c.startswith('yamnet_')]\n",
    "    \n",
    "    print(f\"   YAMNet features: {len(yamnet_cols)}\")\n",
    "    print(f\"   Ecoacoustic features: {len(eco_cols)}\")\n",
    "    print(f\"   Total features: {len(feature_cols)}\")\n",
    "    \n",
    "    return metadata, features, yamnet_cols, eco_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdeae693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distributions(features, yamnet_cols, eco_cols, output_dir):\n",
    "    \"\"\"Plot distributions of raw features before standardization.\"\"\"\n",
    "    print(\"\\nüìä Creating feature distribution plots...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Raw Feature Distributions (Before Standardization)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # YAMNet features distribution\n",
    "    yamnet_sample = features[yamnet_cols[:100]].values.flatten()\n",
    "    yamnet_sample = yamnet_sample[~np.isnan(yamnet_sample)]\n",
    "    \n",
    "    axes[0, 0].hist(yamnet_sample, bins=100, alpha=0.7, color=COLORS[0], edgecolor='black')\n",
    "    axes[0, 0].set_title('YAMNet Features Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Feature Value')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ecoacoustic features distribution\n",
    "    eco_sample = features[eco_cols].values.flatten()\n",
    "    eco_sample = eco_sample[~np.isnan(eco_sample)]\n",
    "    \n",
    "    axes[0, 1].hist(eco_sample, bins=100, alpha=0.7, color=COLORS[1], edgecolor='black')\n",
    "    axes[0, 1].set_title('Ecoacoustic Features Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Feature Value')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Feature statistics boxplot - YAMNet\n",
    "    yamnet_stats = features[yamnet_cols].describe().T[['mean', 'std', 'min', 'max']]\n",
    "    axes[1, 0].boxplot([yamnet_stats['mean'], yamnet_stats['std']], \n",
    "                        labels=['Mean', 'Std Dev'],\n",
    "                        patch_artist=True,\n",
    "                        boxprops=dict(facecolor=COLORS[2], alpha=0.7),\n",
    "                        medianprops=dict(color='red', linewidth=2))\n",
    "    axes[1, 0].set_title('YAMNet Feature Statistics', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Value')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Feature statistics boxplot - Ecoacoustic\n",
    "    eco_stats = features[eco_cols].describe().T[['mean', 'std', 'min', 'max']]\n",
    "    axes[1, 1].boxplot([eco_stats['mean'], eco_stats['std']], \n",
    "                        labels=['Mean', 'Std Dev'],\n",
    "                        patch_artist=True,\n",
    "                        boxprops=dict(facecolor=COLORS[3], alpha=0.7),\n",
    "                        medianprops=dict(color='red', linewidth=2))\n",
    "    axes[1, 1].set_title('Ecoacoustic Feature Statistics', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Value')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / '01_feature_distributions_raw.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"   ‚úì Saved feature distribution plots\")\n",
    "\n",
    "\n",
    "\n",
    "def standardize_features(features):\n",
    "    \"\"\"Standardize features to mean=0, std=1.\"\"\"\n",
    "    print(\"\\nüîß Standardizing features...\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    features_scaled_df = pd.DataFrame(\n",
    "        features_scaled, \n",
    "        columns=features.columns,\n",
    "        index=features.index\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úì Features standardized\")\n",
    "    print(f\"   Mean: {features_scaled.mean():.2e}\")\n",
    "    print(f\"   Std: {features_scaled.std():.2f}\")\n",
    "    \n",
    "    return features_scaled_df, scaler\n",
    "\n",
    "\n",
    "\n",
    "def plot_standardized_comparison(features, features_scaled, yamnet_cols, eco_cols, output_dir):\n",
    "    \"\"\"Compare before and after standardization.\"\"\"\n",
    "    print(\"\\nüìä Creating standardization comparison plots...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Feature Standardization: Before vs After', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Sample some features for visualization\n",
    "    sample_features = yamnet_cols[:5] + eco_cols[:5]\n",
    "    \n",
    "    # Before standardization\n",
    "    before_data = features[sample_features].values\n",
    "    axes[0, 0].boxplot(before_data, labels=range(1, len(sample_features)+1),\n",
    "                       patch_artist=True,\n",
    "                       boxprops=dict(facecolor=COLORS[0], alpha=0.6))\n",
    "    axes[0, 0].set_title('Before Standardization (Sample Features)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Feature Index')\n",
    "    axes[0, 0].set_ylabel('Value')\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # After standardization\n",
    "    after_data = features_scaled[sample_features].values\n",
    "    axes[0, 1].boxplot(after_data, labels=range(1, len(sample_features)+1),\n",
    "                       patch_artist=True,\n",
    "                       boxprops=dict(facecolor=COLORS[1], alpha=0.6))\n",
    "    axes[0, 1].set_title('After Standardization (Sample Features)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Feature Index')\n",
    "    axes[0, 1].set_ylabel('Standardized Value')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Distribution comparison - Before\n",
    "    axes[1, 0].hist(before_data.flatten(), bins=100, alpha=0.7, \n",
    "                    color=COLORS[2], edgecolor='black')\n",
    "    axes[1, 0].set_title('Value Distribution - Before', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Feature Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution comparison - After\n",
    "    axes[1, 1].hist(after_data.flatten(), bins=100, alpha=0.7, \n",
    "                    color=COLORS[3], edgecolor='black')\n",
    "    axes[1, 1].set_title('Value Distribution - After', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Standardized Value')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / '02_standardization_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"   ‚úì Saved standardization comparison\")\n",
    "\n",
    "\n",
    "\n",
    "def apply_pca(features_scaled, variance_threshold=0.95):\n",
    "    \"\"\"Apply PCA for dimensionality reduction.\"\"\"\n",
    "    print(f\"\\nüî¨ Applying PCA (keeping {variance_threshold*100}% variance)...\")\n",
    "    \n",
    "    pca = PCA(n_components=variance_threshold, random_state=42)\n",
    "    features_pca = pca.fit_transform(features_scaled)\n",
    "    \n",
    "    n_components = pca.n_components_\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cumsum_var = np.cumsum(explained_var)\n",
    "    \n",
    "    print(f\"   ‚úì Reduced from {features_scaled.shape[1]} to {n_components} dimensions\")\n",
    "    print(f\"   ‚úì Explained variance: {cumsum_var[-1]*100:.2f}%\")\n",
    "    print(f\"   First 10 components explain: {cumsum_var[9]*100:.2f}%\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    pca_cols = [f'pca_{i}' for i in range(n_components)]\n",
    "    features_pca_df = pd.DataFrame(\n",
    "        features_pca,\n",
    "        columns=pca_cols,\n",
    "        index=features_scaled.index\n",
    "    )\n",
    "    \n",
    "    return features_pca_df, pca, explained_var\n",
    "\n",
    "\n",
    "\n",
    "def plot_pca_analysis(explained_var, output_dir):\n",
    "    \"\"\"Create comprehensive PCA analysis plots.\"\"\"\n",
    "    print(\"\\nüìä Creating PCA analysis plots...\")\n",
    "    \n",
    "    cumsum_var = np.cumsum(explained_var)\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Individual variance (bar plot)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    n_show = min(50, len(explained_var))\n",
    "    ax1.bar(range(1, n_show + 1), explained_var[:n_show], \n",
    "            alpha=0.7, color=COLORS[0], edgecolor='black', linewidth=0.5)\n",
    "    ax1.set_xlabel('Principal Component', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Variance Explained', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Variance Explained by Each Component (First 50)', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Cumulative variance\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(range(1, len(cumsum_var) + 1), cumsum_var, \n",
    "            marker='o', markersize=4, linewidth=2, color=COLORS[1])\n",
    "    ax2.axhline(y=0.95, color='red', linestyle='--', linewidth=2, label='95% threshold')\n",
    "    ax2.axhline(y=0.99, color='orange', linestyle='--', linewidth=2, label='99% threshold')\n",
    "    ax2.set_xlabel('Number of Components', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Cumulative Variance Explained', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Cumulative Variance Explained', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Scree plot (log scale)\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    ax3.plot(range(1, len(explained_var) + 1), explained_var, \n",
    "            marker='o', markersize=3, linewidth=1.5, color=COLORS[2])\n",
    "    ax3.set_xlabel('Principal Component', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Variance Explained (log scale)', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Scree Plot', fontsize=14, fontweight='bold')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.grid(True, alpha=0.3, which='both')\n",
    "    ax3.set_xlim(0, min(200, len(explained_var)))\n",
    "    \n",
    "    # 4. Components needed for different thresholds\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    thresholds = [0.80, 0.85, 0.90, 0.95, 0.99]\n",
    "    n_components_needed = [np.argmax(cumsum_var >= t) + 1 for t in thresholds]\n",
    "    \n",
    "    bars = ax4.barh(range(len(thresholds)), n_components_needed, \n",
    "                    alpha=0.7, color=COLORS[3], edgecolor='black', linewidth=1.5)\n",
    "    ax4.set_yticks(range(len(thresholds)))\n",
    "    ax4.set_yticklabels([f'{int(t*100)}%' for t in thresholds])\n",
    "    ax4.set_xlabel('Number of Components', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Variance Threshold', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('Components Needed for Different Variance Thresholds', fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars, n_components_needed)):\n",
    "        ax4.text(val + 2, i, str(val), va='center', fontweight='bold')\n",
    "    \n",
    "    # 5. Top components distribution\n",
    "    ax5 = fig.add_subplot(gs[2, :])\n",
    "    top_n = min(20, len(explained_var))\n",
    "    x = range(1, top_n + 1)\n",
    "    y = explained_var[:top_n] * 100\n",
    "    \n",
    "    bars = ax5.bar(x, y, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "    # Color bars by importance\n",
    "    for i, bar in enumerate(bars):\n",
    "        bar.set_color(plt.cm.viridis(y[i]/max(y)))\n",
    "    \n",
    "    ax5.set_xlabel('Principal Component', fontsize=12, fontweight='bold')\n",
    "    ax5.set_ylabel('Variance Explained (%)', fontsize=12, fontweight='bold')\n",
    "    ax5.set_title(f'Top {top_n} Principal Components', fontsize=14, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add percentage labels on top of bars\n",
    "    for i, (xi, yi) in enumerate(zip(x, y)):\n",
    "        ax5.text(xi, yi + 0.1, f'{yi:.1f}%', ha='center', va='bottom', \n",
    "                fontsize=8, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('PCA Dimensionality Reduction Analysis', \n",
    "                fontsize=18, fontweight='bold', y=0.995)\n",
    "    \n",
    "    plt.savefig(output_dir / '03_pca_comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"   ‚úì Saved PCA analysis plots\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_pca_feature_space(features_pca, metadata, output_dir):\n",
    "    \"\"\"Plot first few PCA components in 2D space.\"\"\"\n",
    "    print(\"\\nüìä Creating PCA feature space plots...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    fig.suptitle('PCA Feature Space Visualization', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Get logger info for coloring\n",
    "    loggers = metadata['logger'].values\n",
    "    unique_loggers = np.unique(loggers)\n",
    "    logger_colors = {logger: COLORS[i % len(COLORS)] for i, logger in enumerate(unique_loggers)}\n",
    "    \n",
    "    # PC1 vs PC2\n",
    "    for logger in unique_loggers:\n",
    "        mask = loggers == logger\n",
    "        axes[0, 0].scatter(features_pca.iloc[mask, 0], features_pca.iloc[mask, 1],\n",
    "                          alpha=0.3, s=1, c=[logger_colors[logger]], label=f'Logger {logger}')\n",
    "    axes[0, 0].set_xlabel('PC1', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('PC2', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_title('PC1 vs PC2 (colored by Logger)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend(markerscale=10)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # PC2 vs PC3\n",
    "    for logger in unique_loggers:\n",
    "        mask = loggers == logger\n",
    "        axes[0, 1].scatter(features_pca.iloc[mask, 1], features_pca.iloc[mask, 2],\n",
    "                          alpha=0.3, s=1, c=[logger_colors[logger]], label=f'Logger {logger}')\n",
    "    axes[0, 1].set_xlabel('PC2', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('PC3', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_title('PC2 vs PC3 (colored by Logger)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].legend(markerscale=10)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # PC1 vs PC3\n",
    "    for logger in unique_loggers:\n",
    "        mask = loggers == logger\n",
    "        axes[1, 0].scatter(features_pca.iloc[mask, 0], features_pca.iloc[mask, 2],\n",
    "                          alpha=0.3, s=1, c=[logger_colors[logger]], label=f'Logger {logger}')\n",
    "    axes[1, 0].set_xlabel('PC1', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('PC3', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_title('PC1 vs PC3 (colored by Logger)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend(markerscale=10)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # PC3 vs PC4\n",
    "    for logger in unique_loggers:\n",
    "        mask = loggers == logger\n",
    "        axes[1, 1].scatter(features_pca.iloc[mask, 2], features_pca.iloc[mask, 3],\n",
    "                          alpha=0.3, s=1, c=[logger_colors[logger]], label=f'Logger {logger}')\n",
    "    axes[1, 1].set_xlabel('PC3', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('PC4', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_title('PC3 vs PC4 (colored by Logger)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].legend(markerscale=10)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / '04_pca_feature_space.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"   ‚úì Saved PCA feature space plots\")\n",
    "\n",
    "\n",
    "\n",
    "def apply_umap_for_viz(features_pca, n_neighbors=15, min_dist=0.1):\n",
    "    \"\"\"Apply UMAP for 2D visualization.\"\"\"\n",
    "    print(f\"\\nüó∫Ô∏è Applying UMAP for visualization...\")\n",
    "    print(f\"   n_neighbors={n_neighbors}, min_dist={min_dist}\")\n",
    "    \n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=2,\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    embedding_2d = reducer.fit_transform(features_pca)\n",
    "    \n",
    "    umap_df = pd.DataFrame(\n",
    "        embedding_2d,\n",
    "        columns=['umap_x', 'umap_y'],\n",
    "        index=features_pca.index\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úì 2D embedding created\")\n",
    "    \n",
    "    return umap_df, reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03ea7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap_visualizations(umap_df, metadata, output_dir):\n",
    "    \"\"\"Create comprehensive UMAP visualizations.\"\"\"\n",
    "    print(\"\\nüìä Creating UMAP visualization plots...\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Colored by logger\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    for logger in metadata['logger'].unique():\n",
    "        mask = metadata['logger'] == logger\n",
    "        ax1.scatter(umap_df.loc[mask, 'umap_x'], umap_df.loc[mask, 'umap_y'],\n",
    "                   alpha=0.4, s=2, label=f'Logger {logger}')\n",
    "    ax1.set_xlabel('UMAP 1', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('UMAP 2', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('UMAP: Colored by Logger', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(markerscale=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Density plot\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    h = ax2.hexbin(umap_df['umap_x'], umap_df['umap_y'], \n",
    "                   gridsize=50, cmap='viridis', mincnt=1)\n",
    "    ax2.set_xlabel('UMAP 1', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('UMAP 2', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('UMAP: Density Heatmap', fontsize=14, fontweight='bold')\n",
    "    plt.colorbar(h, ax=ax2, label='Point Density')\n",
    "    \n",
    "    # 3. Colored by date (convert to numeric)\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    dates_numeric = pd.to_datetime(metadata['date'], format='%Y%m%d').astype(int) / 10**9\n",
    "    scatter = ax3.scatter(umap_df['umap_x'], umap_df['umap_y'],\n",
    "                         c=dates_numeric, alpha=0.4, s=2, cmap='coolwarm')\n",
    "    ax3.set_xlabel('UMAP 1', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('UMAP 2', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('UMAP: Colored by Date (Temporal)', fontsize=14, fontweight='bold')\n",
    "    plt.colorbar(scatter, ax=ax3, label='Date')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Contour plot - FIXED\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    from scipy.stats import gaussian_kde\n",
    "    \n",
    "    # Sample for KDE (too slow with 50K points)\n",
    "    n_sample = min(5000, len(umap_df))\n",
    "    sample_idx = np.random.choice(len(umap_df), size=n_sample, replace=False)\n",
    "    \n",
    "    umap_sample = umap_df.iloc[sample_idx]\n",
    "    xy_sample = np.vstack([umap_sample['umap_x'], umap_sample['umap_y']])\n",
    "    z = gaussian_kde(xy_sample)(xy_sample)\n",
    "    \n",
    "    # Plot only the sampled points\n",
    "    ax4.scatter(umap_sample['umap_x'], umap_sample['umap_y'], \n",
    "               c=z, s=2, alpha=0.5, cmap='plasma')\n",
    "    ax4.set_xlabel('UMAP 1', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('UMAP 2', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('UMAP: Density Contours (5K sample)', fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Separate by logger (subplots)\n",
    "    ax5 = fig.add_subplot(gs[1, 1:])\n",
    "    loggers = sorted(metadata['logger'].unique())\n",
    "    n_loggers = len(loggers)\n",
    "    \n",
    "    for i, logger in enumerate(loggers):\n",
    "        mask = metadata['logger'] == logger\n",
    "        ax5.scatter(umap_df.loc[mask, 'umap_x'], umap_df.loc[mask, 'umap_y'],\n",
    "                   alpha=0.5, s=3, label=f'Logger {logger}',\n",
    "                   color=COLORS[i % len(COLORS)])\n",
    "    \n",
    "    ax5.set_xlabel('UMAP 1', fontsize=12, fontweight='bold')\n",
    "    ax5.set_ylabel('UMAP 2', fontsize=12, fontweight='bold')\n",
    "    ax5.set_title('UMAP: All Loggers Overlaid', fontsize=14, fontweight='bold')\n",
    "    ax5.legend(markerscale=5, loc='best')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('UMAP 2D Embedding: Multiple Perspectives', \n",
    "                fontsize=18, fontweight='bold', y=0.995)\n",
    "    \n",
    "    plt.savefig(output_dir / '05_umap_comprehensive.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"   ‚úì Saved comprehensive UMAP visualizations\")\n",
    "\n",
    "def plot_logger_date_distribution(metadata, output_dir):\n",
    "    \"\"\"Plot distribution of clips across loggers and dates.\"\"\"\n",
    "    print(\"\\nüìä Creating data distribution plots...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    fig.suptitle('Data Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Clips per logger\n",
    "    logger_counts = metadata['logger'].value_counts().sort_index()\n",
    "    axes[0, 0].bar(range(len(logger_counts)), logger_counts.values, \n",
    "                   color=COLORS[:len(logger_counts)], alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    axes[0, 0].set_xticks(range(len(logger_counts)))\n",
    "    axes[0, 0].set_xticklabels([f'Logger {l}' for l in logger_counts.index], fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Number of Clips', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_title('Clips per Logger', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(logger_counts.values):\n",
    "        axes[0, 0].text(i, v + 200, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Clips per date (time series)\n",
    "    metadata['date_dt'] = pd.to_datetime(metadata['date'], format='%Y%m%d')\n",
    "    date_counts = metadata.groupby('date_dt').size()\n",
    "    \n",
    "    axes[0, 1].plot(date_counts.index, date_counts.values, linewidth=2, color=COLORS[2])\n",
    "    axes[0, 1].fill_between(date_counts.index, date_counts.values, alpha=0.3, color=COLORS[2])\n",
    "    axes[0, 1].set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Number of Clips', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_title('Clips Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Clips per logger per date (heatmap)\n",
    "    pivot_data = metadata.groupby(['logger', 'date']).size().reset_index(name='count')\n",
    "    pivot_table = pivot_data.pivot(index='logger', columns='date', values='count').fillna(0)\n",
    "    \n",
    "    # Sample dates for readability (show every Nth date)\n",
    "    n_dates_show = min(30, pivot_table.shape[1])\n",
    "    step = max(1, pivot_table.shape[1] // n_dates_show)\n",
    "    pivot_sample = pivot_table.iloc[:, ::step]\n",
    "    \n",
    "    sns.heatmap(pivot_sample, cmap='YlOrRd', cbar_kws={'label': 'Clip Count'},\n",
    "               ax=axes[1, 0], linewidths=0.5, linecolor='gray')\n",
    "    axes[1, 0].set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Logger', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_title('Clips per Logger per Date (Sampled Dates)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=90, labelsize=8)\n",
    "    \n",
    "    # 4. Distribution of clips per date\n",
    "    clips_per_date = metadata.groupby('date').size()\n",
    "    axes[1, 1].hist(clips_per_date.values, bins=50, alpha=0.7, color=COLORS[4], edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Clips per Date', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Frequency (Number of Dates)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_title('Distribution of Clips per Date', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    axes[1, 1].axvline(clips_per_date.median(), color='red', linestyle='--', \n",
    "                      linewidth=2, label=f'Median: {clips_per_date.median():.0f}')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / '06_data_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Clean up temporary column\n",
    "    metadata.drop('date_dt', axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"   ‚úì Saved data distribution plots\")\n",
    "\n",
    "def save_preprocessed_data(metadata, features_scaled, features_pca, umap_df, output_dir):\n",
    "    \"\"\"Save all preprocessed data.\"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nüíæ Saving preprocessed data...\")\n",
    "    \n",
    "    # Combine everything\n",
    "    final_df = pd.concat([\n",
    "        metadata.reset_index(drop=True),\n",
    "        features_scaled.reset_index(drop=True),\n",
    "        features_pca.reset_index(drop=True),\n",
    "        umap_df.reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Save full dataset\n",
    "    output_file = output_dir / \"preprocessed_features_full.parquet\"\n",
    "    final_df.to_parquet(output_file, index=False, compression='snappy')\n",
    "    print(f\"   ‚úì Full dataset: {output_file.relative_to(REPO_ROOT)}\")\n",
    "    \n",
    "    # Save PCA-only version (most useful for clustering)\n",
    "    pca_cols = [c for c in final_df.columns if c.startswith('pca_')]\n",
    "    pca_df = pd.concat([\n",
    "        metadata.reset_index(drop=True),\n",
    "        final_df[pca_cols + ['umap_x', 'umap_y']].reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "    \n",
    "    output_file_pca = output_dir / \"preprocessed_features_pca.parquet\"\n",
    "    pca_df.to_parquet(output_file_pca, index=False, compression='snappy')\n",
    "    print(f\"   ‚úì PCA version: {output_file_pca.relative_to(REPO_ROOT)}\")\n",
    "    \n",
    "    print(f\"\\n   Saved {len(final_df):,} clips\")\n",
    "    print(f\"   Full shape: {final_df.shape}\")\n",
    "    print(f\"   PCA shape: {pca_df.shape}\")\n",
    "\n",
    "def create_summary_report(metadata, features, features_pca, explained_var, output_dir):\n",
    "    \"\"\"Create a text summary report.\"\"\"\n",
    "    print(\"\\nüìù Creating summary report...\")\n",
    "    \n",
    "    report_path = output_dir / \"preprocessing_summary.txt\"\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"=\" * 70 + \"\\n\")\n",
    "        f.write(\"FEATURE PREPROCESSING SUMMARY REPORT\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"DATASET OVERVIEW\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(f\"Total clips: {len(metadata):,}\\n\")\n",
    "        f.write(f\"Unique loggers: {metadata['logger'].nunique()}\\n\")\n",
    "        f.write(f\"Unique dates: {metadata['date'].nunique()}\\n\")\n",
    "        f.write(f\"Date range: {metadata['date'].min()} to {metadata['date'].max()}\\n\\n\")\n",
    "        \n",
    "        f.write(\"FEATURE DIMENSIONS\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(f\"Original features: {features.shape[1]}\\n\")\n",
    "        yamnet_cols = [c for c in features.columns if c.startswith('yamnet_')]\n",
    "        eco_cols = [c for c in features.columns if not c.startswith('yamnet_')]\n",
    "        f.write(f\"  - YAMNet embeddings: {len(yamnet_cols)}\\n\")\n",
    "        f.write(f\"  - Ecoacoustic features: {len(eco_cols)}\\n\")\n",
    "        f.write(f\"PCA-reduced features: {features_pca.shape[1]}\\n\")\n",
    "        f.write(f\"Dimensionality reduction: {(1 - features_pca.shape[1]/features.shape[1])*100:.1f}%\\n\\n\")\n",
    "        \n",
    "        f.write(\"PCA ANALYSIS\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        cumsum_var = np.cumsum(explained_var)\n",
    "        n_components = len(explained_var)\n",
    "        \n",
    "        f.write(f\"Total components: {n_components}\\n\")\n",
    "        f.write(f\"Variance explained by PCA: {cumsum_var[-1]*100:.2f}%\\n\\n\")\n",
    "        \n",
    "        # Dynamically show variance for available components\n",
    "        f.write(\"Variance explained by top components:\\n\")\n",
    "        for n in [5, 10, 20, 30, 50, 100]:\n",
    "            if n <= n_components:\n",
    "                f.write(f\"  - Top {n:3d} components: {cumsum_var[n-1]*100:.2f}%\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"Components needed for variance thresholds:\\n\")\n",
    "        for thresh in [0.80, 0.85, 0.90, 0.95, 0.99]:\n",
    "            idx = np.argmax(cumsum_var >= thresh)\n",
    "            if cumsum_var[idx] >= thresh:\n",
    "                n_comp = idx + 1\n",
    "                f.write(f\"  - {int(thresh*100):2d}%: {n_comp} components\\n\")\n",
    "            else:\n",
    "                f.write(f\"  - {int(thresh*100):2d}%: Not achievable (max: {cumsum_var[-1]*100:.2f}%)\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "        f.write(\"FILES GENERATED\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(\"Data files:\\n\")\n",
    "        f.write(\"  1. preprocessed_features_full.parquet - All features\\n\")\n",
    "        f.write(\"  2. preprocessed_features_pca.parquet - PCA features (use for clustering)\\n\\n\")\n",
    "        f.write(\"Visualization files:\\n\")\n",
    "        f.write(\"  3. 01_feature_distributions_raw.png\\n\")\n",
    "        f.write(\"  4. 02_standardization_comparison.png\\n\")\n",
    "        f.write(\"  5. 03_pca_comprehensive_analysis.png\\n\")\n",
    "        f.write(\"  6. 04_pca_feature_space.png\\n\")\n",
    "        f.write(\"  7. 05_umap_comprehensive.png\\n\")\n",
    "        f.write(\"  8. 06_data_distribution.png\\n\\n\")\n",
    "        f.write(\"Report:\\n\")\n",
    "        f.write(\"  9. preprocessing_summary.txt (this file)\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "        f.write(\"NEXT STEPS\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(\"1. Review the visualizations to understand your data\\n\")\n",
    "        f.write(\"2. Use 'preprocessed_features_pca.parquet' for clustering\\n\")\n",
    "        f.write(\"3. Try different clustering algorithms:\\n\")\n",
    "        f.write(\"   - K-means (fast, assumes spherical clusters)\\n\")\n",
    "        f.write(\"   - HDBSCAN (finds clusters of varying density)\\n\")\n",
    "        f.write(\"   - Gaussian Mixture Models (soft clustering)\\n\")\n",
    "        f.write(\"4. Use UMAP coordinates (umap_x, umap_y) for visualization\\n\")\n",
    "        f.write(\"5. Evaluate clusters using silhouette score, Davies-Bouldin index\\n\")\n",
    "    \n",
    "    print(f\"   ‚úì Saved summary report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb4749ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE PREPROCESSING: Standardization + PCA + UMAP\n",
      "======================================================================\n",
      "üìÇ Loading fused features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:   0%|          | 0/271 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [00:08<00:00, 33.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Loaded 50,000 clips\n",
      "   ‚úì Shape: (50000, 1046)\n",
      "\n",
      "üìä Feature breakdown:\n",
      "   YAMNet features: 1024\n",
      "   Ecoacoustic features: 17\n",
      "   Total features: 1041\n",
      "\n",
      "üìä Creating feature distribution plots...\n",
      "   ‚úì Saved feature distribution plots\n",
      "\n",
      "üîß Standardizing features...\n",
      "   ‚úì Features standardized\n",
      "   Mean: -9.75e-18\n",
      "   Std: 1.00\n",
      "\n",
      "üìä Creating standardization comparison plots...\n",
      "   ‚úì Saved standardization comparison\n",
      "\n",
      "üî¨ Applying PCA (keeping 95.0% variance)...\n",
      "   ‚úì Reduced from 1041 to 39 dimensions\n",
      "   ‚úì Explained variance: 95.03%\n",
      "   First 10 components explain: 87.37%\n",
      "\n",
      "üìä Creating PCA analysis plots...\n",
      "   ‚úì Saved PCA analysis plots\n",
      "\n",
      "üìä Creating PCA feature space plots...\n",
      "   ‚úì Saved PCA feature space plots\n",
      "\n",
      "üó∫Ô∏è Applying UMAP for visualization...\n",
      "   n_neighbors=15, min_dist=0.1\n",
      "   ‚úì 2D embedding created\n",
      "\n",
      "üìä Creating UMAP visualization plots...\n",
      "   ‚úì Saved comprehensive UMAP visualizations\n",
      "\n",
      "üìä Creating data distribution plots...\n",
      "   ‚úì Saved data distribution plots\n",
      "\n",
      "üíæ Saving preprocessed data...\n",
      "   ‚úì Full dataset: data/features/embeds_preprocessed_50k/preprocessed_features_full.parquet\n",
      "   ‚úì PCA version: data/features/embeds_preprocessed_50k/preprocessed_features_pca.parquet\n",
      "\n",
      "   Saved 50,000 clips\n",
      "   Full shape: (50000, 1087)\n",
      "   PCA shape: (50000, 46)\n",
      "\n",
      "üìù Creating summary report...\n",
      "   ‚úì Saved summary report\n",
      "\n",
      "======================================================================\n",
      "‚úÖ PREPROCESSING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "üìä Summary:\n",
      "   Original dimensions: 1041\n",
      "   PCA dimensions: 39\n",
      "   Reduction: 96.3%\n",
      "   Variance preserved: 95.03%\n",
      "\n",
      "üíæ Output files (9 total):\n",
      "   üìÑ Data files:\n",
      "      ‚Ä¢ preprocessed_features_full.parquet\n",
      "      ‚Ä¢ preprocessed_features_pca.parquet ‚≠ê (use this for clustering)\n",
      "   üìä Visualization files:\n",
      "      ‚Ä¢ 01_feature_distributions_raw.png\n",
      "      ‚Ä¢ 02_standardization_comparison.png\n",
      "      ‚Ä¢ 03_pca_comprehensive_analysis.png\n",
      "      ‚Ä¢ 04_pca_feature_space.png\n",
      "      ‚Ä¢ 05_umap_comprehensive.png\n",
      "      ‚Ä¢ 06_data_distribution.png\n",
      "   üìù Report:\n",
      "      ‚Ä¢ preprocessing_summary.txt\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"FEATURE PREPROCESSING: Standardization + PCA + UMAP\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load data\n",
    "    df = load_all_fused_features()\n",
    "    \n",
    "    # Separate metadata and features\n",
    "    metadata, features, yamnet_cols, eco_cols = separate_metadata_features(df)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing = features.isnull().sum().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è Found {missing} missing values, filling with 0\")\n",
    "        features = features.fillna(0)\n",
    "    \n",
    "    # Create output directory\n",
    "    OUTPUT_BASE.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Plot raw feature distributions\n",
    "    plot_feature_distributions(features, yamnet_cols, eco_cols, OUTPUT_BASE)\n",
    "    \n",
    "    # 2. Standardization\n",
    "    features_scaled, scaler = standardize_features(features)\n",
    "    \n",
    "    # 3. Plot standardization comparison\n",
    "    plot_standardized_comparison(features, features_scaled, yamnet_cols, eco_cols, OUTPUT_BASE)\n",
    "    \n",
    "    # 4. PCA\n",
    "    features_pca, pca, explained_var = apply_pca(features_scaled, variance_threshold=PCA_VARIANCE)\n",
    "    \n",
    "    # 5. PCA analysis plots\n",
    "    plot_pca_analysis(explained_var, OUTPUT_BASE)\n",
    "    \n",
    "    # 6. PCA feature space plots\n",
    "    plot_pca_feature_space(features_pca, metadata, OUTPUT_BASE)\n",
    "    \n",
    "    # 7. UMAP for visualization\n",
    "    umap_df, umap_reducer = apply_umap_for_viz(features_pca)\n",
    "    \n",
    "    # 8. UMAP visualizations\n",
    "    plot_umap_visualizations(umap_df, metadata, OUTPUT_BASE)\n",
    "    \n",
    "    # 9. Data distribution plots\n",
    "    plot_logger_date_distribution(metadata, OUTPUT_BASE)\n",
    "    \n",
    "    # 10. Save preprocessed data\n",
    "    save_preprocessed_data(metadata, features_scaled, features_pca, umap_df, OUTPUT_BASE)\n",
    "    \n",
    "    # 11. Create summary report\n",
    "    create_summary_report(metadata, features, features_pca, explained_var, OUTPUT_BASE)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ PREPROCESSING COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   Original dimensions: {len(features.columns)}\")\n",
    "    print(f\"   PCA dimensions: {len(features_pca.columns)}\")\n",
    "    print(f\"   Reduction: {(1 - len(features_pca.columns)/len(features.columns))*100:.1f}%\")\n",
    "    print(f\"   Variance preserved: {np.sum(explained_var)*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nüíæ Output files (9 total):\")\n",
    "    print(f\"   üìÑ Data files:\")\n",
    "    print(f\"      ‚Ä¢ preprocessed_features_full.parquet\")\n",
    "    print(f\"      ‚Ä¢ preprocessed_features_pca.parquet ‚≠ê (use this for clustering)\")\n",
    "    print(f\"   üìä Visualization files:\")\n",
    "    print(f\"      ‚Ä¢ 01_feature_distributions_raw.png\")\n",
    "    print(f\"      ‚Ä¢ 02_standardization_comparison.png\")\n",
    "    print(f\"      ‚Ä¢ 03_pca_comprehensive_analysis.png\")\n",
    "    print(f\"      ‚Ä¢ 04_pca_feature_space.png\")\n",
    "    print(f\"      ‚Ä¢ 05_umap_comprehensive.png\")\n",
    "    print(f\"      ‚Ä¢ 06_data_distribution.png\")\n",
    "    print(f\"   üìù Report:\")\n",
    "    print(f\"      ‚Ä¢ preprocessing_summary.txt\")\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reef_zmsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
