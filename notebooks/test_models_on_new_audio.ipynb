{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9158acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e4e6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration for testing\"\"\"\n",
    "    \n",
    "    ROOT_DIR = Path(\"~/Uni-stuff/semester-2/applied_Ml/reef_zmsc\").expanduser()\n",
    "    \n",
    "    # Data paths\n",
    "    CLIP_MANIFEST = ROOT_DIR / \"data/manifests/clip_manifest.parquet\"\n",
    "    TRAINING_FILES = ROOT_DIR / \"data/clustering/results_50k/clustered_data_kmeans.parquet\"\n",
    "    \n",
    "    # Feature extraction\n",
    "    YAMNET_MODEL_PATH = ROOT_DIR / \"models/yamnet\"\n",
    "    \n",
    "    # Trained models\n",
    "    STAGE1_MODEL = ROOT_DIR / \"data/autolabeling_fixed/models/classifier.joblib\"\n",
    "    TWO_STAGE_PIPELINE = ROOT_DIR / \"data/two_stage_detector/models/two_stage_pipeline.joblib\"\n",
    "    PCA_MODEL = ROOT_DIR / \"data/features/embeds_preprocessed_50k/pca_model.joblib\"\n",
    "    \n",
    "    # Output\n",
    "    OUTPUT_DIR = ROOT_DIR / \"data/model_testing\"\n",
    "    RESULTS_DIR = OUTPUT_DIR / \"results\"\n",
    "    \n",
    "    # Test parameters\n",
    "    N_TEST_CLIPS = 20  # Number of clips to test\n",
    "    FEATURE_COLS = [f\"pca_{i}\" for i in range(39)]\n",
    "    \n",
    "    RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9387a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_new_clips(n_clips=20):\n",
    "    \"\"\"\n",
    "    Sample random clips from 1M collection that were NOT in training\n",
    "    \n",
    "    Args:\n",
    "        n_clips: Number of clips to sample\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with sampled clip metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 1: SAMPLING NEW CLIPS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load full manifest\n",
    "    print(f\"\\nüì• Loading clip manifest...\")\n",
    "    manifest_df = pd.read_parquet(Config.CLIP_MANIFEST)\n",
    "    print(f\"   ‚úÖ Total clips available: {len(manifest_df):,}\")\n",
    "    \n",
    "    # Load training files (to exclude them)\n",
    "    print(f\"\\nüì• Loading training files...\")\n",
    "    training_df = pd.read_parquet(Config.TRAINING_FILES)\n",
    "    training_files = set(training_df['filepath'].values)\n",
    "    print(f\"   ‚úÖ Training files: {len(training_files):,}\")\n",
    "    \n",
    "    # Filter to NEW clips only\n",
    "    print(f\"\\nüîç Filtering to NEW clips (not in training)...\")\n",
    "    new_clips = manifest_df[~manifest_df['filepath'].isin(training_files)]\n",
    "    print(f\"   ‚úÖ New clips available: {len(new_clips):,}\")\n",
    "    \n",
    "    # Sample randomly\n",
    "    print(f\"\\nüé≤ Sampling {n_clips} random clips...\")\n",
    "    sampled = new_clips.sample(n=n_clips, random_state=Config.RANDOM_STATE)\n",
    "    \n",
    "    print(f\"\\nüìä Sampled clips:\")\n",
    "    print(f\"   Logger distribution:\")\n",
    "    for logger, count in sampled['logger'].value_counts().items():\n",
    "        print(f\"     {logger}: {count} clips\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Sample file paths:\")\n",
    "    for filepath in sampled['filepath'].head(5):\n",
    "        print(f\"   {filepath}\")\n",
    "    print(f\"   ... and {len(sampled) - 5} more\")\n",
    "    \n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb30df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_yamnet_embeddings(audio_paths):\n",
    "    \"\"\"\n",
    "    Extract YAMNet embeddings for audio files\n",
    "    \n",
    "    Args:\n",
    "        audio_paths: List of audio file paths\n",
    "    \n",
    "    Returns:\n",
    "        Array of embeddings (n_clips, 1024)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 2: EXTRACTING YAMNET FEATURES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  NOTE: This requires YAMNet model and audio loading\")\n",
    "    print(f\"   If you don't have YAMNet setup, we'll use a workaround\")\n",
    "    \n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        import tensorflow_hub as hub\n",
    "        import librosa\n",
    "        \n",
    "        print(f\"\\nüì• Loading YAMNet model...\")\n",
    "        yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
    "        \n",
    "        embeddings = []\n",
    "        \n",
    "        print(f\"\\nüéµ Processing {len(audio_paths)} audio files...\")\n",
    "        for i, filepath in enumerate(audio_paths, 1):\n",
    "            print(f\"   [{i}/{len(audio_paths)}] {Path(filepath).name}\", end='\\r')\n",
    "            \n",
    "            # Load audio\n",
    "            audio, sr = librosa.load(filepath, sr=16000, duration=10)\n",
    "            \n",
    "            # Get YAMNet embeddings\n",
    "            scores, embeds, spectrogram = yamnet_model(audio)\n",
    "            \n",
    "            # Average embeddings (mean pooling)\n",
    "            embed_mean = np.mean(embeds.numpy(), axis=0)\n",
    "            embeddings.append(embed_mean)\n",
    "        \n",
    "        print(f\"\\n   ‚úÖ Extracted embeddings for {len(embeddings)} files\")\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Could not extract YAMNet embeddings: {e}\")\n",
    "        print(f\"   Using workaround: Loading pre-computed features if available\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8321e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ecoacoustic_features(audio_paths):\n",
    "    \"\"\"\n",
    "    Extract ecoacoustic indices (17 features)\n",
    "    Matches the original preprocessing pipeline\n",
    "    \n",
    "    Args:\n",
    "        audio_paths: List of audio file paths\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with ecoacoustic features (17 columns)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 2B: EXTRACTING ECOACOUSTIC FEATURES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  NOTE: This requires librosa and audio loading\")\n",
    "    \n",
    "    try:\n",
    "        import librosa\n",
    "        \n",
    "        features_list = []\n",
    "        \n",
    "        print(f\"\\nüéµ Processing {len(audio_paths)} audio files...\")\n",
    "        for i, filepath in enumerate(audio_paths, 1):\n",
    "            print(f\"   [{i}/{len(audio_paths)}] {Path(filepath).name}\", end='\\r')\n",
    "            \n",
    "            # Load audio\n",
    "            audio, sr = librosa.load(filepath, sr=16000, duration=10)\n",
    "            \n",
    "            # Extract features - EXACTLY 17 features to match original\n",
    "            features = {}\n",
    "            \n",
    "            # 1-2: Spectral centroid (mean, std)\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "            features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n",
    "            features['spectral_centroid_std'] = np.std(spectral_centroid)\n",
    "            \n",
    "            # 3-4: Spectral bandwidth (mean, std)\n",
    "            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "            features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n",
    "            features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)\n",
    "            \n",
    "            # 5-6: Spectral rolloff (mean, std)\n",
    "            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "            features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n",
    "            features['spectral_rolloff_std'] = np.std(spectral_rolloff)\n",
    "            \n",
    "            # 7-8: Zero crossing rate (mean, std)\n",
    "            zcr = librosa.feature.zero_crossing_rate(audio)\n",
    "            features['zcr_mean'] = np.mean(zcr)\n",
    "            features['zcr_std'] = np.std(zcr)\n",
    "            \n",
    "            # 9-10: RMS energy (mean, std)\n",
    "            rms = librosa.feature.rms(y=audio)\n",
    "            features['rms_energy_mean'] = np.mean(rms)\n",
    "            features['rms_energy_std'] = np.std(rms)\n",
    "            \n",
    "            # 11-14: MFCCs - first 2 coefficients only (mean, std each)\n",
    "            # Changed from 4 to 2 to get exactly 17 features\n",
    "            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=2)\n",
    "            for j in range(2):\n",
    "                features[f'mfcc_{j}_mean'] = np.mean(mfccs[j])\n",
    "                features[f'mfcc_{j}_std'] = np.std(mfccs[j])\n",
    "            \n",
    "            # 15: Spectral contrast (mean only)\n",
    "            contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
    "            features['spectral_contrast_mean'] = np.mean(contrast)\n",
    "            \n",
    "            # 16-17: Spectral flatness (mean, std)\n",
    "            flatness = librosa.feature.spectral_flatness(y=audio)\n",
    "            features['spectral_flatness_mean'] = np.mean(flatness)\n",
    "            features['spectral_flatness_std'] = np.std(flatness)\n",
    "            \n",
    "            features_list.append(features)\n",
    "        \n",
    "        print(f\"\\n   ‚úÖ Extracted features for {len(features_list)} files\")\n",
    "        \n",
    "        # Verify we have exactly 17 features\n",
    "        df = pd.DataFrame(features_list)\n",
    "        print(f\"   üìä Feature count: {len(df.columns)} (expected: 17)\")\n",
    "        \n",
    "        if len(df.columns) != 17:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: Expected 17 features, got {len(df.columns)}\")\n",
    "            print(f\"   Features: {list(df.columns)}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Could not extract ecoacoustic features: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e2eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_workaround(sampled_df):\n",
    "    \"\"\"\n",
    "    Workaround: Check if features already exist in processed data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 2 (WORKAROUND): CHECKING FOR EXISTING FEATURES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nüîç Looking for pre-computed features...\")\n",
    "    \n",
    "    # Check if these clips have features in any existing dataset\n",
    "    existing_features_path = Config.ROOT_DIR / \"data/features/embeds_preprocessed_50k/preprocessed_features_pca.parquet\"\n",
    "    \n",
    "    if existing_features_path.exists():\n",
    "        print(f\"   Found: {existing_features_path}\")\n",
    "        existing_df = pd.read_parquet(existing_features_path)\n",
    "        \n",
    "        # Match with sampled clips\n",
    "        matched = sampled_df.merge(\n",
    "            existing_df,\n",
    "            on='filepath',\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        if len(matched) > 0:\n",
    "            print(f\"   ‚úÖ Found {len(matched)} clips with existing features!\")\n",
    "            return matched\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  No existing features found\")\n",
    "    print(f\"   Need to extract features from scratch\")\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb931f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(embeddings, ecoacoustic_features):\n",
    "    \"\"\"\n",
    "    Combine and preprocess features (PCA)\n",
    "    \n",
    "    Args:\n",
    "        embeddings: YAMNet embeddings (n, 1024)\n",
    "        ecoacoustic_features: Ecoacoustic features (n, 17)\n",
    "    \n",
    "    Returns:\n",
    "        PCA features (n, 39)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 3: PREPROCESSING FEATURES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load PCA model\n",
    "    print(f\"\\nüì• Loading PCA model...\")\n",
    "    pca = joblib.load(Config.PCA_MODEL)\n",
    "    print(f\"   ‚úÖ Loaded PCA: {pca.n_components_} components\")\n",
    "    \n",
    "    # Combine features\n",
    "    print(f\"\\nüîó Combining features...\")\n",
    "    combined = np.hstack([embeddings, ecoacoustic_features.values])\n",
    "    print(f\"   ‚úÖ Combined shape: {combined.shape}\")\n",
    "    \n",
    "    # Apply PCA\n",
    "    print(f\"\\nüîÑ Applying PCA transformation...\")\n",
    "    pca_features = pca.transform(combined)\n",
    "    print(f\"   ‚úÖ PCA shape: {pca_features.shape}\")\n",
    "    \n",
    "    return pca_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2470d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_models(features_df):\n",
    "    \"\"\"\n",
    "    Run predictions with all trained models\n",
    "    \n",
    "    Args:\n",
    "        features_df: DataFrame with PCA features\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with predictions from all models\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 4: RUNNING PREDICTIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results_df = features_df.copy()\n",
    "    \n",
    "    # Model 1: Stage 1 only (AMBIENT vs BIO)\n",
    "    print(f\"\\nüìä Model 1: Stage 1 Classifier (AMBIENT vs BIO)\")\n",
    "    try:\n",
    "        stage1_model = joblib.load(Config.STAGE1_MODEL)\n",
    "        X = features_df[Config.FEATURE_COLS].values\n",
    "        \n",
    "        pred_stage1 = stage1_model.predict(X)\n",
    "        proba_stage1 = stage1_model.predict_proba(X)\n",
    "        \n",
    "        results_df['stage1_prediction'] = pred_stage1\n",
    "        results_df['stage1_confidence'] = proba_stage1.max(axis=1)\n",
    "        \n",
    "        print(f\"   ‚úÖ Predictions:\")\n",
    "        for category, count in pd.Series(pred_stage1).value_counts().items():\n",
    "            print(f\"      {category}: {count} clips\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not load Stage 1 model: {e}\")\n",
    "        results_df['stage1_prediction'] = 'UNKNOWN'\n",
    "        results_df['stage1_confidence'] = 0.0\n",
    "    \n",
    "    # Model 2: Two-stage (AMBIENT vs BIO vs HUMAN)\n",
    "    print(f\"\\nüìä Model 2: Two-Stage Classifier (AMBIENT vs BIO vs HUMAN)\")\n",
    "    try:\n",
    "        two_stage = joblib.load(Config.TWO_STAGE_PIPELINE)\n",
    "        \n",
    "        pred_2stage, conf_2stage = two_stage.predict(\n",
    "            features_df,\n",
    "            return_confidence=True\n",
    "        )\n",
    "        \n",
    "        results_df['two_stage_prediction'] = pred_2stage\n",
    "        results_df['two_stage_confidence'] = conf_2stage\n",
    "        \n",
    "        print(f\"   ‚úÖ Predictions:\")\n",
    "        for category, count in pd.Series(pred_2stage).value_counts().items():\n",
    "            print(f\"      {category}: {count} clips\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not load Two-Stage model: {e}\")\n",
    "        results_df['two_stage_prediction'] = 'UNKNOWN'\n",
    "        results_df['two_stage_confidence'] = 0.0\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf60c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictions(results_df):\n",
    "    \"\"\"\n",
    "    Analyze and display prediction results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 5: ANALYZING RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nüìä PREDICTION SUMMARY\")\n",
    "    print(f\"=\" * 80)\n",
    "    \n",
    "    # Stage 1 results\n",
    "    print(f\"\\nüéØ Stage 1 Model (AMBIENT vs BIO):\")\n",
    "    print(f\"   {'-' * 40}\")\n",
    "    for category in ['AMBIENT', 'BIO']:\n",
    "        clips = results_df[results_df['stage1_prediction'] == category]\n",
    "        if len(clips) > 0:\n",
    "            mean_conf = clips['stage1_confidence'].mean()\n",
    "            print(f\"   {category:8s}: {len(clips):2d} clips | Avg conf: {mean_conf:.3f}\")\n",
    "    \n",
    "    # Two-stage results\n",
    "    print(f\"\\nüéØ Two-Stage Model (AMBIENT vs BIO vs HUMAN):\")\n",
    "    print(f\"   {'-' * 40}\")\n",
    "    for category in ['AMBIENT', 'BIO', 'HUMAN']:\n",
    "        clips = results_df[results_df['two_stage_prediction'] == category]\n",
    "        if len(clips) > 0:\n",
    "            mean_conf = clips['two_stage_confidence'].mean()\n",
    "            print(f\"   {category:8s}: {len(clips):2d} clips | Avg conf: {mean_conf:.3f}\")\n",
    "    \n",
    "    # High confidence predictions\n",
    "    print(f\"\\n‚≠ê HIGH CONFIDENCE PREDICTIONS (>0.9):\")\n",
    "    print(f\"   {'-' * 40}\")\n",
    "    high_conf = results_df[results_df['two_stage_confidence'] > 0.9]\n",
    "    if len(high_conf) > 0:\n",
    "        for _, row in high_conf.iterrows():\n",
    "            print(f\"   {Path(row['filepath']).name[:40]:40s} ‚Üí {row['two_stage_prediction']:8s} ({row['two_stage_confidence']:.3f})\")\n",
    "    else:\n",
    "        print(f\"   No high confidence predictions\")\n",
    "    \n",
    "    # Low confidence predictions (ambiguous)\n",
    "    print(f\"\\n‚ùì LOW CONFIDENCE PREDICTIONS (<0.6):\")\n",
    "    print(f\"   {'-' * 40}\")\n",
    "    low_conf = results_df[results_df['two_stage_confidence'] < 0.6]\n",
    "    if len(low_conf) > 0:\n",
    "        for _, row in low_conf.iterrows():\n",
    "            print(f\"   {Path(row['filepath']).name[:40]:40s} ‚Üí {row['two_stage_prediction']:8s} ({row['two_stage_confidence']:.3f})\")\n",
    "    else:\n",
    "        print(f\"   All predictions are confident!\")\n",
    "    \n",
    "    # Disagreements between models\n",
    "    print(f\"\\n‚ö†Ô∏è  MODEL DISAGREEMENTS (Stage1 != Two-Stage):\")\n",
    "    print(f\"   {'-' * 40}\")\n",
    "    disagreements = results_df[results_df['stage1_prediction'] != results_df['two_stage_prediction']]\n",
    "    if len(disagreements) > 0:\n",
    "        for _, row in disagreements.iterrows():\n",
    "            print(f\"   {Path(row['filepath']).name[:40]:40s}\")\n",
    "            print(f\"      Stage1: {row['stage1_prediction']:8s} (c={row['stage1_confidence']:.3f})\")\n",
    "            print(f\"      Two-Stage: {row['two_stage_prediction']:8s} (c={row['two_stage_confidence']:.3f})\")\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"   Both models agree on all predictions!\")\n",
    "    \n",
    "    # Interesting finds\n",
    "    print(f\"\\nüîç INTERESTING FINDS:\")\n",
    "    print(f\"   {'-' * 40}\")\n",
    "    \n",
    "    # HUMAN detections\n",
    "    human_clips = results_df[results_df['two_stage_prediction'] == 'HUMAN']\n",
    "    if len(human_clips) > 0:\n",
    "        print(f\"   üö¢ HUMAN sounds detected: {len(human_clips)} clips\")\n",
    "        for _, row in human_clips.iterrows():\n",
    "            print(f\"      {row['filepath']}\")\n",
    "            print(f\"         Confidence: {row['two_stage_confidence']:.3f}\")\n",
    "    else:\n",
    "        print(f\"   No HUMAN sounds detected (clean reef!)\")\n",
    "    \n",
    "    # BIO detections\n",
    "    bio_clips = results_df[results_df['two_stage_prediction'] == 'BIO']\n",
    "    if len(bio_clips) > 0:\n",
    "        print(f\"\\n   üê† BIO sounds detected: {len(bio_clips)} clips\")\n",
    "        top_bio = bio_clips.nlargest(3, 'two_stage_confidence')\n",
    "        for _, row in top_bio.iterrows():\n",
    "            print(f\"      {Path(row['filepath']).name} (c={row['two_stage_confidence']:.3f})\")\n",
    "\n",
    "\n",
    "def save_results(results_df):\n",
    "    \"\"\"Save results for further analysis\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    Config.RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save full results\n",
    "    results_path = Config.RESULTS_DIR / \"test_predictions.csv\"\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"\\n‚úÖ Saved: {results_path}\")\n",
    "    \n",
    "    # Save clips by category for easy listening\n",
    "    for category in ['AMBIENT', 'BIO', 'HUMAN']:\n",
    "        clips = results_df[results_df['two_stage_prediction'] == category]\n",
    "        if len(clips) > 0:\n",
    "            cat_path = Config.RESULTS_DIR / f\"{category.lower()}_clips.txt\"\n",
    "            with open(cat_path, 'w') as f:\n",
    "                f.write(f\"{category} Clips\\n\")\n",
    "                f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "                for _, row in clips.iterrows():\n",
    "                    f.write(f\"{row['filepath']}\\n\")\n",
    "                    f.write(f\"  Confidence: {row['two_stage_confidence']:.3f}\\n\")\n",
    "                    f.write(f\"  Stage1: {row['stage1_prediction']}\\n\\n\")\n",
    "            print(f\"‚úÖ Saved: {cat_path}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ All results in: {Config.RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7a7727e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üß™ MODEL TESTING ON NEW AUDIO CLIPS\n",
      "================================================================================\n",
      "\n",
      "This will:\n",
      "  1. Sample 20 random clips from 1M collection\n",
      "  2. Extract features (YAMNet + ecoacoustic)\n",
      "  3. Run through trained models\n",
      "  4. Show predictions\n",
      "  5. Save results for listening\n",
      "\n",
      "================================================================================\n",
      "STEP 1: SAMPLING NEW CLIPS\n",
      "================================================================================\n",
      "\n",
      "üì• Loading clip manifest...\n",
      "   ‚úÖ Total clips available: 1,053,610\n",
      "\n",
      "üì• Loading training files...\n",
      "   ‚úÖ Training files: 15,392\n",
      "\n",
      "üîç Filtering to NEW clips (not in training)...\n",
      "   ‚úÖ New clips available: 392,520\n",
      "\n",
      "üé≤ Sampling 20 random clips...\n",
      "\n",
      "üìä Sampled clips:\n",
      "   Logger distribution:\n",
      "\n",
      "üìÅ Sample file paths:\n",
      "   /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/wav/PAPCA_test/2823/20090322/wav/49C60185.wav\n",
      "   /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/wav/PAPCA_test/2823/20090922/wav/4AB8CA41.wav\n",
      "   /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/wav/PAPCA_test/2823/20090506/wav/4A0146F5.wav\n",
      "   /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/wav/PAPCA_test/2823/20090422/wav/49EE6F84.wav\n",
      "   /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/wav/PAPCA_test/2823/20090523/wav/4A186745.wav\n",
      "   ... and 15 more\n",
      "\n",
      "================================================================================\n",
      "STEP 2 (WORKAROUND): CHECKING FOR EXISTING FEATURES\n",
      "================================================================================\n",
      "\n",
      "üîç Looking for pre-computed features...\n",
      "   Found: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/features/embeds_preprocessed_50k/preprocessed_features_pca.parquet\n",
      "\n",
      "‚ö†Ô∏è  No existing features found\n",
      "   Need to extract features from scratch\n",
      "\n",
      "‚ö†Ô∏è  Need to extract features from scratch\n",
      "   This requires audio files and may take a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Proceed with feature extraction? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: EXTRACTING YAMNET FEATURES\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è  NOTE: This requires YAMNet model and audio loading\n",
      "   If you don't have YAMNet setup, we'll use a workaround\n",
      "\n",
      "üì• Loading YAMNet model...\n",
      "\n",
      "üéµ Processing 20 audio files...\n",
      "   [20/20] 4A97AE14.wav\n",
      "   ‚úÖ Extracted embeddings for 20 files\n",
      "\n",
      "================================================================================\n",
      "STEP 2B: EXTRACTING ECOACOUSTIC FEATURES\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è  NOTE: This requires librosa and audio loading\n",
      "\n",
      "üéµ Processing 20 audio files...\n",
      "   [20/20] 4A97AE14.wav\n",
      "   ‚úÖ Extracted features for 20 files\n",
      "   üìä Feature count: 17 (expected: 17)\n",
      "\n",
      "================================================================================\n",
      "STEP 3: PREPROCESSING FEATURES\n",
      "================================================================================\n",
      "\n",
      "üì• Loading PCA model...\n",
      "   ‚úÖ Loaded PCA: 39 components\n",
      "\n",
      "üîó Combining features...\n",
      "   ‚úÖ Combined shape: (20, 1041)\n",
      "\n",
      "üîÑ Applying PCA transformation...\n",
      "   ‚úÖ PCA shape: (20, 39)\n",
      "\n",
      "================================================================================\n",
      "STEP 4: RUNNING PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "üìä Model 1: Stage 1 Classifier (AMBIENT vs BIO)\n",
      "   ‚úÖ Predictions:\n",
      "      BIO: 19 clips\n",
      "      AMBIENT: 1 clips\n",
      "\n",
      "üìä Model 2: Two-Stage Classifier (AMBIENT vs BIO vs HUMAN)\n",
      "   ‚ö†Ô∏è  Could not load Two-Stage model: Can't get attribute 'TwoStageDetector' on <module '__main__'>\n",
      "\n",
      "================================================================================\n",
      "STEP 5: ANALYZING RESULTS\n",
      "================================================================================\n",
      "\n",
      "üìä PREDICTION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üéØ Stage 1 Model (AMBIENT vs BIO):\n",
      "   ----------------------------------------\n",
      "   AMBIENT :  1 clips | Avg conf: 1.000\n",
      "   BIO     : 19 clips | Avg conf: 1.000\n",
      "\n",
      "üéØ Two-Stage Model (AMBIENT vs BIO vs HUMAN):\n",
      "   ----------------------------------------\n",
      "\n",
      "‚≠ê HIGH CONFIDENCE PREDICTIONS (>0.9):\n",
      "   ----------------------------------------\n",
      "   No high confidence predictions\n",
      "\n",
      "‚ùì LOW CONFIDENCE PREDICTIONS (<0.6):\n",
      "   ----------------------------------------\n",
      "   49C60185.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   4AB8CA41.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   4A0146F5.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   49EE6F84.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   4A186745.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   4A9DD899.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   4A4C5AE0.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   4A9F2A19.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   4A550FC8.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   4A48EF87.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   47D4F882.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   49E0A959.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   4A1D6A79.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   4ABACB89.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   4A68EEE5.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   4AC0E0F5.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   4AC68965.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   4A3A8799.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   49EBD009.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "   4A97AE14.wav                             ‚Üí UNKNOWN  (0.000)\n",
      "\n",
      "‚ö†Ô∏è  MODEL DISAGREEMENTS (Stage1 != Two-Stage):\n",
      "   ----------------------------------------\n",
      "   49C60185.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   4AB8CA41.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   4A0146F5.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   49EE6F84.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   4A186745.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   4A9DD899.wav                            \n",
      "      Stage1: AMBIENT  (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   4A4C5AE0.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   4A9F2A19.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   4A550FC8.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   4A48EF87.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   47D4F882.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   49E0A959.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   4A1D6A79.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   4ABACB89.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   4A68EEE5.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   4AC0E0F5.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   4AC68965.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   4A3A8799.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   49EBD009.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "   4A97AE14.wav                            \n",
      "      Stage1: BIO      (c=1.000)\n",
      "      Two-Stage: UNKNOWN  (c=0.000)\n",
      "\n",
      "\n",
      "üîç INTERESTING FINDS:\n",
      "   ----------------------------------------\n",
      "   No HUMAN sounds detected (clean reef!)\n",
      "\n",
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Saved: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/model_testing/results/test_predictions.csv\n",
      "\n",
      "üìÅ All results in: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/model_testing/results\n",
      "\n",
      "================================================================================\n",
      "‚úÖ TESTING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üéµ Next steps:\n",
      "   1. Check results in: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/model_testing/results\n",
      "   2. Listen to clips in each category\n",
      "   3. Verify predictions match what you hear\n",
      "   4. Report any errors/disagreements\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Run complete testing pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üß™ MODEL TESTING ON NEW AUDIO CLIPS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nThis will:\")\n",
    "    print(f\"  1. Sample {Config.N_TEST_CLIPS} random clips from 1M collection\")\n",
    "    print(f\"  2. Extract features (YAMNet + ecoacoustic)\")\n",
    "    print(f\"  3. Run through trained models\")\n",
    "    print(f\"  4. Show predictions\")\n",
    "    print(f\"  5. Save results for listening\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Sample new clips\n",
    "        sampled_df = sample_new_clips(Config.N_TEST_CLIPS)\n",
    "        \n",
    "        # Step 2: Get features (try workaround first)\n",
    "        features_df = create_features_workaround(sampled_df)\n",
    "        \n",
    "        if features_df is None or len(features_df) == 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  Need to extract features from scratch\")\n",
    "            print(f\"   This requires audio files and may take a while...\")\n",
    "            \n",
    "            response = input(f\"\\nProceed with feature extraction? (y/n): \")\n",
    "            if response.lower() != 'y':\n",
    "                print(f\"\\n‚ùå Aborted. Try using clips that have existing features.\")\n",
    "                return\n",
    "            \n",
    "            # Extract features\n",
    "            audio_paths = sampled_df['filepath'].tolist()\n",
    "            embeddings = extract_yamnet_embeddings(audio_paths)\n",
    "            ecoacoustic = extract_ecoacoustic_features(audio_paths)\n",
    "            \n",
    "            if embeddings is None or ecoacoustic is None:\n",
    "                print(f\"\\n‚ùå Feature extraction failed\")\n",
    "                print(f\"   Please check YAMNet model and audio files\")\n",
    "                return\n",
    "            \n",
    "            # Preprocess\n",
    "            pca_features = preprocess_features(embeddings, ecoacoustic)\n",
    "            \n",
    "            # Create DataFrame\n",
    "            features_df = sampled_df.copy()\n",
    "            for i, col in enumerate(Config.FEATURE_COLS):\n",
    "                features_df[col] = pca_features[:, i]\n",
    "        \n",
    "        # Step 4: Predict\n",
    "        results_df = predict_with_models(features_df)\n",
    "        \n",
    "        # Step 5: Analyze\n",
    "        analyze_predictions(results_df)\n",
    "        \n",
    "        # Step 6: Save\n",
    "        save_results(results_df)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"‚úÖ TESTING COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(f\"\\nüéµ Next steps:\")\n",
    "        print(f\"   1. Check results in: {Config.RESULTS_DIR}\")\n",
    "        print(f\"   2. Listen to clips in each category\")\n",
    "        print(f\"   3. Verify predictions match what you hear\")\n",
    "        print(f\"   4. Report any errors/disagreements\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        print(f\"\\nüí° Troubleshooting:\")\n",
    "        print(f\"   - Check file paths in Config\")\n",
    "        print(f\"   - Ensure models are trained\")\n",
    "        print(f\"   - Verify audio files exist\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac27b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reef_zmsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
