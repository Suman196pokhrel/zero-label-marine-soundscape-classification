{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bfd159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Paths\n",
    "HOME = Path(os.environ[\"HOME\"])\n",
    "REPO_ROOT = HOME / \"Uni-stuff/semester-2/applied_Ml/reef_zmsc\"\n",
    "\n",
    "CLUSTERS_PATH = REPO_ROOT / \"data/clusters/clusters_hdbscan.parquet\"\n",
    "FUSED_FEATURES_PATH = REPO_ROOT / \"data/features/embeds_fused_pilot/PAPCA\"\n",
    "OUTPUT_BASE = REPO_ROOT / \"data/auto_labels\"\n",
    "PLOTS_DIR = REPO_ROOT / \"plots/auto_labels\"\n",
    "\n",
    "# Label categories\n",
    "CATEGORIES = {\n",
    "    'biological': 'Biological sounds (fish, marine mammals, invertebrates)',\n",
    "    'anthropogenic': 'Human-made sounds (boats, ships, construction)',\n",
    "    'ambient': 'Natural environmental sounds (waves, weather, currents)',\n",
    "    'unknown': 'Uncertain or mixed sounds'\n",
    "}\n",
    "\n",
    "\n",
    "def load_cluster_features():\n",
    "    \"\"\"Load clusters with their original ecoacoustic features.\"\"\"\n",
    "    print(\"Loading clustered data with features...\")\n",
    "    \n",
    "    # Load all fused features\n",
    "    all_data = []\n",
    "    for parquet_file in FUSED_FEATURES_PATH.rglob(\"features.parquet\"):\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "        all_data.append(df)\n",
    "    \n",
    "    fused_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Load cluster assignments\n",
    "    clusters_df = pd.read_parquet(CLUSTERS_PATH)\n",
    "    \n",
    "    # Merge\n",
    "    merged = clusters_df.merge(\n",
    "        fused_df, \n",
    "        on=['logger', 'date', 'start_s'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded {len(merged):,} clips with features\")\n",
    "    return merged\n",
    "\n",
    "\n",
    "def compute_cluster_feature_profiles(df):\n",
    "    \"\"\"Compute feature statistics per cluster.\"\"\"\n",
    "    print(\"\\nComputing cluster feature profiles...\")\n",
    "    \n",
    "    # Ecoacoustic feature columns\n",
    "    eco_features = [\n",
    "        'spectral_centroid_mean', 'spectral_centroid_std',\n",
    "        'spectral_bandwidth_mean', 'spectral_rolloff_mean',\n",
    "        'spectral_flatness_mean', 'spectral_contrast_mean',\n",
    "        'aci', 'spectral_entropy', 'temporal_entropy',\n",
    "        'zcr_mean', 'rms_mean', 'rms_std', 'dynamic_range_db',\n",
    "        'snr_db', 'low_freq_energy', 'mid_freq_energy', 'high_freq_energy'\n",
    "    ]\n",
    "    \n",
    "    profiles = []\n",
    "    \n",
    "    for cluster_id in sorted(df['cluster'].unique()):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        \n",
    "        cluster_data = df[df['cluster'] == cluster_id]\n",
    "        \n",
    "        profile = {'cluster_id': cluster_id, 'size': len(cluster_data)}\n",
    "        \n",
    "        # Compute mean and std for each feature\n",
    "        for feat in eco_features:\n",
    "            if feat in cluster_data.columns:\n",
    "                profile[f'{feat}_mean'] = cluster_data[feat].mean()\n",
    "                profile[f'{feat}_std'] = cluster_data[feat].std()\n",
    "        \n",
    "        profiles.append(profile)\n",
    "    \n",
    "    return pd.DataFrame(profiles)\n",
    "\n",
    "\n",
    "def apply_labeling_rules(profiles):\n",
    "    \"\"\"\n",
    "    Apply weak supervision rules based on acoustic features.\n",
    "    Returns label and confidence for each cluster.\n",
    "    \"\"\"\n",
    "    print(\"\\nApplying auto-labeling rules...\")\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for _, row in profiles.iterrows():\n",
    "        cluster_id = row['cluster_id']\n",
    "        \n",
    "        # Extract key features (already normalized 0-1)\n",
    "        low_freq = row.get('low_freq_energy_mean_mean', 0.5)\n",
    "        mid_freq = row.get('mid_freq_energy_mean_mean', 0.3)\n",
    "        high_freq = row.get('high_freq_energy_mean_mean', 0.2)\n",
    "        \n",
    "        spectral_flatness = row.get('spectral_flatness_mean_mean', 0.5)\n",
    "        aci = row.get('aci_mean', 0.5)\n",
    "        temporal_entropy = row.get('temporal_entropy_mean_mean', 0.5)\n",
    "        snr = row.get('snr_db_mean', 0.5)\n",
    "        dynamic_range = row.get('dynamic_range_db_mean', 0.5)\n",
    "        spectral_centroid = row.get('spectral_centroid_mean_mean', 0.5)\n",
    "        \n",
    "        # Initialize scores\n",
    "        bio_score = 0\n",
    "        anthro_score = 0\n",
    "        ambient_score = 0\n",
    "        \n",
    "        # === BIOLOGICAL RULES ===\n",
    "        \n",
    "        # Rule B1: High-frequency energy (snapping shrimp signature)\n",
    "        if high_freq > 0.15:  # Lowered threshold\n",
    "            bio_score += 3\n",
    "            if high_freq > 0.25:\n",
    "                bio_score += 2  # Strong signal\n",
    "        \n",
    "        # Rule B2: High ACI (complex temporal patterns = biological activity)\n",
    "        if aci > 0.45:\n",
    "            bio_score += 3\n",
    "            if aci > 0.6:\n",
    "                bio_score += 2\n",
    "        \n",
    "        # Rule B3: Mid-frequency + high dynamic range (fish vocalizations)\n",
    "        if mid_freq > 0.35 and dynamic_range > 0.25:\n",
    "            bio_score += 3\n",
    "        \n",
    "        # Rule B4: High temporal entropy (variable biological sounds)\n",
    "        if temporal_entropy > 0.65:\n",
    "            bio_score += 2\n",
    "        \n",
    "        # Rule B5: Moderate-to-high spectral centroid (not deep rumbles)\n",
    "        if 0.15 < spectral_centroid < 0.4:\n",
    "            bio_score += 1\n",
    "        \n",
    "        # === ANTHROPOGENIC RULES ===\n",
    "        \n",
    "        # Rule A1: Strong low-frequency dominance + tonal (boats/ships)\n",
    "        if low_freq > 0.6 and spectral_flatness < 0.1:\n",
    "            anthro_score += 4\n",
    "        \n",
    "        # Rule A2: Very low ACI (constant drone)\n",
    "        if aci < 0.35:\n",
    "            anthro_score += 2\n",
    "            if aci < 0.25:\n",
    "                anthro_score += 2\n",
    "        \n",
    "        # Rule A3: Low temporal entropy + low frequency (machinery)\n",
    "        if temporal_entropy < 0.5 and low_freq > 0.55:\n",
    "            anthro_score += 3\n",
    "        \n",
    "        # Rule A4: Very tonal signature (engines)\n",
    "        if spectral_flatness < 0.05:\n",
    "            anthro_score += 2\n",
    "        \n",
    "        # === AMBIENT RULES ===\n",
    "        \n",
    "        # Rule AM1: High spectral flatness (noise-like)\n",
    "        if spectral_flatness > 0.3:\n",
    "            ambient_score += 3\n",
    "            if spectral_flatness > 0.5:\n",
    "                ambient_score += 2\n",
    "        \n",
    "        # Rule AM2: Low SNR (diffuse sound)\n",
    "        if snr < 0.35:\n",
    "            ambient_score += 2\n",
    "        \n",
    "        # Rule AM3: Low dynamic range (constant level)\n",
    "        if dynamic_range < 0.15:\n",
    "            ambient_score += 2\n",
    "        \n",
    "        # Rule AM4: Balanced frequency distribution (broadband)\n",
    "        freq_balance = abs(low_freq - 0.33) + abs(mid_freq - 0.33) + abs(high_freq - 0.33)\n",
    "        if freq_balance < 0.3:  # All frequencies roughly equal\n",
    "            ambient_score += 2\n",
    "        \n",
    "        # === PENALTY RULES (prevent misclassification) ===\n",
    "        \n",
    "        # Penalize anthro if high-frequency present (boats are low-freq)\n",
    "        if high_freq > 0.2:\n",
    "            anthro_score = max(0, anthro_score - 2)\n",
    "        \n",
    "        # Penalize bio if too tonal (biology is more varied)\n",
    "        if spectral_flatness < 0.08:\n",
    "            bio_score = max(0, bio_score - 2)\n",
    "        \n",
    "        # Determine label based on scores\n",
    "        scores = {\n",
    "            'biological': bio_score,\n",
    "            'anthropogenic': anthro_score,\n",
    "            'ambient': ambient_score\n",
    "        }\n",
    "        \n",
    "        max_score = max(scores.values())\n",
    "        \n",
    "        # Abstain if no clear winner\n",
    "        if max_score < 3:\n",
    "            label = 'unknown'\n",
    "            confidence = 0.2\n",
    "        else:\n",
    "            label = max(scores, key=scores.get)\n",
    "            \n",
    "            # Confidence based on score margin\n",
    "            sorted_scores = sorted(scores.values(), reverse=True)\n",
    "            margin = sorted_scores[0] - sorted_scores[1] if len(sorted_scores) > 1 else sorted_scores[0]\n",
    "            \n",
    "            # Higher threshold for confidence\n",
    "            confidence = min(margin / 6.0, 0.95)\n",
    "        \n",
    "        labels.append({\n",
    "            'cluster_id': cluster_id,\n",
    "            'auto_label': label,\n",
    "            'confidence': confidence,\n",
    "            'bio_score': bio_score,\n",
    "            'anthro_score': anthro_score,\n",
    "            'ambient_score': ambient_score\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9913710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_labels_to_clips(df, cluster_labels):\n",
    "    \"\"\"Apply cluster labels to all clips.\"\"\"\n",
    "    print(\"\\nPropagating labels to clips...\")\n",
    "    \n",
    "    # Merge cluster labels\n",
    "    df_labeled = df.merge(\n",
    "        cluster_labels[['cluster_id', 'auto_label', 'confidence']], \n",
    "        left_on='cluster', \n",
    "        right_on='cluster_id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Handle noise\n",
    "    df_labeled.loc[df_labeled['cluster'] == -1, 'auto_label'] = 'unknown'\n",
    "    df_labeled.loc[df_labeled['cluster'] == -1, 'confidence'] = 0.0\n",
    "    \n",
    "    return df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cf2df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_confidence(df_labeled, min_confidence=0.3):\n",
    "    \"\"\"Create high-confidence pseudo-labeled dataset.\"\"\"\n",
    "    print(f\"\\nFiltering by confidence >= {min_confidence}...\")\n",
    "    \n",
    "    high_conf = df_labeled[df_labeled['confidence'] >= min_confidence].copy()\n",
    "    \n",
    "    print(f\"High-confidence clips: {len(high_conf):,} / {len(df_labeled):,} ({len(high_conf)/len(df_labeled)*100:.1f}%)\")\n",
    "    \n",
    "    label_dist = high_conf['auto_label'].value_counts()\n",
    "    print(\"\\nLabel distribution (high confidence):\")\n",
    "    for label, count in label_dist.items():\n",
    "        print(f\"  {label}: {count:,} ({count/len(high_conf)*100:.1f}%)\")\n",
    "    \n",
    "    return high_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ffe1844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(cluster_labels, df_labeled, high_conf):\n",
    "    \"\"\"Create visualizations of auto-labeling results.\"\"\"\n",
    "    PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # 1. Cluster label distribution\n",
    "    ax1 = fig.add_subplot(2, 3, 1)\n",
    "    label_counts = cluster_labels['auto_label'].value_counts()\n",
    "    colors_map = {'biological': 'green', 'anthropogenic': 'red', 'ambient': 'blue', 'unknown': 'gray'}\n",
    "    colors = [colors_map.get(label, 'gray') for label in label_counts.index]\n",
    "    ax1.bar(range(len(label_counts)), label_counts.values, color=colors)\n",
    "    ax1.set_xticks(range(len(label_counts)))\n",
    "    ax1.set_xticklabels(label_counts.index, rotation=45)\n",
    "    ax1.set_ylabel('Number of Clusters')\n",
    "    ax1.set_title('Clusters by Auto-Label')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Confidence distribution per label\n",
    "    ax2 = fig.add_subplot(2, 3, 2)\n",
    "    for label in cluster_labels['auto_label'].unique():\n",
    "        label_data = cluster_labels[cluster_labels['auto_label'] == label]\n",
    "        ax2.hist(label_data['confidence'], bins=20, alpha=0.5, label=label)\n",
    "    ax2.set_xlabel('Confidence')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Confidence Distribution by Label')\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. Clips per label (all vs high-confidence)\n",
    "    ax3 = fig.add_subplot(2, 3, 3)\n",
    "    all_counts = df_labeled['auto_label'].value_counts()\n",
    "    high_counts = high_conf['auto_label'].value_counts()\n",
    "    \n",
    "    x = np.arange(len(all_counts))\n",
    "    width = 0.35\n",
    "    ax3.bar(x - width/2, all_counts.values, width, label='All clips', alpha=0.7)\n",
    "    ax3.bar(x + width/2, [high_counts.get(label, 0) for label in all_counts.index], \n",
    "            width, label='High confidence', alpha=0.7)\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(all_counts.index, rotation=45)\n",
    "    ax3.set_ylabel('Number of Clips')\n",
    "    ax3.set_title('Clips by Label (All vs High-Conf)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Score distribution\n",
    "    ax4 = fig.add_subplot(2, 3, 4)\n",
    "    score_cols = ['bio_score', 'anthro_score', 'ambient_score']\n",
    "    for col in score_cols:\n",
    "        ax4.hist(cluster_labels[col], bins=10, alpha=0.5, label=col.replace('_score', ''))\n",
    "    ax4.set_xlabel('Rule Score')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Rule Scores Distribution')\n",
    "    ax4.legend()\n",
    "    \n",
    "    # 5. Confidence vs cluster size\n",
    "    ax5 = fig.add_subplot(2, 3, 5)\n",
    "    scatter = ax5.scatter(cluster_labels['cluster_id'], cluster_labels['confidence'],\n",
    "                         c=cluster_labels['auto_label'].map(colors_map), alpha=0.6)\n",
    "    ax5.set_xlabel('Cluster ID')\n",
    "    ax5.set_ylabel('Confidence')\n",
    "    ax5.set_title('Confidence per Cluster')\n",
    "    ax5.axhline(0.5, color='r', linestyle='--', alpha=0.5, label='Threshold')\n",
    "    ax5.legend()\n",
    "    \n",
    "    # 6. Label transitions (sankey-style bar)\n",
    "    ax6 = fig.add_subplot(2, 3, 6)\n",
    "    filtered_out = len(df_labeled) - len(high_conf)\n",
    "    retained_by_label = high_conf['auto_label'].value_counts()\n",
    "    \n",
    "    data = [filtered_out] + list(retained_by_label.values)\n",
    "    labels_bar = ['Filtered\\n(low conf)'] + [f'{label}\\n(high conf)' for label in retained_by_label.index]\n",
    "    colors_bar = ['lightgray'] + [colors_map.get(label, 'gray') for label in retained_by_label.index]\n",
    "    \n",
    "    ax6.bar(range(len(data)), data, color=colors_bar)\n",
    "    ax6.set_xticks(range(len(data)))\n",
    "    ax6.set_xticklabels(labels_bar, rotation=45, ha='right')\n",
    "    ax6.set_ylabel('Number of Clips')\n",
    "    ax6.set_title('Filtering by Confidence')\n",
    "    ax6.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = PLOTS_DIR / \"auto_labeling_overview.png\"\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nPlot saved: {plot_path.relative_to(REPO_ROOT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c7e6202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(cluster_labels, df_labeled, high_conf):\n",
    "    \"\"\"Save auto-labeled datasets.\"\"\"\n",
    "    OUTPUT_BASE.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save cluster-level labels\n",
    "    cluster_labels.to_csv(OUTPUT_BASE / \"cluster_auto_labels.csv\", index=False)\n",
    "    print(f\"\\nCluster labels saved: {OUTPUT_BASE / 'cluster_auto_labels.csv'}\")\n",
    "    \n",
    "    # Save all clips with labels\n",
    "    df_labeled.to_parquet(OUTPUT_BASE / \"clips_auto_labeled_all.parquet\", index=False)\n",
    "    print(f\"All labeled clips: {OUTPUT_BASE / 'clips_auto_labeled_all.parquet'}\")\n",
    "    \n",
    "    # Save high-confidence subset (for training)\n",
    "    high_conf.to_parquet(OUTPUT_BASE / \"clips_auto_labeled_high_confidence.parquet\", index=False)\n",
    "    print(f\"High-confidence clips: {OUTPUT_BASE / 'clips_auto_labeled_high_confidence.parquet'}\")\n",
    "    \n",
    "    # Save summary\n",
    "    with open(OUTPUT_BASE / \"labeling_summary.txt\", 'w') as f:\n",
    "        f.write(\"AUTO-LABELING SUMMARY\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Total clips: {len(df_labeled):,}\\n\")\n",
    "        f.write(f\"High-confidence clips: {len(high_conf):,} ({len(high_conf)/len(df_labeled)*100:.1f}%)\\n\\n\")\n",
    "        \n",
    "        f.write(\"All clips label distribution:\\n\")\n",
    "        for label, count in df_labeled['auto_label'].value_counts().items():\n",
    "            f.write(f\"  {label}: {count:,} ({count/len(df_labeled)*100:.1f}%)\\n\")\n",
    "        \n",
    "        f.write(\"\\nHigh-confidence label distribution:\\n\")\n",
    "        for label, count in high_conf['auto_label'].value_counts().items():\n",
    "            f.write(f\"  {label}: {count:,} ({count/len(high_conf)*100:.1f}%)\\n\")\n",
    "    \n",
    "    print(f\"Summary saved: {OUTPUT_BASE / 'labeling_summary.txt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686432d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AUTO-LABELING WITH WEAK SUPERVISION\n",
      "============================================================\n",
      "Loading clustered data with features...\n",
      "Loaded 786,180 clips with features\n",
      "\n",
      "Computing cluster feature profiles...\n",
      "\n",
      "Applying auto-labeling rules...\n",
      "\n",
      "Cluster labeling results:\n",
      "auto_label\n",
      "anthropogenic    177\n",
      "dtype: int64\n",
      "\n",
      "Propagating labels to clips...\n",
      "\n",
      "Filtering by confidence >= 0.5...\n",
      "High-confidence clips: 19,523 / 786,180 (2.5%)\n",
      "\n",
      "Label distribution (high confidence):\n",
      "  anthropogenic: 19,523 (100.0%)\n",
      "\n",
      "Plot saved: plots/auto_labels/auto_labeling_overview.png\n",
      "\n",
      "Cluster labels saved: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/auto_labels/cluster_auto_labels.csv\n",
      "All labeled clips: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/auto_labels/clips_auto_labeled_all.parquet\n",
      "High-confidence clips: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/auto_labels/clips_auto_labeled_high_confidence.parquet\n",
      "Summary saved: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/auto_labels/labeling_summary.txt\n",
      "\n",
      "============================================================\n",
      "AUTO-LABELING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Next step: Train classifier on high-confidence pseudo-labels\n",
      "  Training data: 19,523 clips\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"AUTO-LABELING WITH WEAK SUPERVISION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load data\n",
    "    df = load_cluster_features()\n",
    "    \n",
    "    # Compute cluster feature profiles\n",
    "    profiles = compute_cluster_feature_profiles(df)\n",
    "    \n",
    "    # Apply labeling rules\n",
    "    cluster_labels = apply_labeling_rules(profiles)\n",
    "    \n",
    "    print(\"\\nCluster labeling results:\")\n",
    "    print(cluster_labels.groupby('auto_label').size())\n",
    "    \n",
    "    # Apply labels to all clips\n",
    "    df_labeled = apply_labels_to_clips(df, cluster_labels)\n",
    "    \n",
    "    # Filter by confidence\n",
    "    high_conf = filter_by_confidence(df_labeled, min_confidence=0.5)\n",
    "    \n",
    "    # Visualize\n",
    "    create_visualizations(cluster_labels, df_labeled, high_conf)\n",
    "    \n",
    "    # Save\n",
    "    save_results(cluster_labels, df_labeled, high_conf)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AUTO-LABELING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nNext step: Train classifier on high-confidence pseudo-labels\")\n",
    "    print(f\"  Training data: {len(high_conf):,} clips\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203c51f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reef_zmsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
