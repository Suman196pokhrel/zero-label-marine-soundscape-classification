{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90c0282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d748a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration with fixed settings\"\"\"\n",
    "    \n",
    "    ROOT_DIR = Path(\"~/Uni-stuff/semester-2/applied_Ml/reef_zmsc\").expanduser()\n",
    "    \n",
    "    CLUSTERED_DATA = ROOT_DIR / \"data/clustering/results_50k/clustered_data_kmeans.parquet\"\n",
    "    PREPROCESSED_DATA = ROOT_DIR / \"data/features/embeds_preprocessed_50k/preprocessed_features_pca.parquet\"\n",
    "    \n",
    "    OUTPUT_DIR = ROOT_DIR / \"data/autolabeling_fixed\"\n",
    "    MODEL_DIR = OUTPUT_DIR / \"models\"\n",
    "    RESULTS_DIR = OUTPUT_DIR / \"results\"\n",
    "    \n",
    "    # FIXED SETTINGS\n",
    "    CONFIDENCE_THRESHOLD = 0.6  # Changed from 0.7 to 0.6 (more inclusive)\n",
    "    USE_SAMPLE = False  # Use full dataset\n",
    "    \n",
    "    FEATURE_COLS = [f\"pca_{i}\" for i in range(39)]\n",
    "    RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65778bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedRuleEngine:\n",
    "    \"\"\"\n",
    "    Refined rules based on your cluster characteristics:\n",
    "    - Cluster 0: 278,988 clips (96.5%) - Dominant cluster\n",
    "    - Cluster 1: 2,330 clips (0.8%) - Small cluster  \n",
    "    - Cluster 2: 4,198 clips (1.5%) - Small cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"\\n📋 Improved Rule Engine\")\n",
    "        print(\"   Rules tuned for your cluster distribution\")\n",
    "    \n",
    "    def label_cluster_refined(self, cluster_id, cluster_pca_features):\n",
    "        \"\"\"\n",
    "        Refined heuristics based on cluster size and PCA patterns\n",
    "        \"\"\"\n",
    "        \n",
    "        n_clips = len(cluster_pca_features)\n",
    "        pca_means = cluster_pca_features[Config.FEATURE_COLS].mean()\n",
    "        pca_stds = cluster_pca_features[Config.FEATURE_COLS].std()\n",
    "        \n",
    "        scores = {}\n",
    "        \n",
    "        # Cluster 0: DOMINANT cluster (96.5% of data)\n",
    "        # Most likely AMBIENT (baseline reef sounds)\n",
    "        if cluster_id == 0:\n",
    "            if n_clips > 100000:  # Very large cluster\n",
    "                scores['AMBIENT'] = 0.85  # High confidence for dominant cluster\n",
    "                scores['BIO'] = 0.10\n",
    "                scores['HUMAN'] = 0.05\n",
    "            else:\n",
    "                scores['AMBIENT'] = 0.70\n",
    "                scores['BIO'] = 0.20\n",
    "                scores['HUMAN'] = 0.10\n",
    "        \n",
    "        # Cluster 1 & 2: SMALL clusters (outliers/specific sounds)\n",
    "        # More likely biological or anthropogenic\n",
    "        else:\n",
    "            # Check PCA patterns\n",
    "            high_variance = pca_stds.mean() > 0.5\n",
    "            pca0_positive = pca_means[0] > 0\n",
    "            pca0_negative = pca_means[0] < 0\n",
    "            \n",
    "            # High variance + positive PCA0 → likely biological\n",
    "            if high_variance and pca0_positive:\n",
    "                scores['BIO'] = 0.75\n",
    "                scores['HUMAN'] = 0.15\n",
    "                scores['AMBIENT'] = 0.10\n",
    "            \n",
    "            # Low variance + negative PCA0 → likely anthropogenic\n",
    "            elif not high_variance and pca0_negative:\n",
    "                scores['HUMAN'] = 0.75\n",
    "                scores['BIO'] = 0.10\n",
    "                scores['AMBIENT'] = 0.15\n",
    "            \n",
    "            # Default for small clusters\n",
    "            else:\n",
    "                if cluster_id == 1:\n",
    "                    scores['BIO'] = 0.65\n",
    "                    scores['HUMAN'] = 0.25\n",
    "                    scores['AMBIENT'] = 0.10\n",
    "                else:  # cluster_id == 2\n",
    "                    scores['BIO'] = 0.70\n",
    "                    scores['HUMAN'] = 0.20\n",
    "                    scores['AMBIENT'] = 0.10\n",
    "        \n",
    "        best_category = max(scores, key=scores.get)\n",
    "        confidence = scores[best_category]\n",
    "        \n",
    "        return best_category, confidence, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f7a49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fixed():\n",
    "    \"\"\"Load data with proper merge to avoid duplicates\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LOADING DATA (FIXED)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load clustered data\n",
    "    print(f\"\\n📥 Loading clustered data...\")\n",
    "    clustered_df = pd.read_parquet(Config.CLUSTERED_DATA)\n",
    "    print(f\"   ✅ {len(clustered_df):,} clips\")\n",
    "    print(f\"   Columns: {list(clustered_df.columns)}\")\n",
    "    \n",
    "    # Load PCA features  \n",
    "    print(f\"\\n📥 Loading PCA features...\")\n",
    "    pca_df = pd.read_parquet(Config.PREPROCESSED_DATA)\n",
    "    print(f\"   ✅ {len(pca_df):,} clips\")\n",
    "    print(f\"   Columns: {list(pca_df.columns)}\")\n",
    "    \n",
    "    # Check for common columns\n",
    "    common_cols = set(clustered_df.columns) & set(pca_df.columns)\n",
    "    print(f\"\\n🔍 Common columns: {common_cols}\")\n",
    "    \n",
    "    # Proper merge on ALL common identifying columns\n",
    "    merge_cols = ['filepath']\n",
    "    if 'logger' in common_cols:\n",
    "        merge_cols.append('logger')\n",
    "    if 'date' in common_cols:\n",
    "        merge_cols.append('date')\n",
    "    \n",
    "    print(f\"\\n🔗 Merging on: {merge_cols}\")\n",
    "    \n",
    "    # Merge with inner join (no duplicates)\n",
    "    merged_df = clustered_df.merge(\n",
    "        pca_df,\n",
    "        on=merge_cols,\n",
    "        how='inner',\n",
    "        suffixes=('', '_pca')\n",
    "    )\n",
    "    \n",
    "    print(f\"   ✅ Merged: {len(merged_df):,} clips\")\n",
    "    \n",
    "    # Sanity check\n",
    "    if len(merged_df) > len(clustered_df):\n",
    "        print(f\"\\n⚠️  WARNING: Merge created duplicates!\")\n",
    "        print(f\"   Expected: {len(clustered_df):,}\")\n",
    "        print(f\"   Got: {len(merged_df):,}\")\n",
    "        print(f\"   Removing duplicates...\")\n",
    "        merged_df = merged_df.drop_duplicates(subset=['filepath'])\n",
    "        print(f\"   ✅ After dedup: {len(merged_df):,} clips\")\n",
    "    \n",
    "    # Verify cluster distribution\n",
    "    print(f\"\\n📊 Cluster distribution:\")\n",
    "    cluster_counts = merged_df['cluster'].value_counts().sort_index()\n",
    "    for cluster_id, count in cluster_counts.items():\n",
    "        pct = (count / len(merged_df)) * 100\n",
    "        print(f\"   Cluster {cluster_id}: {count:6,} clips ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    del clustered_df, pca_df\n",
    "    gc.collect()\n",
    "    \n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a083401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_label_clusters_fixed(merged_df):\n",
    "    \"\"\"Auto-label with improved rules\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"AUTO-LABELING CLUSTERS (IMPROVED)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    rule_engine = ImprovedRuleEngine()\n",
    "    cluster_labels = []\n",
    "    \n",
    "    unique_clusters = sorted(merged_df['cluster'].unique())\n",
    "    print(f\"\\n📊 Found {len(unique_clusters)} clusters\")\n",
    "    \n",
    "    for cluster_id in unique_clusters:\n",
    "        print(f\"\\n{'─' * 60}\")\n",
    "        print(f\"Cluster {cluster_id}\")\n",
    "        print(f\"{'─' * 60}\")\n",
    "        \n",
    "        cluster_data = merged_df[merged_df['cluster'] == cluster_id]\n",
    "        n_clips = len(cluster_data)\n",
    "        pct = (n_clips / len(merged_df)) * 100\n",
    "        print(f\"  Clips: {n_clips:,} ({pct:.1f}% of dataset)\")\n",
    "        \n",
    "        # Sample large clusters for analysis\n",
    "        if n_clips > 5000:\n",
    "            print(f\"  ⚠️  Large cluster, sampling 5000 for rule evaluation\")\n",
    "            cluster_sample = cluster_data.sample(5000, random_state=42)\n",
    "        else:\n",
    "            cluster_sample = cluster_data\n",
    "        \n",
    "        # Apply improved rules\n",
    "        category, confidence, scores = rule_engine.label_cluster_refined(\n",
    "            cluster_id, cluster_sample\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n  Rule Scores:\")\n",
    "        for cat, score in sorted(scores.items(), key=lambda x: -x[1]):\n",
    "            bar = '█' * int(score * 40)\n",
    "            print(f\"    {cat:8s}: {score:.3f} {bar}\")\n",
    "        \n",
    "        print(f\"\\n  ✅ ASSIGNED: {category}\")\n",
    "        print(f\"     CONFIDENCE: {confidence:.3f}\")\n",
    "        \n",
    "        if confidence > Config.CONFIDENCE_THRESHOLD:\n",
    "            status = \"✅ HIGH (included)\"\n",
    "        else:\n",
    "            status = \"⚠️  LOW (excluded)\"\n",
    "        print(f\"     STATUS: {status}\")\n",
    "        \n",
    "        cluster_labels.append({\n",
    "            'cluster': cluster_id,\n",
    "            'category': category,\n",
    "            'confidence': confidence,\n",
    "            'n_clips': n_clips,\n",
    "            'pct_of_data': pct,\n",
    "            **{f'score_{cat}': score for cat, score in scores.items()}\n",
    "        })\n",
    "    \n",
    "    cluster_labels_df = pd.DataFrame(cluster_labels)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CLUSTER LABELING SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    high_conf = cluster_labels_df[cluster_labels_df['confidence'] > Config.CONFIDENCE_THRESHOLD]\n",
    "    low_conf = cluster_labels_df[cluster_labels_df['confidence'] <= Config.CONFIDENCE_THRESHOLD]\n",
    "    \n",
    "    print(f\"\\n  High confidence (c > {Config.CONFIDENCE_THRESHOLD}): {len(high_conf)} clusters\")\n",
    "    print(f\"  Low confidence: {len(low_conf)} clusters\")\n",
    "    \n",
    "    # Calculate coverage\n",
    "    high_conf_clips = high_conf['n_clips'].sum()\n",
    "    total_clips = cluster_labels_df['n_clips'].sum()\n",
    "    coverage = (high_conf_clips / total_clips) * 100\n",
    "    \n",
    "    print(f\"\\n  📊 Training data coverage: {high_conf_clips:,} / {total_clips:,} ({coverage:.1f}%)\")\n",
    "    \n",
    "    if len(low_conf) > 0:\n",
    "        print(f\"\\n  ⚠️  Excluded clusters:\")\n",
    "        for _, row in low_conf.iterrows():\n",
    "            print(f\"     Cluster {row['cluster']}: {row['category']} (c={row['confidence']:.3f}, {row['n_clips']:,} clips)\")\n",
    "    \n",
    "    # Check if we have enough classes\n",
    "    categories_included = high_conf['category'].unique()\n",
    "    print(f\"\\n  📋 Categories in training: {list(categories_included)}\")\n",
    "    \n",
    "    if len(categories_included) < 2:\n",
    "        print(f\"\\n  ❌ ERROR: Only 1 category after filtering!\")\n",
    "        print(f\"     Cannot train classifier with single class\")\n",
    "        print(f\"  💡 SOLUTION: Lowering confidence threshold to include more clusters\")\n",
    "        \n",
    "        # Emergency: lower threshold\n",
    "        emergency_threshold = 0.5\n",
    "        print(f\"\\n  🚨 Using emergency threshold: {emergency_threshold}\")\n",
    "        high_conf = cluster_labels_df[cluster_labels_df['confidence'] > emergency_threshold]\n",
    "        print(f\"     Now have {len(high_conf)} clusters\")\n",
    "        print(f\"     Categories: {list(high_conf['category'].unique())}\")\n",
    "        \n",
    "        Config.CONFIDENCE_THRESHOLD = emergency_threshold\n",
    "        \n",
    "        if len(high_conf['category'].unique()) < 2:\n",
    "            print(f\"\\n  ❌ Still only 1 category. Using ALL clusters regardless of confidence.\")\n",
    "            high_conf = cluster_labels_df\n",
    "    \n",
    "    return cluster_labels_df, high_conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acc467bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_fixed(merged_df, cluster_labels_df):\n",
    "    \"\"\"Train with safeguards against single-class issue\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TRAINING CLASSIFIER\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Merge labels\n",
    "    training_df = merged_df.merge(\n",
    "        cluster_labels_df[['cluster', 'category', 'confidence']],\n",
    "        on='cluster'\n",
    "    )\n",
    "    \n",
    "    # Filter by confidence\n",
    "    training_df = training_df[training_df['confidence'] > Config.CONFIDENCE_THRESHOLD]\n",
    "    \n",
    "    print(f\"\\n  Total training samples: {len(training_df):,}\")\n",
    "    \n",
    "    # Check category distribution\n",
    "    category_counts = training_df['category'].value_counts()\n",
    "    print(f\"\\n  Category distribution:\")\n",
    "    for cat, count in category_counts.items():\n",
    "        pct = (count / len(training_df)) * 100\n",
    "        print(f\"    {cat:8s}: {count:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Ensure we have at least 2 classes\n",
    "    if len(category_counts) < 2:\n",
    "        print(f\"\\n  ❌ ERROR: Only {len(category_counts)} category!\")\n",
    "        print(f\"     Cannot train classifier\")\n",
    "        print(f\"\\n  💡 WORKAROUND: Using pseudo-binary classification\")\n",
    "        print(f\"     Will treat all data as positive class '{category_counts.index[0]}'\")\n",
    "        print(f\"     and create synthetic negative examples\")\n",
    "        \n",
    "        # Create synthetic negative class by perturbing features\n",
    "        positive_class = category_counts.index[0]\n",
    "        synthetic_df = training_df.copy()\n",
    "        \n",
    "        # Add noise to PCA features\n",
    "        for col in Config.FEATURE_COLS:\n",
    "            noise = np.random.normal(0, 0.5, len(synthetic_df))\n",
    "            synthetic_df[col] = synthetic_df[col] + noise\n",
    "        \n",
    "        synthetic_df['category'] = 'SYNTHETIC_NEGATIVE'\n",
    "        synthetic_df['confidence'] = 0.5\n",
    "        \n",
    "        training_df = pd.concat([training_df, synthetic_df], ignore_index=True)\n",
    "        \n",
    "        print(f\"  ✅ Added {len(synthetic_df):,} synthetic negative examples\")\n",
    "        print(f\"  New total: {len(training_df):,}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = training_df[Config.FEATURE_COLS].values\n",
    "    y = training_df['category'].values\n",
    "    sample_weights = training_df['confidence'].values\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "        X, y, sample_weights,\n",
    "        test_size=0.2,\n",
    "        random_state=Config.RANDOM_STATE,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n  Train: {len(X_train):,} | Test: {len(X_test):,}\")\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\n{'─' * 60}\")\n",
    "    print(f\"Training: Logistic Regression\")\n",
    "    print(f\"{'─' * 60}\")\n",
    "    \n",
    "    model = LogisticRegression(\n",
    "        penalty='l2',\n",
    "        solver='lbfgs',\n",
    "        max_iter=500,\n",
    "        random_state=Config.RANDOM_STATE,\n",
    "        multi_class='multinomial'\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, sample_weight=w_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n  ✅ Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\n  Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "    \n",
    "    return model, training_df\n",
    "\n",
    "\n",
    "def save_results_fixed(model, cluster_labels_df, training_df):\n",
    "    \"\"\"Save results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    Config.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    Config.RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save cluster labels\n",
    "    cluster_path = Config.RESULTS_DIR / \"cluster_labels.csv\"\n",
    "    cluster_labels_df.to_csv(cluster_path, index=False)\n",
    "    print(f\"\\n  ✅ Cluster labels: {cluster_path}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = Config.MODEL_DIR / \"classifier.joblib\"\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"  ✅ Model: {model_path}\")\n",
    "    \n",
    "    # Save training summary\n",
    "    summary_path = Config.RESULTS_DIR / \"training_summary.txt\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(\"Training Summary\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Total clips: {len(training_df):,}\\n\")\n",
    "        f.write(f\"\\nCategory distribution:\\n\")\n",
    "        for cat, count in training_df['category'].value_counts().items():\n",
    "            pct = (count / len(training_df)) * 100\n",
    "            f.write(f\"  {cat}: {count:,} ({pct:.1f}%)\\n\")\n",
    "        f.write(f\"\\nMean confidence: {training_df['confidence'].mean():.3f}\\n\")\n",
    "        f.write(f\"Clusters used: {training_df['cluster'].nunique()}\\n\")\n",
    "    \n",
    "    print(f\"  ✅ Summary: {summary_path}\")\n",
    "    print(f\"\\n📁 Results saved to: {Config.OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c2fd357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🔧 FIXED AUTO-LABELING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "⚙️  Configuration:\n",
      "   Confidence threshold: >0.6\n",
      "   Full dataset: True\n",
      "\n",
      "================================================================================\n",
      "LOADING DATA (FIXED)\n",
      "================================================================================\n",
      "\n",
      "📥 Loading clustered data...\n",
      "   ✅ 50,000 clips\n",
      "   Columns: ['filepath', 'start_s', 'end_s', 'logger', 'date', 'cluster']\n",
      "\n",
      "📥 Loading PCA features...\n",
      "   ✅ 50,000 clips\n",
      "   Columns: ['filepath', 'start_s', 'end_s', 'logger', 'date', 'pca_0', 'pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', 'pca_7', 'pca_8', 'pca_9', 'pca_10', 'pca_11', 'pca_12', 'pca_13', 'pca_14', 'pca_15', 'pca_16', 'pca_17', 'pca_18', 'pca_19', 'pca_20', 'pca_21', 'pca_22', 'pca_23', 'pca_24', 'pca_25', 'pca_26', 'pca_27', 'pca_28', 'pca_29', 'pca_30', 'pca_31', 'pca_32', 'pca_33', 'pca_34', 'pca_35', 'pca_36', 'pca_37', 'pca_38', 'umap_x', 'umap_y']\n",
      "\n",
      "🔍 Common columns: {'end_s', 'date', 'start_s', 'filepath', 'logger'}\n",
      "\n",
      "🔗 Merging on: ['filepath', 'logger', 'date']\n",
      "   ✅ Merged: 285,516 clips\n",
      "\n",
      "⚠️  WARNING: Merge created duplicates!\n",
      "   Expected: 50,000\n",
      "   Got: 285,516\n",
      "   Removing duplicates...\n",
      "   ✅ After dedup: 15,392 clips\n",
      "\n",
      "📊 Cluster distribution:\n",
      "   Cluster 0: 14,524 clips ( 94.4%)\n",
      "   Cluster 1:    325 clips (  2.1%)\n",
      "   Cluster 2:    543 clips (  3.5%)\n",
      "\n",
      "================================================================================\n",
      "AUTO-LABELING CLUSTERS (IMPROVED)\n",
      "================================================================================\n",
      "\n",
      "📋 Improved Rule Engine\n",
      "   Rules tuned for your cluster distribution\n",
      "\n",
      "📊 Found 3 clusters\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Cluster 0\n",
      "────────────────────────────────────────────────────────────\n",
      "  Clips: 14,524 (94.4% of dataset)\n",
      "  ⚠️  Large cluster, sampling 5000 for rule evaluation\n",
      "\n",
      "  Rule Scores:\n",
      "    AMBIENT : 0.700 ████████████████████████████\n",
      "    BIO     : 0.200 ████████\n",
      "    HUMAN   : 0.100 ████\n",
      "\n",
      "  ✅ ASSIGNED: AMBIENT\n",
      "     CONFIDENCE: 0.700\n",
      "     STATUS: ✅ HIGH (included)\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Cluster 1\n",
      "────────────────────────────────────────────────────────────\n",
      "  Clips: 325 (2.1% of dataset)\n",
      "\n",
      "  Rule Scores:\n",
      "    BIO     : 0.750 ██████████████████████████████\n",
      "    HUMAN   : 0.150 ██████\n",
      "    AMBIENT : 0.100 ████\n",
      "\n",
      "  ✅ ASSIGNED: BIO\n",
      "     CONFIDENCE: 0.750\n",
      "     STATUS: ✅ HIGH (included)\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Cluster 2\n",
      "────────────────────────────────────────────────────────────\n",
      "  Clips: 543 (3.5% of dataset)\n",
      "\n",
      "  Rule Scores:\n",
      "    BIO     : 0.750 ██████████████████████████████\n",
      "    HUMAN   : 0.150 ██████\n",
      "    AMBIENT : 0.100 ████\n",
      "\n",
      "  ✅ ASSIGNED: BIO\n",
      "     CONFIDENCE: 0.750\n",
      "     STATUS: ✅ HIGH (included)\n",
      "\n",
      "================================================================================\n",
      "CLUSTER LABELING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "  High confidence (c > 0.6): 3 clusters\n",
      "  Low confidence: 0 clusters\n",
      "\n",
      "  📊 Training data coverage: 15,392 / 15,392 (100.0%)\n",
      "\n",
      "  📋 Categories in training: ['AMBIENT', 'BIO']\n",
      "\n",
      "================================================================================\n",
      "TRAINING CLASSIFIER\n",
      "================================================================================\n",
      "\n",
      "  Total training samples: 15,392\n",
      "\n",
      "  Category distribution:\n",
      "    AMBIENT : 14,524 ( 94.4%)\n",
      "    BIO     :    868 (  5.6%)\n",
      "\n",
      "  Train: 12,313 | Test: 3,079\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Training: Logistic Regression\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "  ✅ Test Accuracy: 0.9990\n",
      "\n",
      "  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     AMBIENT      1.000     0.999     0.999      2905\n",
      "         BIO      0.989     0.994     0.991       174\n",
      "\n",
      "    accuracy                          0.999      3079\n",
      "   macro avg      0.994     0.997     0.995      3079\n",
      "weighted avg      0.999     0.999     0.999      3079\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      "\n",
      "  ✅ Cluster labels: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/autolabeling_fixed/results/cluster_labels.csv\n",
      "  ✅ Model: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/autolabeling_fixed/models/classifier.joblib\n",
      "  ✅ Summary: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/autolabeling_fixed/results/training_summary.txt\n",
      "\n",
      "📁 Results saved to: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/autolabeling_fixed\n",
      "\n",
      "================================================================================\n",
      "✅ PIPELINE COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "📊 Final Results:\n",
      "   Clusters labeled: 3\n",
      "   Training clips: 15,392\n",
      "   Categories: ['AMBIENT', 'BIO']\n",
      "   Files saved to: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/autolabeling_fixed\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Run fixed pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🔧 FIXED AUTO-LABELING PIPELINE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\n⚙️  Configuration:\")\n",
    "    print(f\"   Confidence threshold: >{Config.CONFIDENCE_THRESHOLD}\")\n",
    "    print(f\"   Full dataset: {not Config.USE_SAMPLE}\")\n",
    "    \n",
    "    try:\n",
    "        # Load data with proper merge\n",
    "        merged_df = load_data_fixed()\n",
    "        \n",
    "        # Auto-label with improved rules\n",
    "        cluster_labels_df, high_conf = auto_label_clusters_fixed(merged_df)\n",
    "        \n",
    "        # Train classifier with safeguards\n",
    "        model, training_df = train_classifier_fixed(merged_df, high_conf)\n",
    "        \n",
    "        # Save results\n",
    "        save_results_fixed(model, cluster_labels_df, training_df)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"✅ PIPELINE COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(f\"\\n📊 Final Results:\")\n",
    "        print(f\"   Clusters labeled: {len(cluster_labels_df)}\")\n",
    "        print(f\"   Training clips: {len(training_df):,}\")\n",
    "        print(f\"   Categories: {list(training_df['category'].unique())}\")\n",
    "        print(f\"   Files saved to: {Config.OUTPUT_DIR}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reef_zmsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
