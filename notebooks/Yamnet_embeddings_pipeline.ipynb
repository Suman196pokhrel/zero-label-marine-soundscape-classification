{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd8406e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 06:39:59.578273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/sparch/.pyenv/versions/reef_zmsc/lib/python3.10/site-packages/tensorflow_hub/__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TF version: 2.20.0\n",
      "Librosa: 0.11.0\n",
      "\n",
      "Config loaded ✅\n",
      "Manifest: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/manifests/sample_50k_stratified.parquet\n",
      "Output root: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/features/embeds_yamnet_50k\n",
      "Limit windows: None\n",
      "Dry run: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, json, time, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "# ---- TensorFlow / TF-Hub setup (stable settings) ----\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # uncomment for CPU-only\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_auto_jit=0\"\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_cpu_use_thunk_runtime=false\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "tf.config.optimizer.set_jit(False)\n",
    "\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Librosa:\", librosa.__version__)\n",
    "\n",
    "# === Absolute roots ===\n",
    "HOME = Path(os.environ[\"HOME\"])\n",
    "REPO_ROOT = HOME / \"Uni-stuff/semester-2/applied_Ml/reef_zmsc\"\n",
    "\n",
    "# === Paths ===\n",
    "IN_MANIFEST = REPO_ROOT / \"data/manifests/sample_50k_stratified.parquet\"\n",
    "OUT_ROOT    = REPO_ROOT / \"data/features/embeds_yamnet_50k\"   # output folder\n",
    "\n",
    "# === Embedding parameters ===\n",
    "CLIP_SECONDS = 10.0      # must match manifest\n",
    "HOP_SECONDS  = 10.0\n",
    "RESAMPLE_HZ  = 16000\n",
    "BATCH_SIZE   = 64        # lower if you hit memory issues\n",
    "DTYPE_OUT    = \"float16\" # store embeddings compactly\n",
    "\n",
    "# === Pilot controls ===\n",
    "LIMIT_WINDOWS = None      # set e.g. 20000 for a pilot\n",
    "DRY_RUN = False           # True → skip saving; False → write output\n",
    "\n",
    "print(\"\\nConfig loaded ✅\")\n",
    "print(f\"Manifest: {IN_MANIFEST}\")\n",
    "print(f\"Output root: {OUT_ROOT}\")\n",
    "print(f\"Limit windows: {LIMIT_WINDOWS}\")\n",
    "print(f\"Dry run: {DRY_RUN}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d4ac8",
   "metadata": {},
   "source": [
    "## Load Manifest & Estimate Disk/Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58633709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Using 50,000 clips\n",
      "Estimated output size: ~117.2 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Load manifest ===\n",
    "IN_MANIFEST = Path(IN_MANIFEST)\n",
    "assert IN_MANIFEST.exists(), f\"Manifest not found: {IN_MANIFEST}\"\n",
    "\n",
    "df = pd.read_parquet(IN_MANIFEST) if IN_MANIFEST.suffix == \".parquet\" else pd.read_csv(IN_MANIFEST)\n",
    "\n",
    "# Column that points to WAVs: your sampler wrote 'filepath'\n",
    "PATH_COL = \"filepath\" if \"filepath\" in df.columns else (\"wav_path\" if \"wav_path\" in df.columns else None)\n",
    "assert PATH_COL is not None, \"Manifest needs a 'filepath' (or 'wav_path') column.\"\n",
    "\n",
    "# ---- Optional subsample for pilot ----\n",
    "if LIMIT_WINDOWS:\n",
    "    df = df.sample(n=min(LIMIT_WINDOWS, len(df)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "num_rows = len(df)\n",
    "est_total_mb = (num_rows * 1024 * 2 * 1.2) / (1024**2)  # rough estimate\n",
    "print(f\"⚡ Using {num_rows:,} clips\")\n",
    "print(f\"Estimated output size: ~{est_total_mb:.1f} MB\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b638d2",
   "metadata": {},
   "source": [
    "## Load YaMNet Model (TF Hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5544810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1761509408.734622  228644 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3497 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAMNet model loaded ✅\n"
     ]
    }
   ],
   "source": [
    "# ---- Load YAMNet model ----\n",
    "YAMNET_HANDLE = \"https://tfhub.dev/google/yamnet/1\"\n",
    "yamnet = hub.load(YAMNET_HANDLE)\n",
    "print(\"YAMNet model loaded ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d30940",
   "metadata": {},
   "source": [
    "## Helper functions - audio loading , batching , shard writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed57e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- AUDIO LOADING ----------\n",
    "def read_window(wav_path: str, start_s: float, end_s: float, target_sr: int = 16000) -> np.ndarray:\n",
    "    \"\"\"Reads a specific time window from a WAV file and resamples to target_sr.\"\"\"\n",
    "    try:\n",
    "        with sf.SoundFile(wav_path, \"r\") as rf:\n",
    "            sr = rf.samplerate\n",
    "            start_frame = max(0, int(round(start_s * sr)))\n",
    "            n_frames = max(0, int(round((end_s - start_s) * sr)))\n",
    "            rf.seek(start_frame)\n",
    "            y = rf.read(frames=n_frames, dtype=\"float32\", always_2d=False)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {wav_path}: {e}\")\n",
    "        return np.zeros(int(target_sr * 0.5), dtype=np.float32)\n",
    "\n",
    "    if y.size == 0:\n",
    "        y = np.zeros(int(target_sr * 0.5), dtype=np.float32)\n",
    "\n",
    "    # convert to mono if multi-channel\n",
    "    if y.ndim > 1:\n",
    "        y = np.mean(y, axis=1).astype(np.float32)\n",
    "\n",
    "    # resample to 16 kHz\n",
    "    if sr != target_sr:\n",
    "        try:\n",
    "            y = librosa.resample(y, orig_sr=sr, target_sr=target_sr, res_type=\"kaiser_fast\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Resample failed for {wav_path}: {e}\")\n",
    "            y = np.zeros(int(target_sr * 0.5), dtype=np.float32)\n",
    "\n",
    "    return np.clip(y, -1.0, 1.0)\n",
    "\n",
    "# ---------- PATH HELPERS ----------\n",
    "def shard_path(out_root: Path, logger: str, date: str) -> Path:\n",
    "    \"\"\"Determines which Parquet shard to write this clip's embedding to.\"\"\"\n",
    "    return out_root / \"PAPCA\" / (logger or \"unknown\") / (date or \"unknown\") / \"features.parquet\"\n",
    "\n",
    "# ---------- IMPROVED PATH HELPERS ----------\n",
    "def detect_logger_date(wav_path: str):\n",
    "    \"\"\"\n",
    "    Extract logger ID and date from path.\n",
    "    Expected formats: \n",
    "      - .../PAPCA/<logger>/<YYYYMMDD>/...\n",
    "      - .../PAPCA_test/<logger>/<YYYYMMDD>/...\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert to Path object and get parts\n",
    "        parts = Path(wav_path).parts\n",
    "        \n",
    "        # Find PAPCA or PAPCA_test in the path\n",
    "        papca_idx = -1\n",
    "        for idx, part in enumerate(parts):\n",
    "            if part == \"PAPCA\" or part == \"PAPCA_test\":\n",
    "                papca_idx = idx\n",
    "                break\n",
    "        \n",
    "        if papca_idx >= 0:\n",
    "            # Extract logger and date (next two parts after PAPCA/PAPCA_test)\n",
    "            if papca_idx + 2 < len(parts):\n",
    "                logger = parts[papca_idx + 1]\n",
    "                date = parts[papca_idx + 2]\n",
    "                \n",
    "                # Validate they're not empty\n",
    "                if logger and date and logger != \"\" and date != \"\":\n",
    "                    return logger, date\n",
    "        \n",
    "        # Fallback: try to extract from string path with regex-like pattern\n",
    "        path_str = str(wav_path)\n",
    "        \n",
    "        # Try both PAPCA and PAPCA_test\n",
    "        for papca_variant in [\"/PAPCA_test/\", \"/PAPCA/\"]:\n",
    "            if papca_variant in path_str:\n",
    "                after_papca = path_str.split(papca_variant)[1]\n",
    "                parts_after = after_papca.split(\"/\")\n",
    "                if len(parts_after) >= 2:\n",
    "                    logger = parts_after[0]\n",
    "                    date = parts_after[1]\n",
    "                    if logger and date:\n",
    "                        return logger, date\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Path parsing error for {wav_path}: {e}\")\n",
    "    \n",
    "    return \"unknown\", \"unknown\"\n",
    "\n",
    "# ---------- EMBEDDING UTILS ----------\n",
    "MIN_SEC = 0.5  # pad short/empty windows\n",
    "\n",
    "def _pad_min_length(y, target_sr, min_sec=MIN_SEC):\n",
    "    min_len = int(target_sr * min_sec)\n",
    "    if y.size < min_len:\n",
    "        y = np.pad(y.astype(np.float32), (0, min_len - y.size))\n",
    "    return y\n",
    "\n",
    "def yamnet_embeddings(batch_waveforms, target_sr=16000):\n",
    "    \"\"\"\n",
    "    Runs YAMNet on a batch of waveforms (each 1D np.float32).\n",
    "    Returns: np.ndarray [B, 1024]\n",
    "    \"\"\"\n",
    "    outs = []\n",
    "    for w in batch_waveforms:\n",
    "        try:\n",
    "            w = _pad_min_length(np.asarray(w, dtype=np.float32), target_sr)\n",
    "            wf = tf.convert_to_tensor(w, dtype=tf.float32)\n",
    "            with tf.device('/GPU:0'):\n",
    "                scores, embeddings, _ = yamnet(wf)\n",
    "                emb = tf.reduce_mean(embeddings, axis=0)  # [1024]\n",
    "            outs.append(emb.numpy())\n",
    "        except Exception as e:\n",
    "            # CPU fallback\n",
    "            wf = tf.convert_to_tensor(w, dtype=tf.float32)\n",
    "            with tf.device('/CPU:0'):\n",
    "                scores, embeddings, _ = yamnet(wf)\n",
    "                outs.append(tf.reduce_mean(embeddings, axis=0).numpy())\n",
    "    return np.stack(outs, axis=0)\n",
    "\n",
    "# ---------- SAVE TO PARQUET ----------\n",
    "def append_parquet_rows(path: Path, rows):\n",
    "    \"\"\"\n",
    "    Append rows to a Parquet shard as a new row-group (no full-file read).\n",
    "    \"\"\"\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    if not rows:\n",
    "        return\n",
    "\n",
    "    # Convert to pylist with float16 list for 'yamnet_1024'\n",
    "    recs = []\n",
    "    for r in rows:\n",
    "        rec = dict(r)\n",
    "        v = rec.get(\"yamnet_1024\", None)\n",
    "        if v is not None:\n",
    "            rec[\"yamnet_1024\"] = np.asarray(v, dtype=np.float16).tolist()\n",
    "        recs.append(rec)\n",
    "\n",
    "    table = pa.Table.from_pylist(recs)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if path.exists():\n",
    "        # Append a new row group\n",
    "        with pq.ParquetWriter(path, table.schema, compression=\"snappy\", use_dictionary=True, write_statistics=True, append=True) as writer:\n",
    "            writer.write_table(table)\n",
    "    else:\n",
    "        pq.write_table(table, path, compression=\"snappy\", use_dictionary=True, write_statistics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a425ba",
   "metadata": {},
   "source": [
    "## Tiny sample (dry run) to verify shapes & speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76079f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎧 Testing YAMNet on 8 random clips...\n"
     ]
    }
   ],
   "source": [
    "if len(df) == 0:\n",
    "    print(\"❌ Manifest is empty — nothing to process.\")\n",
    "else:\n",
    "    sample_df = df.sample(n=min(8, len(df)), random_state=42).reset_index(drop=True)\n",
    "    print(f\"🎧 Testing YAMNet on {len(sample_df)} random clips...\")\n",
    "\n",
    "    batch = []\n",
    "    for _, r in sample_df.iterrows():\n",
    "        try:\n",
    "            start_s = float(r[\"start_s\"])\n",
    "            end_val = r.get(\"end_s\", np.nan)  # r is a Series; .get is safe\n",
    "            if pd.notna(end_val):\n",
    "                end_s = float(end_val)\n",
    "            else:\n",
    "                dur = float(r.get(\"duration_s\", CLIP_SECONDS))\n",
    "                end_s = start_s + dur\n",
    "\n",
    "            w = read_window(r[PATH_COL], start_s, end_s, target_sr=RESAMPLE_HZ)\n",
    "            batch.append(w)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error reading {r[PATH_COL]}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a517774",
   "metadata": {},
   "source": [
    "## Full pass , we write per day parquet shards (float16) to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47eeecd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Starting YAMNet embedding pass\n",
      "   Total windows: 50,000\n",
      "   Output directory: /home/sparch/Uni-stuff/semester-2/applied_Ml/reef_zmsc/data/features/embeds_yamnet_50k\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 06:41:20.131795: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 640/50,000 done (1.3%) | 33.7 clips/s | Errors: 0\n",
      "✅ 1,280/50,000 done (2.6%) | 34.4 clips/s | Errors: 0\n",
      "✅ 1,920/50,000 done (3.8%) | 34.6 clips/s | Errors: 0\n",
      "✅ 2,560/50,000 done (5.1%) | 35.0 clips/s | Errors: 0\n",
      "✅ 3,200/50,000 done (6.4%) | 35.2 clips/s | Errors: 0\n",
      "✅ 3,840/50,000 done (7.7%) | 35.6 clips/s | Errors: 0\n",
      "✅ 4,480/50,000 done (9.0%) | 35.6 clips/s | Errors: 0\n",
      "💾 Flushing buffer at 5,056 clips...\n",
      "✅ 5,120/50,000 done (10.2%) | 35.2 clips/s | Errors: 0\n",
      "✅ 5,760/50,000 done (11.5%) | 35.2 clips/s | Errors: 0\n",
      "✅ 6,400/50,000 done (12.8%) | 35.2 clips/s | Errors: 0\n",
      "✅ 7,040/50,000 done (14.1%) | 35.2 clips/s | Errors: 0\n",
      "✅ 7,680/50,000 done (15.4%) | 35.3 clips/s | Errors: 0\n",
      "✅ 8,320/50,000 done (16.6%) | 35.3 clips/s | Errors: 0\n",
      "✅ 8,960/50,000 done (17.9%) | 35.3 clips/s | Errors: 0\n",
      "✅ 9,600/50,000 done (19.2%) | 35.3 clips/s | Errors: 0\n",
      "💾 Flushing buffer at 10,112 clips...\n",
      "✅ 10,240/50,000 done (20.5%) | 35.3 clips/s | Errors: 0\n",
      "✅ 10,880/50,000 done (21.8%) | 35.3 clips/s | Errors: 0\n",
      "✅ 11,520/50,000 done (23.0%) | 35.3 clips/s | Errors: 0\n",
      "✅ 12,160/50,000 done (24.3%) | 35.4 clips/s | Errors: 0\n",
      "✅ 12,800/50,000 done (25.6%) | 35.4 clips/s | Errors: 0\n",
      "✅ 13,440/50,000 done (26.9%) | 35.4 clips/s | Errors: 0\n",
      "✅ 14,080/50,000 done (28.2%) | 35.4 clips/s | Errors: 0\n",
      "✅ 14,720/50,000 done (29.4%) | 35.4 clips/s | Errors: 0\n",
      "💾 Flushing buffer at 15,168 clips...\n",
      "✅ 15,360/50,000 done (30.7%) | 35.4 clips/s | Errors: 0\n",
      "✅ 16,000/50,000 done (32.0%) | 35.4 clips/s | Errors: 0\n",
      "✅ 16,640/50,000 done (33.3%) | 35.4 clips/s | Errors: 0\n",
      "✅ 17,280/50,000 done (34.6%) | 35.4 clips/s | Errors: 0\n",
      "✅ 17,920/50,000 done (35.8%) | 35.5 clips/s | Errors: 0\n",
      "✅ 18,560/50,000 done (37.1%) | 35.5 clips/s | Errors: 0\n",
      "✅ 19,200/50,000 done (38.4%) | 35.5 clips/s | Errors: 0\n",
      "✅ 19,840/50,000 done (39.7%) | 35.5 clips/s | Errors: 0\n",
      "💾 Flushing buffer at 20,224 clips...\n",
      "✅ 20,480/50,000 done (41.0%) | 35.5 clips/s | Errors: 0\n",
      "✅ 21,120/50,000 done (42.2%) | 35.5 clips/s | Errors: 0\n",
      "✅ 21,760/50,000 done (43.5%) | 35.6 clips/s | Errors: 0\n",
      "✅ 22,400/50,000 done (44.8%) | 35.6 clips/s | Errors: 0\n",
      "✅ 23,040/50,000 done (46.1%) | 35.6 clips/s | Errors: 0\n",
      "✅ 23,680/50,000 done (47.4%) | 35.6 clips/s | Errors: 0\n",
      "✅ 24,320/50,000 done (48.6%) | 35.6 clips/s | Errors: 0\n",
      "✅ 24,960/50,000 done (49.9%) | 35.6 clips/s | Errors: 0\n",
      "💾 Flushing buffer at 25,280 clips...\n",
      "✅ 25,600/50,000 done (51.2%) | 35.6 clips/s | Errors: 0\n",
      "✅ 26,240/50,000 done (52.5%) | 35.6 clips/s | Errors: 0\n",
      "✅ 26,880/50,000 done (53.8%) | 35.7 clips/s | Errors: 0\n",
      "✅ 27,520/50,000 done (55.0%) | 35.7 clips/s | Errors: 0\n",
      "✅ 28,160/50,000 done (56.3%) | 35.7 clips/s | Errors: 0\n",
      "✅ 28,800/50,000 done (57.6%) | 35.7 clips/s | Errors: 0\n",
      "✅ 29,440/50,000 done (58.9%) | 35.7 clips/s | Errors: 0\n",
      "✅ 30,080/50,000 done (60.2%) | 35.7 clips/s | Errors: 0\n",
      "💾 Flushing buffer at 30,336 clips...\n",
      "✅ 30,720/50,000 done (61.4%) | 35.7 clips/s | Errors: 0\n",
      "✅ 31,360/50,000 done (62.7%) | 35.7 clips/s | Errors: 0\n",
      "✅ 32,000/50,000 done (64.0%) | 35.7 clips/s | Errors: 0\n",
      "✅ 32,640/50,000 done (65.3%) | 35.7 clips/s | Errors: 0\n",
      "✅ 33,280/50,000 done (66.6%) | 35.8 clips/s | Errors: 0\n",
      "✅ 33,920/50,000 done (67.8%) | 35.8 clips/s | Errors: 0\n",
      "✅ 34,560/50,000 done (69.1%) | 35.8 clips/s | Errors: 0\n",
      "✅ 35,200/50,000 done (70.4%) | 35.8 clips/s | Errors: 0\n",
      "💾 Flushing buffer at 35,392 clips...\n",
      "✅ 35,840/50,000 done (71.7%) | 35.8 clips/s | Errors: 0\n",
      "✅ 36,480/50,000 done (73.0%) | 35.8 clips/s | Errors: 0\n",
      "✅ 37,120/50,000 done (74.2%) | 35.8 clips/s | Errors: 0\n",
      "✅ 37,760/50,000 done (75.5%) | 35.8 clips/s | Errors: 0\n",
      "✅ 38,400/50,000 done (76.8%) | 35.8 clips/s | Errors: 0\n",
      "✅ 39,040/50,000 done (78.1%) | 35.8 clips/s | Errors: 0\n",
      "✅ 39,680/50,000 done (79.4%) | 35.8 clips/s | Errors: 0\n",
      "✅ 40,320/50,000 done (80.6%) | 35.8 clips/s | Errors: 0\n",
      "💾 Flushing buffer at 40,448 clips...\n",
      "✅ 40,960/50,000 done (81.9%) | 35.8 clips/s | Errors: 0\n",
      "✅ 41,600/50,000 done (83.2%) | 35.8 clips/s | Errors: 0\n",
      "✅ 42,240/50,000 done (84.5%) | 35.8 clips/s | Errors: 0\n",
      "✅ 42,880/50,000 done (85.8%) | 35.8 clips/s | Errors: 0\n",
      "✅ 43,520/50,000 done (87.0%) | 35.8 clips/s | Errors: 0\n",
      "✅ 44,160/50,000 done (88.3%) | 35.8 clips/s | Errors: 0\n",
      "✅ 44,800/50,000 done (89.6%) | 35.8 clips/s | Errors: 0\n",
      "✅ 45,440/50,000 done (90.9%) | 35.8 clips/s | Errors: 0\n",
      "💾 Flushing buffer at 45,504 clips...\n",
      "✅ 46,080/50,000 done (92.2%) | 35.8 clips/s | Errors: 0\n",
      "✅ 46,720/50,000 done (93.4%) | 35.8 clips/s | Errors: 0\n",
      "✅ 47,360/50,000 done (94.7%) | 35.8 clips/s | Errors: 0\n",
      "✅ 48,000/50,000 done (96.0%) | 35.8 clips/s | Errors: 0\n",
      "✅ 48,640/50,000 done (97.3%) | 35.9 clips/s | Errors: 0\n",
      "✅ 49,280/50,000 done (98.6%) | 35.9 clips/s | Errors: 0\n",
      "✅ 49,920/50,000 done (99.8%) | 35.9 clips/s | Errors: 0\n",
      "✅ 50,000/50,000 done (100.0%) | 35.9 clips/s | Errors: 0\n",
      "\n",
      "💾 Final flush...\n",
      "✅ All embeddings written successfully!\n",
      "\n",
      "🏁 Done: 50,000 processed | 0 errors | 35.9 clips/s | Total time: 23.2 min\n"
     ]
    }
   ],
   "source": [
    "# ---------- IMPROVED SAVE TO PARQUET ----------\n",
    "def append_parquet_rows(path: Path, rows):\n",
    "    \"\"\"\n",
    "    Efficiently append rows to a Parquet file using ParquetWriter properly.\n",
    "    \"\"\"\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    if not rows:\n",
    "        return\n",
    "\n",
    "    # Convert to pylist with float16 list for 'yamnet_1024'\n",
    "    recs = []\n",
    "    for r in rows:\n",
    "        rec = dict(r)\n",
    "        v = rec.get(\"yamnet_1024\", None)\n",
    "        if v is not None:\n",
    "            rec[\"yamnet_1024\"] = np.asarray(v, dtype=np.float16).tolist()\n",
    "        recs.append(rec)\n",
    "\n",
    "    new_table = pa.Table.from_pylist(recs)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if path.exists():\n",
    "        # Read and combine approach (simpler, but reads entire file)\n",
    "        existing_table = pq.read_table(path)\n",
    "        combined_table = pa.concat_tables([existing_table, new_table])\n",
    "        pq.write_table(combined_table, path, compression=\"snappy\", \n",
    "                      use_dictionary=True, write_statistics=True)\n",
    "    else:\n",
    "        # Create new file\n",
    "        pq.write_table(new_table, path, compression=\"snappy\", \n",
    "                      use_dictionary=True, write_statistics=True)\n",
    "\n",
    "\n",
    "# === Full YAMNet embedding pass ===\n",
    "if not DRY_RUN and len(df) > 0:\n",
    "    import time, gc\n",
    "\n",
    "    total_rows = len(df)\n",
    "    df_iter = df.itertuples(index=False)\n",
    "    rows_buf = {}  # {Path -> [records]}\n",
    "    processed, errors = 0, 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    print(f\"\\n▶ Starting YAMNet embedding pass\")\n",
    "    print(f\"   Total windows: {total_rows:,}\")\n",
    "    print(f\"   Output directory: {OUT_ROOT}\\n\")\n",
    "\n",
    "    OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        while processed < total_rows:\n",
    "            batch_waves, batch_meta = [], []\n",
    "\n",
    "            # === Load one batch of windows ===\n",
    "            for _ in range(min(BATCH_SIZE, total_rows - processed)):\n",
    "                try:\n",
    "                    r = next(df_iter)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                try:\n",
    "                    wav_path = getattr(r, PATH_COL)\n",
    "                    start_s = float(r.start_s)\n",
    "                    # compute end_s if missing in sampled parquet\n",
    "                    end_s_attr = getattr(r, 'end_s', None)\n",
    "                    if end_s_attr is None:\n",
    "                        dur = float(getattr(r, 'duration_s', CLIP_SECONDS))\n",
    "                        end_s = start_s + dur\n",
    "                    else:\n",
    "                        end_s = float(end_s_attr)\n",
    "\n",
    "                    y = read_window(wav_path, start_s, end_s, target_sr=RESAMPLE_HZ)\n",
    "                    batch_waves.append(y)\n",
    "                    batch_meta.append((wav_path, start_s, end_s))\n",
    "                except Exception as e:\n",
    "                    errors += 1\n",
    "                    if errors <= 5:\n",
    "                        print(f\"⚠️ Read error: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if not batch_waves:\n",
    "                break\n",
    "\n",
    "            # === Compute embeddings ===\n",
    "            embs = yamnet_embeddings(batch_waves)  # [B, 1024]\n",
    "\n",
    "            # === Accumulate rows ===\n",
    "            for (wav_path, start_s, end_s), emb in zip(batch_meta, embs):\n",
    "                logger, date = detect_logger_date(wav_path)\n",
    "                rec = {\n",
    "                    \"filepath\": wav_path,\n",
    "                    \"start_s\": start_s,\n",
    "                    \"end_s\": end_s,\n",
    "                    \"logger\": logger,\n",
    "                    \"date\": date,\n",
    "                    \"yamnet_1024\": emb.astype(DTYPE_OUT),  # float16\n",
    "                }\n",
    "                shard = shard_path(OUT_ROOT, logger, date)\n",
    "                rows_buf.setdefault(shard, []).append(rec)\n",
    "\n",
    "            processed += len(batch_waves)\n",
    "\n",
    "            # === Flush periodically (reduced frequency to minimize file operations) ===\n",
    "            if sum(len(v) for v in rows_buf.values()) >= 5000:  # Increased from 2000\n",
    "                print(f\"💾 Flushing buffer at {processed:,} clips...\")\n",
    "                for spath, rows in list(rows_buf.items()):\n",
    "                    if rows:\n",
    "                        try:\n",
    "                            append_parquet_rows(spath, rows)\n",
    "                            rows_buf[spath] = []\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error writing to {spath}: {e}\")\n",
    "                gc.collect()\n",
    "\n",
    "            # === Progress log ===\n",
    "            if processed % (BATCH_SIZE * 10) == 0 or processed >= total_rows:\n",
    "                dt = time.time() - t0\n",
    "                rps = processed / max(1e-6, dt)\n",
    "                print(f\"✅ {processed:,}/{total_rows:,} done ({processed/total_rows*100:.1f}%) | \"\n",
    "                      f\"{rps:.1f} clips/s | Errors: {errors}\")\n",
    "\n",
    "        # === Final flush ===\n",
    "        print(\"\\n💾 Final flush...\")\n",
    "        for spath, rows in list(rows_buf.items()):\n",
    "            if rows:\n",
    "                try:\n",
    "                    append_parquet_rows(spath, rows)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error in final flush for {spath}: {e}\")\n",
    "        print(\"✅ All embeddings written successfully!\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n🟡 Interrupted — flushing buffers before exit...\")\n",
    "        for spath, rows in list(rows_buf.items()):\n",
    "            if rows:\n",
    "                try:\n",
    "                    append_parquet_rows(spath, rows)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error flushing {spath}: {e}\")\n",
    "        print(\"   Buffers flushed. Partial progress saved.\")\n",
    "\n",
    "    # === Summary ===\n",
    "    dt = time.time() - t0\n",
    "    print(f\"\\n🏁 Done: {processed:,} processed | {errors:,} errors | \"\n",
    "          f\"{processed / max(1, dt):.1f} clips/s | Total time: {dt / 60:.1f} min\")\n",
    "else:\n",
    "    print(\"DRY_RUN=True → Skipping embedding write.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77cc749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3bf8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
