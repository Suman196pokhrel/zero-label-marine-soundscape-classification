**Zero-Label Marine Soundscape Classification (ZMSC)** pipeline:

---

## ğŸ“‚ Folders

### `data/`

This is your **working data directory**.

* **raw/** â†’ Holds your untouched `.DAT` files (as downloaded).
* **clips/** â†’ 10-second `.wav` slices after conversion & slicing.
* **mels/** â†’ Log-mel spectrograms (as `.npy` or `.png` previews for quick checks).
* **indices/** â†’ Ecoacoustic indices (e.g., spectral entropy, ACI, band energies) stored in `.parquet` or `.csv`.
* **previews/** â†’ Lightweight sanity checks (e.g., small `.mp3` clips + spectrogram images sampled per hour).
* **manifests/** â†’ Tables (`.csv`/`.parquet`) describing the dataset at each stage:

  * `clips.parquet` â†’ all metadata for every clip.
  * `features.parquet` â†’ embeddings (PANNs, SSL, indices).
  * `clusters.parquet` â†’ clustering + auto-labels.

---

### `models/`

* Stores pretrained weights (e.g., **PANNs**, **YAMNet**) and your fine-tuned self-supervised models (BYOL-A, SimCLR).
* Keeps everything local so the pipeline can run reproducibly, even offline.

---

### `cfg/`

* YAML config files controlling the pipeline.
* Example: `default.yaml` defines paths, audio params (sr, hop), feature extraction settings, clustering hyperparameters, etc.
* Advantage: you donâ€™t hard-code values inside scripts â†’ you can swap configs (e.g., `cfg/perth_canyon.yaml`, `cfg/kangaroo_island.yaml`).

---

### `src/`

* Your **pipeline scripts**, named by order:

  * `00_dat_to_wav.py` â†’ convert `.DAT` â†’ `.wav`.
  * `01_slice.py` â†’ cut into 10s clips with 5s hop.
  * `02_features_deep.py` â†’ extract embeddings from pretrained models (PANNs, YAMNet).
  * `03_features_ssl.py` â†’ self-supervised embeddings (BYOL-A, SimCLR).
  * `04_features_indices.py` â†’ ecoacoustic indices.
  * `05_reduce_cluster.py` â†’ PCA â†’ UMAP â†’ HDBSCAN.
  * `06_constrained_clustering.py` â†’ apply must-link / cannot-link constraints.
  * `07_autolabel_train.py` â†’ auto-label BIO / AMBIENT / HUMAN + train classifier.
  * `99_freeze_drop.py` â†’ final â€œdataset dropâ€ (Step 10).

---

### `scripts/`

* Helper shell scripts.
* Example: `run_all.sh` to run the full pipeline.
* Can also keep â€œone-linersâ€ here (e.g., daily reslice, recompute indices).

---

## ğŸ“ Root files

* **README.md** â†’ Quick description of the project, folder structure, and how to run.
* **Makefile** â†’ Defines pipeline stages so you can just run `make all`.

---

## ğŸ§  Why this structure matters

* **Reproducibility** â†’ Data, configs, code, and outputs are cleanly separated.
* **Modularity** â†’ You can rerun only a specific stage (e.g., just clustering) without touching others.
* **Scalability** â†’ Easy to extend for new deployments (just add a new config).
* **Portability** â†’ Step 10 freeze creates a â€œdropâ€ you can share with supervisors, or re-use for new experiments.

---

ğŸ‘‰ Do you want me to also sketch **what each `manifest` table should contain** (column names + datatypes), so you have a clear schema before you start generating them?
